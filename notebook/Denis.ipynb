{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12639078,"sourceType":"datasetVersion","datasetId":7986830},{"sourceId":12640174,"sourceType":"datasetVersion","datasetId":7987619},{"sourceId":12640533,"sourceType":"datasetVersion","datasetId":7987871}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install ultralytics > /dev/null","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-01T12:23:11.423667Z","iopub.execute_input":"2025-08-01T12:23:11.423834Z","iopub.status.idle":"2025-08-01T12:24:24.267621Z","shell.execute_reply.started":"2025-08-01T12:23:11.423818Z","shell.execute_reply":"2025-08-01T12:24:24.266574Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom ultralytics import YOLO\nfrom ultralytics.nn.modules import Conv, C2f, SPPF, Detect\nfrom ultralytics.utils import LOGGER\nfrom ultralytics.models.yolo.detect import DetectionTrainer\nfrom ultralytics.utils.torch_utils import select_device, smart_inference_mode\nimport numpy as np\nfrom typing import Dict, List, Tuple, Optional, Any, Union\nimport math\nimport os\nimport sys\nfrom pathlib import Path\nimport yaml\nimport shutil\nfrom sklearn.model_selection import train_test_split\nfrom torch.optim import AdamW, SGD\nimport warnings\nimport logging\nimport gc\nimport time\nfrom scipy import ndimage\nimport cv2\nfrom PIL import Image\nwarnings.filterwarnings('ignore')\n\n# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Ä–µ–∂–∏–º–∞ –æ—Ç–ª–∞–¥–∫–∏\n# –ï—Å–ª–∏ True - –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ 800 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö, 1 —ç–ø–æ—Ö–∞, –≤–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö\n# –ï—Å–ª–∏ False - –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö\nIS_DEBUG = True\n\n# –ü—É—Ç–∏ –∫ –¥–∞—Ç–∞—Å–µ—Ç–∞–º Kaggle\nTRAIN_DATASET_1 = '/kaggle/input/01trains1datasethumanrescu1'\nTRAIN_DATASET_2 = '/kaggle/input/02secondpartdatasethumanrescue'\nVAL_DATASET_PUBLIC = '/kaggle/input/03validationdatasethumanrescue/public'\nVAL_DATASET_PRIVATE = '/kaggle/input/03validationdatasethumanrescue/private'\n\nlog_dir = \"/kaggle/working/\"\n\nlogger = logging.getLogger('ml')\nlogger.setLevel(logging.DEBUG)\n\n# Prevent adding handlers multiple times\nif logger.hasHandlers():\n    logger.handlers.clear()\n\n# File handlers for different log levels\nfrom logging.handlers import RotatingFileHandler\n\n# Error log handler\nerror_handler = RotatingFileHandler(\n    os.path.join(log_dir, 'main_error.log'),\n    maxBytes=1*1024*1024,  # 1MB\n    backupCount=5,\n    encoding='utf-8'\n)\nerror_handler.setLevel(logging.ERROR)\n\n# Warning log handler\nwarning_handler = RotatingFileHandler(\n    os.path.join(log_dir, 'main_warning.log'),\n    maxBytes=1*1024*1024,  # 1MB\n    backupCount=5,\n    encoding='utf-8'\n)\nwarning_handler.setLevel(logging.WARNING)\n\n# Debug log handler\ndebug_handler = RotatingFileHandler(\n    os.path.join(log_dir, 'main_debug.log'),\n    maxBytes=1*1024*1024,  # 1MB\n    backupCount=5,\n    encoding='utf-8'\n)\ndebug_handler.setLevel(logging.DEBUG)\n\n# Console handler\nconsole_handler = logging.StreamHandler()\nconsole_handler.setLevel(logging.INFO)\n\n# Formatter\nformatter = logging.Formatter(\n    '%(asctime)s - MAIN - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'\n)\nerror_handler.setFormatter(formatter)\nwarning_handler.setFormatter(formatter)\ndebug_handler.setFormatter(formatter)\nconsole_handler.setFormatter(formatter)\n\n# Add handlers\nlogger.addHandler(error_handler)\nlogger.addHandler(warning_handler)\nlogger.addHandler(debug_handler)\nlogger.addHandler(console_handler)\n\nlogger.info(\"Logging setup completed\")\nprint(\"[DEBUG] Logging setup completed successfully.\")\n\ndef create_combined_dataset_yaml(output_path: str = 'combined_data.yaml') -> str:\n    \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ YAML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\"\"\"\n    \n    # –í —Ä–µ–∂–∏–º–µ –æ—Ç–ª–∞–¥–∫–∏ —Å–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É\n    if IS_DEBUG:\n        logger.info(\"üêõ –°–æ–∑–¥–∞–Ω–∏–µ YAML –¥–ª—è debug —Ä–µ–∂–∏–º–∞\")\n        \n        # –°–æ–∑–¥–∞–Ω–∏–µ debug —Å—Ç—Ä—É–∫—Ç—É—Ä—ã\n        train_datasets = [TRAIN_DATASET_1, TRAIN_DATASET_2]\n        debug_train_dir = create_debug_dataset_structure(train_datasets, 800)\n        \n        if debug_train_dir:\n            yaml_config = {\n                'path': '/kaggle/working/debug_dataset',\n                'train': os.path.join(debug_train_dir, 'images'),\n                'val': os.path.join(VAL_DATASET_PUBLIC, 'images') if os.path.exists(VAL_DATASET_PUBLIC) else '',\n                'test': os.path.join(VAL_DATASET_PRIVATE, 'images') if os.path.exists(VAL_DATASET_PRIVATE) else '',\n                'nc': 1,\n                'names': ['person']\n            }\n            \n            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ YAML —Ñ–∞–π–ª–∞\n            try:\n                with open(output_path, 'w', encoding='utf-8') as f:\n                    yaml.dump(yaml_config, f, default_flow_style=False, allow_unicode=True)\n                logger.info(f\"‚úÖ Debug YAML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_path}\")\n                logger.info(f\"   –û–±—É—á–µ–Ω–∏–µ: {yaml_config['train']}\")\n                logger.info(f\"   –í–∞–ª–∏–¥–∞—Ü–∏—è: {yaml_config['val']}\")\n                return output_path\n            except Exception as e:\n                logger.error(f\"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è debug YAML: {e}\")\n                return ''\n    \n    # –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º - –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\n    datasets_info = {\n        'train_1': {'path': TRAIN_DATASET_1, 'exists': False},\n        'train_2': {'path': TRAIN_DATASET_2, 'exists': False},\n        'val_public': {'path': VAL_DATASET_PUBLIC, 'exists': False},\n        'val_private': {'path': VAL_DATASET_PRIVATE, 'exists': False}\n    }\n    \n    for name, info in datasets_info.items():\n        if os.path.exists(info['path']):\n            info['exists'] = True\n            images_path = os.path.join(info['path'], 'images')\n            labels_path = os.path.join(info['path'], 'labels')\n            if os.path.exists(images_path) and os.path.exists(labels_path):\n                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞—Ç–∞—Å–µ—Ç–∞\n                subdirs = [d for d in os.listdir(images_path) if os.path.isdir(os.path.join(images_path, d))]\n                \n                if subdirs:  # –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞\n                    image_label_pairs = collect_hierarchical_images(info['path'])\n                    image_count = len(image_label_pairs)\n                    label_count = len([pair for pair in image_label_pairs if os.path.exists(pair[1])])\n                    logger.info(f\"üìä {name} (–∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è): {image_count} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, {label_count} –º–µ—Ç–æ–∫ –≤ {len(subdirs)} –ø–æ–¥–ø–∞–ø–∫–∞—Ö\")\n                else:  # –ü–ª–æ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞\n                    image_count = len([f for f in os.listdir(images_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n                    label_count = len([f for f in os.listdir(labels_path) if f.lower().endswith('.txt')])\n                    logger.info(f\"üìä {name} (–ø–ª–æ—Å–∫–∞—è): {image_count} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, {label_count} –º–µ—Ç–æ–∫\")\n            else:\n                logger.warning(f\"‚ö†Ô∏è {name}: –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø–∞–ø–∫–∏ images –∏–ª–∏ labels\")\n        else:\n            logger.warning(f\"‚ö†Ô∏è {name}: –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏ {info['path']}\")\n    \n    # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ YAML\n    yaml_config = {\n        'path': os.path.dirname(os.path.abspath(output_path)),\n        'train': [],\n        'val': os.path.join(VAL_DATASET_PUBLIC, 'images') if datasets_info['val_public']['exists'] else '',\n        'test': os.path.join(VAL_DATASET_PRIVATE, 'images') if datasets_info['val_private']['exists'] else '',\n        'nc': 1,\n        'names': ['person']\n    }\n    \n    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\n    if datasets_info['train_1']['exists']:\n        yaml_config['train'].append(os.path.join(TRAIN_DATASET_1, 'images'))\n    if datasets_info['train_2']['exists']:\n        yaml_config['train'].append(os.path.join(TRAIN_DATASET_2, 'images'))\n    \n    # –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –¥–∞—Ç–∞—Å–µ—Ç, —É–±–∏—Ä–∞–µ–º —Å–ø–∏—Å–æ–∫\n    if len(yaml_config['train']) == 1:\n        yaml_config['train'] = yaml_config['train'][0]\n    elif len(yaml_config['train']) == 0:\n        logger.error(\"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –æ–±—É—á–∞—é—â–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞!\")\n        yaml_config['train'] = ''\n    \n    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ YAML —Ñ–∞–π–ª–∞\n    try:\n        with open(output_path, 'w', encoding='utf-8') as f:\n            yaml.dump(yaml_config, f, default_flow_style=False, allow_unicode=True)\n        logger.info(f\"‚úÖ YAML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_path}\")\n        return output_path\n    except Exception as e:\n        logger.error(f\"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è YAML: {e}\")\n        return ''\n\ndef collect_hierarchical_images(dataset_path: str) -> List[Tuple[str, str]]:\n    \"\"\"\n    –°–±–æ—Ä –≤—Å–µ—Ö –ø–∞—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-–º–µ—Ç–∫–∞ –∏–∑ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–∞\n    \n    Args:\n        dataset_path: –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É —Å –ø–æ–¥–ø–∞–ø–∫–∞–º–∏\n        \n    Returns:\n        List[Tuple[str, str]]: –°–ø–∏—Å–æ–∫ –ø–∞—Ä (–ø—É—Ç—å_–∫_–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é, –ø—É—Ç—å_–∫_–º–µ—Ç–∫–µ)\n    \"\"\"\n    image_label_pairs = []\n    \n    images_base = os.path.join(dataset_path, 'images')\n    labels_base = os.path.join(dataset_path, 'labels')\n    \n    if not (os.path.exists(images_base) and os.path.exists(labels_base)):\n        logger.warning(f\"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω—ã –ø–∞–ø–∫–∏ images –∏–ª–∏ labels –≤ {dataset_path}\")\n        return image_label_pairs\n    \n    # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –ø–æ–¥–ø–∞–ø–æ–∫ –≤ images\n    try:\n        image_subdirs = [d for d in os.listdir(images_base) \n                        if os.path.isdir(os.path.join(images_base, d))]\n        image_subdirs = sorted(image_subdirs)\n        \n        logger.info(f\"üìÅ –ù–∞–π–¥–µ–Ω–æ {len(image_subdirs)} –ø–æ–¥–ø–∞–ø–æ–∫ –≤ {images_base}: {image_subdirs}\")\n        \n        for subdir in image_subdirs:\n            images_subdir = os.path.join(images_base, subdir)\n            labels_subdir = os.path.join(labels_base, subdir)\n            \n            if not os.path.exists(labels_subdir):\n                logger.warning(f\"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∞—è –ø–∞–ø–∫–∞ –º–µ—Ç–æ–∫: {labels_subdir}\")\n                continue\n            \n            # –°–±–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ –ø–æ–¥–ø–∞–ø–∫–∏\n            try:\n                image_files = [f for f in os.listdir(images_subdir) \n                             if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n                \n                for img_file in image_files:\n                    img_path = os.path.join(images_subdir, img_file)\n                    \n                    # –ü–æ–∏—Å–∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π –º–µ—Ç–∫–∏\n                    label_name = os.path.splitext(img_file)[0] + '.txt'\n                    label_path = os.path.join(labels_subdir, label_name)\n                    \n                    if os.path.exists(label_path):\n                        image_label_pairs.append((img_path, label_path))\n                    else:\n                        logger.warning(f\"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–∞ –º–µ—Ç–∫–∞ –¥–ª—è {img_file}: {label_path}\")\n                        \n            except Exception as e:\n                logger.error(f\"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ–¥–ø–∞–ø–∫–∏ {subdir}: {e}\")\n                continue\n        \n        logger.info(f\"‚úÖ –°–æ–±—Ä–∞–Ω–æ {len(image_label_pairs)} –ø–∞—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-–º–µ—Ç–∫–∞ –∏–∑ {dataset_path}\")\n        \n    except Exception as e:\n        logger.error(f\"‚ùå –û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ {dataset_path}: {e}\")\n    \n    return image_label_pairs\n\ndef create_debug_dataset_structure(train_datasets: List[str], limit: int = 800) -> str:\n    \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\"\"\"\n    if not IS_DEBUG:\n        return None\n    \n    logger.info(f\"üêõ –°–æ–∑–¥–∞–Ω–∏–µ debug —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º {limit} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\")\n    \n    # –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π\n    debug_base_dir = '/kaggle/working/debug_dataset'\n    debug_train_dir = os.path.join(debug_base_dir, 'train')\n    debug_train_images = os.path.join(debug_train_dir, 'images')\n    debug_train_labels = os.path.join(debug_train_dir, 'labels')\n    \n    # –û—á–∏—Å—Ç–∫–∞ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π\n    if os.path.exists(debug_base_dir):\n        shutil.rmtree(debug_base_dir)\n    \n    os.makedirs(debug_train_images, exist_ok=True)\n    os.makedirs(debug_train_labels, exist_ok=True)\n    \n    total_copied = 0\n    \n    # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ñ–∞–π–ª–æ–≤\n    for dataset_path in train_datasets:\n        if total_copied >= limit:\n            break\n            \n        images_dir = os.path.join(dataset_path, 'images')\n        labels_dir = os.path.join(dataset_path, 'labels')\n        \n        if not (os.path.exists(images_dir) and os.path.exists(labels_dir)):\n            logger.warning(f\"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω—ã –ø–∞–ø–∫–∏ images/labels –≤ {dataset_path}\")\n            continue\n        \n        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É (–ø–æ–¥–ø–∞–ø–∫–∏)\n        try:\n            subdirs = [d for d in os.listdir(images_dir) \n                      if os.path.isdir(os.path.join(images_dir, d))]\n            \n            if subdirs:  # –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ (–∫–∞–∫ TRAIN_DATASET_1)\n                logger.info(f\"üìÅ –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤ {dataset_path}\")\n                image_label_pairs = collect_hierarchical_images(dataset_path)\n                \n                # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–∞—Ä –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏\n                selected_pairs = image_label_pairs[:limit - total_copied]\n                \n                for img_path, label_path in selected_pairs:\n                    if total_copied >= limit:\n                        break\n                    \n                    # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞\n                    img_filename = os.path.basename(img_path)\n                    label_filename = os.path.basename(label_path)\n                    \n                    # –°–æ–∑–¥–∞–Ω–∏–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏–º–µ–Ω –¥–ª—è –æ—Ç–ª–∞–¥–æ—á–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n                    dst_img = os.path.join(debug_train_images, f\"{total_copied:06d}_{img_filename}\")\n                    dst_label = os.path.join(debug_train_labels, f\"{total_copied:06d}_{label_filename}\")\n                    \n                    # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤\n                    try:\n                        shutil.copy2(img_path, dst_img)\n                        shutil.copy2(label_path, dst_label)\n                        total_copied += 1\n                    except Exception as e:\n                        logger.warning(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è {img_path}: {e}\")\n                        continue\n                        \n            else:  # –ü–ª–æ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ (—Å—Ç–∞—Ä—ã–π –∫–æ–¥)\n                logger.info(f\"üìÑ –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –ø–ª–æ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤ {dataset_path}\")\n                \n                # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n                images = [f for f in os.listdir(images_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n                images = sorted(images)[:min(limit - total_copied, len(images))]\n                \n                for img_file in images:\n                    if total_copied >= limit:\n                        break\n                        \n                    # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n                    src_img = os.path.join(images_dir, img_file)\n                    dst_img = os.path.join(debug_train_images, f\"{total_copied:06d}_{img_file}\")\n                    \n                    try:\n                        shutil.copy2(src_img, dst_img)\n                        \n                        # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π –º–µ—Ç–∫–∏\n                        label_file = os.path.splitext(img_file)[0] + '.txt'\n                        src_label = os.path.join(labels_dir, label_file)\n                        dst_label = os.path.join(debug_train_labels, f\"{total_copied:06d}_{label_file}\")\n                        \n                        if os.path.exists(src_label):\n                            shutil.copy2(src_label, dst_label)\n                        \n                        total_copied += 1\n                        \n                    except Exception as e:\n                        logger.warning(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è {img_file}: {e}\")\n                        continue\n                        \n        except Exception as e:\n            logger.error(f\"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ {dataset_path}: {e}\")\n            continue\n    \n    logger.info(f\"‚úÖ –°–æ–∑–¥–∞–Ω–∞ debug —Å—Ç—Ä—É–∫—Ç—É—Ä–∞: {total_copied} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ {debug_train_dir}\")\n    return debug_train_dir\n\ndef prepare_debug_dataset(train_paths: List[str], limit: int = 800) -> List[str]:\n    \"\"\"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏\"\"\"\n    if not IS_DEBUG:\n        return train_paths\n    \n    logger.info(f\"üêõ –†–µ–∂–∏–º –æ—Ç–ª–∞–¥–∫–∏: –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–æ {limit} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\")\n    \n    # –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã\n    debug_dir = create_debug_dataset_structure(train_paths, limit)\n    if debug_dir:\n        return [debug_dir]\n    \n    # Fallback –∫ —Å—Ç–∞—Ä–æ–π –ª–æ–≥–∏–∫–µ –µ—Å–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –Ω–µ —É–¥–∞–ª–æ—Å—å\n    debug_train_paths = []\n    total_images = 0\n    \n    for train_path in train_paths:\n        if isinstance(train_path, str):\n            images_dir = train_path\n        else:\n            continue\n            \n        if os.path.exists(images_dir):\n            images = [f for f in os.listdir(images_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n            images = sorted(images)[:min(limit - total_images, len(images))]\n            \n            if images:\n                debug_train_paths.append(images_dir)\n                total_images += len(images)\n                logger.info(f\"üìä –î–æ–±–∞–≤–ª–µ–Ω–æ {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ {images_dir}\")\n            \n            if total_images >= limit:\n                break\n    \n    logger.info(f\"üéØ –ò—Ç–æ–≥–æ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏: {total_images} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\")\n    return debug_train_paths\n\ndef get_dataset_statistics(dataset_path: str) -> Dict:\n    \"\"\"–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã\"\"\"\n    stats = {\n        'images_count': 0,\n        'labels_count': 0,\n        'classes_distribution': {},\n        'image_sizes': [],\n        'exists': False,\n        'structure_type': 'unknown'\n    }\n    \n    if not os.path.exists(dataset_path):\n        return stats\n    \n    stats['exists'] = True\n    images_dir = os.path.join(dataset_path, 'images')\n    labels_dir = os.path.join(dataset_path, 'labels')\n    \n    if not (os.path.exists(images_dir) and os.path.exists(labels_dir)):\n        return stats\n    \n    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É\n    try:\n        subdirs = [d for d in os.listdir(images_dir) \n                  if os.path.isdir(os.path.join(images_dir, d))]\n        \n        if subdirs:  # –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞\n            stats['structure_type'] = 'hierarchical'\n            logger.info(f\"üìÅ –ê–Ω–∞–ª–∏–∑ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≤ {dataset_path}\")\n            \n            # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ collect_hierarchical_images –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞\n            image_label_pairs = collect_hierarchical_images(dataset_path)\n            stats['images_count'] = len(image_label_pairs)\n            stats['labels_count'] = len(image_label_pairs)\n            \n            # –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Å–æ–≤ –∏–∑ –º–µ—Ç–æ–∫\n            class_counts = {}\n            for _, label_path in image_label_pairs:\n                try:\n                    with open(label_path, 'r') as f:\n                        lines = f.readlines()\n                        for line in lines:\n                            if line.strip():\n                                class_id = int(line.split()[0])\n                                class_counts[class_id] = class_counts.get(class_id, 0) + 1\n                except Exception as e:\n                    continue\n            \n            stats['classes_distribution'] = class_counts\n            \n        else:  # –ü–ª–æ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞\n            stats['structure_type'] = 'flat'\n            logger.info(f\"üìÑ –ê–Ω–∞–ª–∏–∑ –ø–ª–æ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≤ {dataset_path}\")\n            \n            # –ü–æ–¥—Å—á–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n            try:\n                image_files = [f for f in os.listdir(images_dir) \n                              if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n                stats['images_count'] = len(image_files)\n                \n                # –ü–æ–¥—Å—á–µ—Ç –º–µ—Ç–æ–∫\n                label_files = [f for f in os.listdir(labels_dir) \n                              if f.lower().endswith('.txt')]\n                stats['labels_count'] = len(label_files)\n                \n                # –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Å–æ–≤\n                class_counts = {}\n                for label_file in label_files:\n                    label_path = os.path.join(labels_dir, label_file)\n                    try:\n                        with open(label_path, 'r') as f:\n                            lines = f.readlines()\n                            for line in lines:\n                                if line.strip():\n                                    class_id = int(line.split()[0])\n                                    class_counts[class_id] = class_counts.get(class_id, 0) + 1\n                    except Exception as e:\n                        continue\n                \n                stats['classes_distribution'] = class_counts\n                \n            except Exception as e:\n                logger.error(f\"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø–ª–æ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã {dataset_path}: {e}\")\n                \n    except Exception as e:\n        logger.error(f\"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–∞ {dataset_path}: {e}\")\n    \n    return stats\n\ndef validate_on_private_dataset(model_path: str, private_dataset_path: str = VAL_DATASET_PRIVATE) -> Dict:\n    \"\"\"–í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –Ω–∞ –ø—Ä–∏–≤–∞—Ç–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ\"\"\"\n    logger.info(\"üîí –ù–∞—á–∞–ª–æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –Ω–∞ –ø—Ä–∏–≤–∞—Ç–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ...\")\n    \n    if not os.path.exists(model_path):\n        logger.error(f\"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}\")\n        return {'error': 'Model not found'}\n    \n    if not os.path.exists(private_dataset_path):\n        logger.error(f\"‚ùå –ü—Ä–∏–≤–∞—Ç–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {private_dataset_path}\")\n        return {'error': 'Private dataset not found'}\n    \n    try:\n        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏\n        model = YOLO(model_path)\n        logger.info(f\"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_path}\")\n        \n        # –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ YAML –¥–ª—è –ø—Ä–∏–≤–∞—Ç–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n        private_yaml_config = {\n            'path': os.path.dirname(private_dataset_path),\n            'train': os.path.join(TRAIN_DATASET_1, 'images'),  # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π 'train' –∫–ª—é—á\n            'val': os.path.join(private_dataset_path, 'images'),\n            'nc': 1,\n            'names': ['person']\n        }\n        \n        private_yaml_path = 'private_validation.yaml'\n        with open(private_yaml_path, 'w', encoding='utf-8') as f:\n            yaml.dump(private_yaml_config, f, default_flow_style=False, allow_unicode=True)\n        \n        # –í–∞–ª–∏–¥–∞—Ü–∏—è\n        logger.info(\"üîç –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏...\")\n        val_results = model.val(\n            data=private_yaml_path,\n            imgsz=640,\n            batch=16,\n            conf=0.25,\n            iou=0.7,\n            device='auto',\n            plots=True,\n            save_json=True\n        )\n        \n        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n        results_dict = {\n            'map50': float(val_results.box.map50) if hasattr(val_results, 'box') else 0.0,\n            'map50_95': float(val_results.box.map) if hasattr(val_results, 'box') else 0.0,\n            'precision': float(val_results.box.mp) if hasattr(val_results, 'box') else 0.0,\n            'recall': float(val_results.box.mr) if hasattr(val_results, 'box') else 0.0,\n            'f1_score': 0.0,\n            'custom_metric_q': 0.0\n        }\n        \n        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ F1-score\n        if results_dict['precision'] > 0 and results_dict['recall'] > 0:\n            results_dict['f1_score'] = 2 * (results_dict['precision'] * results_dict['recall']) / (results_dict['precision'] + results_dict['recall'])\n        \n        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏ Q\n        logger.info(\"üéØ –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏ Q...\")\n        try:\n            predictions, ground_truths = get_model_predictions_as_masks(model, private_dataset_path)\n            if len(predictions) > 0 and len(ground_truths) > 0:\n                custom_q = calculate_custom_metric(predictions, ground_truths, beta=1.0)\n                results_dict['custom_metric_q'] = custom_q\n            else:\n                logger.warning(\"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –º–∞—Å–∫–∏ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏\")\n        except Exception as e:\n            logger.error(f\"‚ùå –û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏: {e}\")\n        \n        logger.info(\"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –Ω–∞ –ø—Ä–∏–≤–∞—Ç–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ:\")\n        logger.info(f\"   mAP@0.5: {results_dict['map50']:.4f}\")\n        logger.info(f\"   mAP@0.5:0.95: {results_dict['map50_95']:.4f}\")\n        logger.info(f\"   Precision: {results_dict['precision']:.4f}\")\n        logger.info(f\"   Recall: {results_dict['recall']:.4f}\")\n        logger.info(f\"   F1-Score: {results_dict['f1_score']:.4f}\")\n        logger.info(f\"   üéØ –ö–∞—Å—Ç–æ–º–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ Q: {results_dict['custom_metric_q']:.4f}\")\n        \n        # –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞\n        try:\n            os.remove(private_yaml_path)\n        except:\n            pass\n        \n        return results_dict\n        \n    except Exception as e:\n        logger.error(f\"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –Ω–∞ –ø—Ä–∏–≤–∞—Ç–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ: {e}\")\n        import traceback\n        traceback.print_exc()\n        return {'error': str(e)}\n\ndef calculate_custom_metric(predictions: List[np.ndarray], ground_truths: List[np.ndarray], beta: float = 1.0) -> float:\n    \"\"\"\n    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏ —Å–æ–≥–ª–∞—Å–Ω–æ —Ñ–æ—Ä–º—É–ª–µ (1)\n    \n    Args:\n        predictions: –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –º–∞—Å–æ–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n        ground_truths: –°–ø–∏—Å–æ–∫ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –º–∞—Å–æ–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è  \n        beta: –ü–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è F-beta –º–µ—Ä—ã (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1.0)\n        \n    Returns:\n        float: –ó–Ω–∞—á–µ–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏ Q\n    \"\"\"\n    # –ü–æ—Ä–æ–≥–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –æ—Ç 0.3 –¥–æ 0.93 —Å —à–∞–≥–æ–º 0.07\n    thresholds = np.arange(0.3, 0.94, 0.07)\n    n_thresholds = len(thresholds)\n    \n    logger.info(f\"üìä –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è {n_thresholds} –ø–æ—Ä–æ–≥–æ–≤: {thresholds}\")\n    \n    total_f_beta = 0.0\n    \n    for threshold in thresholds:\n        f_beta_t = calculate_f_beta_for_threshold(predictions, ground_truths, threshold, beta)\n        total_f_beta += f_beta_t\n        logger.info(f\"   –ü–æ—Ä–æ–≥ {threshold:.2f}: F-beta = {f_beta_t:.4f}\")\n    \n    # –§–∏–Ω–∞–ª—å–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ - —Å—Ä–µ–¥–Ω–µ–µ –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–æ–µ\n    custom_metric = total_f_beta / n_thresholds\n    \n    logger.info(f\"üéØ –ò—Ç–æ–≥–æ–≤–∞—è –∫–∞—Å—Ç–æ–º–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ Q = {custom_metric:.4f}\")\n    return custom_metric\n\n\ndef calculate_f_beta_for_threshold(predictions: List[np.ndarray], ground_truths: List[np.ndarray], \n                                  threshold: float, beta: float = 1.0) -> float:\n    \"\"\"\n    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ F-beta –º–µ—Ä—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞ IoU\n    \n    Args:\n        predictions: –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –º–∞—Å–æ–∫\n        ground_truths: –°–ø–∏—Å–æ–∫ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –º–∞—Å–æ–∫\n        threshold: –ü–æ—Ä–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ IoU\n        beta: –ü–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è F-beta –º–µ—Ä—ã\n        \n    Returns:\n        float: –ó–Ω–∞—á–µ–Ω–∏–µ F-beta –º–µ—Ä—ã –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞\n    \"\"\"\n    total_tp = 0\n    total_fp = 0\n    total_fn = 0\n    \n    for pred_mask, gt_mask in zip(predictions, ground_truths):\n        tp, fp, fn = calculate_tp_fp_fn_for_image(pred_mask, gt_mask, threshold)\n        total_tp += tp\n        total_fp += fp\n        total_fn += fn\n    \n    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ F-beta –º–µ—Ä—ã –ø–æ —Ñ–æ—Ä–º—É–ª–µ (2)\n    if total_tp + total_fp == 0 and total_tp + total_fn == 0:\n        return 0.0\n    \n    if total_tp + total_fp == 0:\n        precision = 0.0\n    else:\n        precision = total_tp / (total_tp + total_fp)\n    \n    if total_tp + total_fn == 0:\n        recall = 0.0\n    else:\n        recall = total_tp / (total_tp + total_fn)\n    \n    if precision + recall == 0:\n        return 0.0\n    \n    # F-beta —Ñ–æ—Ä–º—É–ª–∞ —Å beta = 1\n    f_beta = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n    \n    return f_beta\n\n\ndef calculate_tp_fp_fn_for_image(pred_mask: np.ndarray, gt_mask: np.ndarray, threshold: float) -> Tuple[int, int, int]:\n    \"\"\"\n    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ TP, FP, FN –¥–ª—è –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–Ω–æ –∞–ª–≥–æ—Ä–∏—Ç–º—É\n    \n    Args:\n        pred_mask: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–∞—Å–∫–∞ (2D –º–∞—Å—Å–∏–≤ —Å 0 –∏ 1)\n        gt_mask: –ò—Å—Ç–∏–Ω–Ω–∞—è –º–∞—Å–∫–∞ (2D –º–∞—Å—Å–∏–≤ —Å 0 –∏ 1)\n        threshold: –ü–æ—Ä–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ IoU\n        \n    Returns:\n        Tuple[int, int, int]: TP, FP, FN\n    \"\"\"\n    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–±–ª–∞—Å—Ç–µ–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ —Ä–∞–∑–º–µ—Ç–∫–∏\n    pred_regions = extract_regions(pred_mask)\n    gt_regions = extract_regions(gt_mask)\n    \n    if len(pred_regions) == 0 and len(gt_regions) == 0:\n        return 0, 0, 0\n    \n    if len(pred_regions) == 0:\n        return 0, 0, len(gt_regions)\n    \n    if len(gt_regions) == 0:\n        return 0, len(pred_regions), 0\n    \n    # –°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è IoU\n    iou_matrix = create_iou_matrix(pred_regions, gt_regions)\n    \n    # –ü–æ–¥—Å—á–µ—Ç TP, FP, FN —Å–æ–≥–ª–∞—Å–Ω–æ –∞–ª–≥–æ—Ä–∏—Ç–º—É\n    tp, fp, fn = count_tp_fp_fn_from_matrix(iou_matrix, threshold)\n    \n    return tp, fp, fn\n\n\ndef extract_regions(mask: np.ndarray) -> List[np.ndarray]:\n    \"\"\"\n    –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–≤—è–∑–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π –∏–∑ –±–∏–Ω–∞—Ä–Ω–æ–π –º–∞—Å–∫–∏\n    \n    Args:\n        mask: –ë–∏–Ω–∞—Ä–Ω–∞—è –º–∞—Å–∫–∞ (2D –º–∞—Å—Å–∏–≤ —Å 0 –∏ 1)\n        \n    Returns:\n        List[np.ndarray]: –°–ø–∏—Å–æ–∫ –º–∞—Å–æ–∫ –¥–ª—è –∫–∞–∂–¥–æ–π —Å–≤—è–∑–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏\n    \"\"\"\n    # –ü–æ–∏—Å–∫ —Å–≤—è–∑–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç\n    labeled_mask, num_features = ndimage.label(mask)\n    \n    regions = []\n    for i in range(1, num_features + 1):\n        region_mask = (labeled_mask == i).astype(np.uint8)\n        regions.append(region_mask)\n    \n    return regions\n\n\ndef get_model_predictions_as_masks(model, dataset_path: str, img_size: int = 640, conf_threshold: float = 0.25) -> Tuple[List[np.ndarray], List[np.ndarray]]:\n    \"\"\"\n    –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–∏ –≤ –≤–∏–¥–µ –±–∏–Ω–∞—Ä–Ω—ã—Ö –º–∞—Å–æ–∫\n    \n    Args:\n        model: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å YOLO\n        dataset_path: –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É\n        img_size: –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞\n        conf_threshold: –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–π\n        \n    Returns:\n        Tuple[List[np.ndarray], List[np.ndarray]]: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–∞—Å–∫–∏ –∏ –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–∞—Å–∫–∏\n    \"\"\"\n    images_dir = os.path.join(dataset_path, 'images')\n    labels_dir = os.path.join(dataset_path, 'labels')\n    \n    if not os.path.exists(images_dir) or not os.path.exists(labels_dir):\n        logger.error(f\"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã –ø–∞–ø–∫–∏ images –∏–ª–∏ labels –≤ {dataset_path}\")\n        return [], []\n    \n    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞—Ç–∞—Å–µ—Ç–∞\n    subdirs = [d for d in os.listdir(images_dir) if os.path.isdir(os.path.join(images_dir, d))]\n    \n    if subdirs:  # –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞\n        logger.info(f\"üìÅ –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å {len(subdirs)} –ø–æ–¥–ø–∞–ø–∫–∞–º–∏\")\n        image_label_pairs = collect_hierarchical_images(dataset_path)\n        \n        if not image_label_pairs:\n            logger.error(f\"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–∞—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-—Ä–∞–∑–º–µ—Ç–∫–∞ –≤ {dataset_path}\")\n            return [], []\n            \n        predictions = []\n        ground_truths = []\n        \n        logger.info(f\"üîç –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(image_label_pairs)} –ø–∞—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-—Ä–∞–∑–º–µ—Ç–∫–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –º–∞—Å–æ–∫...\")\n        \n        for i, (img_path, label_path) in enumerate(image_label_pairs):\n            if i % 50 == 0:\n                logger.info(f\"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i}/{len(image_label_pairs)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\")\n            \n            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n            try:\n                image = Image.open(img_path)\n                img_width, img_height = image.size\n            except Exception as e:\n                logger.warning(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {img_path}: {e}\")\n                continue\n            \n            # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏\n            try:\n                results = model.predict(img_path, imgsz=img_size, conf=conf_threshold, verbose=False)\n                pred_mask = create_mask_from_yolo_results(results[0], img_width, img_height)\n            except Exception as e:\n                logger.warning(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è {img_path}: {e}\")\n                pred_mask = np.zeros((img_height, img_width), dtype=np.uint8)\n            \n            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ç–∏–Ω–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏\n            try:\n                gt_mask = create_mask_from_yolo_labels(label_path, img_width, img_height)\n            except Exception as e:\n                logger.warning(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–∞–∑–º–µ—Ç–∫–∏ –¥–ª—è {label_path}: {e}\")\n                gt_mask = np.zeros((img_height, img_width), dtype=np.uint8)\n            \n            predictions.append(pred_mask)\n            ground_truths.append(gt_mask)\n            \n    else:  # –ü–ª–æ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞\n        logger.info(f\"üìÑ –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –ø–ª–æ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞\")\n        image_files = [f for f in os.listdir(images_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n        \n        predictions = []\n        ground_truths = []\n        \n        logger.info(f\"üîç –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(image_files)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –º–∞—Å–æ–∫...\")\n        \n        for i, img_file in enumerate(image_files):\n            if i % 50 == 0:\n                logger.info(f\"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i}/{len(image_files)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\")\n                \n            img_path = os.path.join(images_dir, img_file)\n            label_path = os.path.join(labels_dir, img_file.rsplit('.', 1)[0] + '.txt')\n            \n            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n            try:\n                image = Image.open(img_path)\n                img_width, img_height = image.size\n            except Exception as e:\n                logger.warning(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {img_file}: {e}\")\n                continue\n            \n            # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏\n            try:\n                results = model.predict(img_path, imgsz=img_size, conf=conf_threshold, verbose=False)\n                pred_mask = create_mask_from_yolo_results(results[0], img_width, img_height)\n            except Exception as e:\n                logger.warning(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è {img_file}: {e}\")\n                pred_mask = np.zeros((img_height, img_width), dtype=np.uint8)\n            \n            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ç–∏–Ω–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏\n            try:\n                gt_mask = create_mask_from_yolo_labels(label_path, img_width, img_height)\n            except Exception as e:\n                logger.warning(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–∞–∑–º–µ—Ç–∫–∏ –¥–ª—è {img_file}: {e}\")\n                gt_mask = np.zeros((img_height, img_width), dtype=np.uint8)\n            \n            predictions.append(pred_mask)\n            ground_truths.append(gt_mask)\n    \n    logger.info(f\"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(predictions)} –ø–∞—Ä –º–∞—Å–æ–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏\")\n    return predictions, ground_truths\n\n\ndef create_mask_from_yolo_results(results, img_width: int, img_height: int) -> np.ndarray:\n    \"\"\"\n    –°–æ–∑–¥–∞–Ω–∏–µ –±–∏–Ω–∞—Ä–Ω–æ–π –º–∞—Å–∫–∏ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ YOLO\n    \n    Args:\n        results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ—Ç–µ–∫—Ü–∏–∏ YOLO\n        img_width: –®–∏—Ä–∏–Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n        img_height: –í—ã—Å–æ—Ç–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n        \n    Returns:\n        np.ndarray: –ë–∏–Ω–∞—Ä–Ω–∞—è –º–∞—Å–∫–∞\n    \"\"\"\n    mask = np.zeros((img_height, img_width), dtype=np.uint8)\n    \n    if results.boxes is not None and len(results.boxes) > 0:\n        boxes = results.boxes.xyxy.cpu().numpy()  # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã bbox –≤ —Ñ–æ—Ä–º–∞—Ç–µ x1,y1,x2,y2\n        \n        for box in boxes:\n            x1, y1, x2, y2 = map(int, box[:4])\n            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —Ä–∞–∑–º–µ—Ä–∞–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n            x1 = max(0, min(x1, img_width-1))\n            y1 = max(0, min(y1, img_height-1))\n            x2 = max(0, min(x2, img_width-1))\n            y2 = max(0, min(y2, img_height-1))\n            \n            # –ó–∞–ø–æ–ª–Ω—è–µ–º –æ–±–ª–∞—Å—Ç—å bbox –µ–¥–∏–Ω–∏—Ü–∞–º–∏\n            mask[y1:y2+1, x1:x2+1] = 1\n    \n    return mask\n\n\ndef create_mask_from_yolo_labels(label_path: str, img_width: int, img_height: int) -> np.ndarray:\n    \"\"\"\n    –°–æ–∑–¥–∞–Ω–∏–µ –±–∏–Ω–∞—Ä–Ω–æ–π –º–∞—Å–∫–∏ –∏–∑ YOLO —Ä–∞–∑–º–µ—Ç–∫–∏\n    \n    Args:\n        label_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Ä–∞–∑–º–µ—Ç–∫–∏\n        img_width: –®–∏—Ä–∏–Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n        img_height: –í—ã—Å–æ—Ç–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n        \n    Returns:\n        np.ndarray: –ë–∏–Ω–∞—Ä–Ω–∞—è –º–∞—Å–∫–∞\n    \"\"\"\n    mask = np.zeros((img_height, img_width), dtype=np.uint8)\n    \n    if not os.path.exists(label_path):\n        return mask\n    \n    try:\n        with open(label_path, 'r') as f:\n            lines = f.readlines()\n        \n        for line in lines:\n            parts = line.strip().split()\n            if len(parts) >= 5:\n                # YOLO —Ñ–æ—Ä–º–∞—Ç: class_id center_x center_y width height (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã)\n                center_x = float(parts[1]) * img_width\n                center_y = float(parts[2]) * img_height\n                width = float(parts[3]) * img_width\n                height = float(parts[4]) * img_height\n                \n                # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã bbox\n                x1 = int(center_x - width/2)\n                y1 = int(center_y - height/2)\n                x2 = int(center_x + width/2)\n                y2 = int(center_y + height/2)\n                \n                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —Ä–∞–∑–º–µ—Ä–∞–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n                x1 = max(0, min(x1, img_width-1))\n                y1 = max(0, min(y1, img_height-1))\n                x2 = max(0, min(x2, img_width-1))\n                y2 = max(0, min(y2, img_height-1))\n                \n                # –ó–∞–ø–æ–ª–Ω—è–µ–º –æ–±–ª–∞—Å—Ç—å bbox –µ–¥–∏–Ω–∏—Ü–∞–º–∏\n                mask[y1:y2+1, x1:x2+1] = 1\n    \n    except Exception as e:\n        logger.warning(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ç–∫–∏ {label_path}: {e}\")\n    \n    return mask\n\n\ndef calculate_iou(region_a: np.ndarray, region_b: np.ndarray) -> float:\n    \"\"\"\n    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ IoU –º–µ–∂–¥—É –¥–≤—É–º—è –æ–±–ª–∞—Å—Ç—è–º–∏ –ø–æ —Ñ–æ—Ä–º—É–ª–µ (3)\n    \n    Args:\n        region_a: –ü–µ—Ä–≤–∞—è –æ–±–ª–∞—Å—Ç—å (–±–∏–Ω–∞—Ä–Ω–∞—è –º–∞—Å–∫–∞)\n        region_b: –í—Ç–æ—Ä–∞—è –æ–±–ª–∞—Å—Ç—å (–±–∏–Ω–∞—Ä–Ω–∞—è –º–∞—Å–∫–∞)\n        \n    Returns:\n        float: –ó–Ω–∞—á–µ–Ω–∏–µ IoU\n    \"\"\"\n    intersection = np.logical_and(region_a, region_b).sum()\n    union = np.logical_or(region_a, region_b).sum()\n    \n    if union == 0:\n        return 0.0\n    \n    return intersection / union\n\n\ndef create_iou_matrix(pred_regions: List[np.ndarray], gt_regions: List[np.ndarray]) -> np.ndarray:\n    \"\"\"\n    –°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã IoU –º–µ–∂–¥—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º–∏ –∏ –∏—Å—Ç–∏–Ω–Ω—ã–º–∏ –æ–±–ª–∞—Å—Ç—è–º–∏\n    \n    Args:\n        pred_regions: –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π\n        gt_regions: –°–ø–∏—Å–æ–∫ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π\n        \n    Returns:\n        np.ndarray: –ú–∞—Ç—Ä–∏—Ü–∞ IoU —Ä–∞–∑–º–µ—Ä–æ–º [len(pred_regions), len(gt_regions)]\n    \"\"\"\n    iou_matrix = np.zeros((len(pred_regions), len(gt_regions)))\n    \n    for i, pred_region in enumerate(pred_regions):\n        for j, gt_region in enumerate(gt_regions):\n            iou_matrix[i, j] = calculate_iou(pred_region, gt_region)\n    \n    return iou_matrix\n\n\ndef count_tp_fp_fn_from_matrix(iou_matrix: np.ndarray, threshold: float) -> Tuple[int, int, int]:\n    \"\"\"\n    –ü–æ–¥—Å—á–µ—Ç TP, FP, FN –∏–∑ –º–∞—Ç—Ä–∏—Ü—ã IoU —Å–æ–≥–ª–∞—Å–Ω–æ –æ–ø–∏—Å–∞–Ω–Ω–æ–º—É –∞–ª–≥–æ—Ä–∏—Ç–º—É\n    \n    Args:\n        iou_matrix: –ú–∞—Ç—Ä–∏—Ü–∞ IoU\n        threshold: –ü–æ—Ä–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ\n        \n    Returns:\n        Tuple[int, int, int]: TP, FP, FN\n    \"\"\"\n    tp = 0\n    matrix_copy = iou_matrix.copy()\n    \n    while matrix_copy.size > 0:\n        # –ü–æ–∏—Å–∫ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞\n        max_val = np.max(matrix_copy)\n        \n        if max_val >= threshold:\n            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º TP\n            tp += 1\n            \n            # –ù–∞—Ö–æ–¥–∏–º –ø–æ–∑–∏—Ü–∏—é –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞\n            max_pos = np.unravel_index(np.argmax(matrix_copy), matrix_copy.shape)\n            row_idx, col_idx = max_pos\n            \n            # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫—É –∏ —Å—Ç–æ–ª–±–µ—Ü\n            matrix_copy = np.delete(matrix_copy, row_idx, axis=0)\n            matrix_copy = np.delete(matrix_copy, col_idx, axis=1)\n        else:\n            # –ü—Ä–µ—Ä—ã–≤–∞–µ–º –ø—Ä–æ—Ü–µ–¥—É—Ä—É\n            break\n    \n    # FN = –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è —Å—Ç–æ–ª–±—Ü–æ–≤ (–∏—Å—Ç–∏–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤)\n    fn = matrix_copy.shape[1] if matrix_copy.size > 0 else 0\n    \n    # FP = –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è —Å—Ç—Ä–æ–∫ (–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤)\n    fp = matrix_copy.shape[0] if matrix_copy.size > 0 else 0\n    \n    return tp, fp, fn\n\n\ndef cleanup_debug_files():\n    \"\"\"–û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö debug —Ñ–∞–π–ª–æ–≤\"\"\"\n    debug_base_dir = '/kaggle/working'\n    \n    if os.path.exists(debug_base_dir):\n        try:\n            shutil.rmtree(debug_base_dir)\n            logger.info(\"üßπ –í—Ä–µ–º–µ–Ω–Ω—ã–µ debug —Ñ–∞–π–ª—ã –æ—á–∏—â–µ–Ω—ã\")\n        except Exception as e:\n            logger.warning(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ debug —Ñ–∞–π–ª–æ–≤: {e}\")\n    \n    # –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö YAML —Ñ–∞–π–ª–æ–≤\n    temp_yaml_files = ['kaggle_combined_data.yaml', 'combined_data.yaml', 'private_validation.yaml']\n    for yaml_file in temp_yaml_files:\n        if os.path.exists(yaml_file):\n            try:\n                os.remove(yaml_file)\n            except:\n                pass\n\ndef analyze_all_datasets() -> Dict:\n    \"\"\"–ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\"\"\"\n    logger.info(\"üìä –ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤...\")\n    \n    datasets = {\n        'train_dataset_1': get_dataset_statistics(TRAIN_DATASET_1),\n        'train_dataset_2': get_dataset_statistics(TRAIN_DATASET_2),\n        'val_dataset_public': get_dataset_statistics(VAL_DATASET_PUBLIC),\n        'val_dataset_private': get_dataset_statistics(VAL_DATASET_PRIVATE)\n    }\n    \n    total_train_images = 0\n    total_train_labels = 0\n    \n    for name, stats in datasets.items():\n        if stats['exists']:\n            structure_info = f\" ({stats['structure_type']})\" if 'structure_type' in stats else \"\"\n            logger.info(f\"‚úÖ {name}{structure_info}: {stats['images_count']} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, {stats['labels_count']} –º–µ—Ç–æ–∫\")\n            if 'train' in name:\n                total_train_images += stats['images_count']\n                total_train_labels += stats['labels_count']\n            if stats['classes_distribution']:\n                logger.info(f\"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {stats['classes_distribution']}\")\n        else:\n            logger.warning(f\"‚ùå {name}: –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω\")\n    \n    logger.info(f\"üéØ –ò—Ç–æ–≥–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {total_train_images} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, {total_train_labels} –º–µ—Ç–æ–∫\")\n    \n    return datasets\n\nclass AdvancedDynamicRoutingModule(nn.Module):\n    \"\"\"–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –º–æ–¥—É–ª—å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ —Å –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–µ–π –¥–ª—è YOLOv13\"\"\"\n    \n    def __init__(self, in_channels: int, out_channels: int, num_routes: int = 3, \n                 routing_iterations: int = 2, capsule_dim: int = 8):\n        super().__init__()\n        self.num_routes = num_routes\n        self.routing_iterations = routing_iterations\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.capsule_dim = capsule_dim\n        \n        # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–µ –∫–∞–ø—Å—É–ª—ã –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏\n        self.primary_capsules = nn.ModuleList([\n            nn.Sequential(\n                nn.Conv2d(in_channels, out_channels // num_routes, 3, padding=1),\n                nn.BatchNorm2d(out_channels // num_routes),\n                nn.SiLU()\n            ) for _ in range(num_routes)\n        ])\n        \n        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤–µ—Å–æ–≤—ã–µ –º–∞—Ç—Ä–∏—Ü—ã\n        self.routing_weights = nn.Parameter(torch.randn(num_routes, capsule_dim, capsule_dim) * 0.1)\n        self.temperature = nn.Parameter(torch.ones(1))\n        \n        # UAV-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏\n        self.altitude_encoder = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(out_channels // num_routes, capsule_dim, 1),\n            nn.SiLU()\n        )\n        \n        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –º–∞—Ä—à—Ä—É—Ç–æ–≤\n        self.route_fusion = nn.Conv2d(out_channels, out_channels, 1)\n        self.norm = nn.BatchNorm2d(out_channels)\n        \n    def enhanced_squash(self, tensor: torch.Tensor, dim: int = -1, epsilon: float = 1e-8) -> torch.Tensor:\n        \"\"\"–£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–∂–∞—Ç–∏—è\"\"\"\n        squared_norm = (tensor ** 2).sum(dim=dim, keepdim=True)\n        scale = squared_norm / (1 + squared_norm + epsilon)\n        unit_vector = tensor / torch.sqrt(squared_norm + epsilon)\n        return scale * unit_vector\n    \n    def dynamic_routing(self, route_features: List[torch.Tensor]) -> torch.Tensor:\n        \"\"\"–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –º–µ–∂–¥—É –ø—É—Ç—è–º–∏\"\"\"\n        batch_size = route_features[0].size(0)\n        device = route_features[0].device\n        \n        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏\n        routing_logits = torch.zeros(batch_size, self.num_routes, device=device)\n        \n        for iteration in range(self.routing_iterations):\n            # Softmax –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ—Å–æ–≤\n            routing_weights = F.softmax(routing_logits / self.temperature, dim=1)\n            \n            # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ\n            weighted_features = []\n            for i, features in enumerate(route_features):\n                weight = routing_weights[:, i:i+1, None, None]\n                weighted_features.append(features * weight)\n            \n            combined = torch.stack(weighted_features, dim=1).sum(dim=1)\n            \n            if iteration < self.routing_iterations - 1:\n                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ª–æ–≥–∏—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏\n                for i, features in enumerate(route_features):\n                    agreement = F.cosine_similarity(\n                        features.flatten(1), combined.flatten(1), dim=1\n                    )\n                    routing_logits[:, i] += agreement\n        \n        return combined\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–∞—Ä—à—Ä—É—Ç—ã\n        route_outputs = []\n        for capsule in self.primary_capsules:\n            route_out = capsule(x)\n            route_outputs.append(route_out)\n        \n        # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è\n        routed_output = self.dynamic_routing(route_outputs)\n        \n        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–∞—Ä—à—Ä—É—Ç–æ–≤\n        all_routes = torch.cat(route_outputs, dim=1)\n        fused = self.route_fusion(all_routes)\n        \n        # –û—Å—Ç–∞—Ç–æ—á–Ω–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –º–∞—Ä—à—Ä—É—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –≤—ã—Ö–æ–¥–æ–º\n        if routed_output.size(1) == fused.size(1):\n            output = fused + 0.3 * routed_output\n        else:\n            output = fused\n        \n        return self.norm(output)\n\nclass EnhancedUAVAdaptiveBlock(nn.Module):\n    \"\"\"–£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –±–ª–æ–∫ –¥–ª—è UAV —Å –º–Ω–æ–≥–æ—Ñ–∞–∫—Ç–æ—Ä–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–µ–π\"\"\"\n    \n    def __init__(self, in_channels: int, out_channels: int, reduction_ratio: int = 16):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        \n        # –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–µ —Å–ª–æ–∏\n        self.conv1 = Conv(in_channels, out_channels, 3, 1)\n        self.conv2 = Conv(out_channels, out_channels, 3, 1)\n        \n        # –ö–∞–Ω–∞–ª—å–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ (—É–ø—Ä–æ—â–µ–Ω–Ω–æ–µ)\n        self.channel_attention = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(out_channels, max(out_channels // reduction_ratio, 1), 1),\n            nn.SiLU(),\n            nn.Conv2d(max(out_channels // reduction_ratio, 1), out_channels, 1),\n            nn.Sigmoid()\n        )\n        \n        # –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ\n        self.spatial_attention = nn.Sequential(\n            nn.Conv2d(2, 1, 7, padding=3),\n            nn.Sigmoid()\n        )\n        \n        # UAV-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏\n        self.altitude_adaptation = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(out_channels, out_channels, 1),\n            nn.Sigmoid()\n        )\n        \n        # –ü—Ä–æ–µ–∫—Ü–∏—è –¥–ª—è –æ—Å—Ç–∞—Ç–æ—á–Ω–æ–≥–æ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è\n        self.shortcut = nn.Identity() if in_channels == out_channels else Conv(in_channels, out_channels, 1, 1)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        residual = self.shortcut(x)\n        \n        # –û—Å–Ω–æ–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞\n        out = self.conv1(x)\n        out = self.conv2(out)\n        \n        # –ö–∞–Ω–∞–ª—å–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ\n        ca = self.channel_attention(out)\n        out = out * ca\n        \n        # –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ\n        avg_pool = torch.mean(out, dim=1, keepdim=True)\n        max_pool, _ = torch.max(out, dim=1, keepdim=True)\n        spatial_input = torch.cat([avg_pool, max_pool], dim=1)\n        sa = self.spatial_attention(spatial_input)\n        out = out * sa\n        \n        # UAV –∞–¥–∞–ø—Ç–∞—Ü–∏—è\n        altitude_att = self.altitude_adaptation(out)\n        out = out * altitude_att\n        \n        # –û—Å—Ç–∞—Ç–æ—á–Ω–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ\n        return out + residual\n\nclass YOLOv13DynamicUAV(nn.Module):\n    \"\"\"YOLOv13 —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–µ–π –¥–ª—è UAV\"\"\"\n    \n    def __init__(self, base_model, num_classes=1):\n        super().__init__()\n        self.base_model = base_model\n        self.num_classes = num_classes\n        \n        # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö –º–æ–¥—É–ª–µ–π –≤ backbone\n        self.dynamic_modules = nn.ModuleList([\n            AdvancedDynamicRoutingModule(64, 64),\n            AdvancedDynamicRoutingModule(128, 128),\n            AdvancedDynamicRoutingModule(256, 256),\n        ])\n        \n        # UAV –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –±–ª–æ–∫–∏\n        self.uav_blocks = nn.ModuleList([\n            EnhancedUAVAdaptiveBlock(64, 64),\n            EnhancedUAVAdaptiveBlock(128, 128),\n            EnhancedUAVAdaptiveBlock(256, 256),\n        ])\n        \n    def forward(self, x):\n        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é YOLO –º–æ–¥–µ–ª—å —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –Ω–∞—à–∏—Ö –º–æ–¥—É–ª–µ–π\n        return self.base_model(x)\n\ndef optimize_gpu_memory():\n    \"\"\"–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è GPU –ø–∞–º—è—Ç–∏ –¥–ª—è Kaggle\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        gc.collect()\n        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Ñ—Ä–∞–∫—Ü–∏–∏ –ø–∞–º—è—Ç–∏ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è OOM\n        torch.cuda.set_per_process_memory_fraction(0.8)\n        logger.info(\"üöÄ GPU –ø–∞–º—è—Ç—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞\")\n\ndef detect_device_kaggle():\n    \"\"\"–£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –¥–ª—è Kaggle\"\"\"\n    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è Kaggle\n    is_kaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE') is not None\n    kaggle_accelerator = os.environ.get('KAGGLE_ACCELERATOR', 'None')\n    \n    if is_kaggle:\n        logger.info(f\"üèÉ –ó–∞–ø—É—Å–∫ –≤ Kaggle, —É—Å–∫–æ—Ä–∏—Ç–µ–ª—å: {kaggle_accelerator}\")\n    \n    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ CUDA\n    if torch.cuda.is_available() and torch.cuda.device_count() > 0:\n        try:\n            # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ GPU\n            device = torch.device('cuda:0')\n            test_tensor = torch.randn(100, 100, device=device)\n            _ = test_tensor @ test_tensor.T\n            \n            gpu_name = torch.cuda.get_device_name(0)\n            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n            \n            logger.info(f\"üî• –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPU: {gpu_name}\")\n            logger.info(f\"üíæ GPU –ø–∞–º—è—Ç—å: {gpu_memory:.2f} GB\")\n            \n            # –û—á–∏—Å—Ç–∫–∞\n            del test_tensor\n            torch.cuda.empty_cache()\n            \n            # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è GPU\n            optimize_gpu_memory()\n            \n            return 'cuda:0'\n            \n        except Exception as e:\n            logger.warning(f\"‚ö†Ô∏è GPU –¥–æ—Å—Ç—É–ø–µ–Ω, –Ω–æ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç: {e}\")\n            return 'cpu'\n    else:\n        logger.info(\"üíª –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU (GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)\")\n        return 'cpu'\n\ndef get_optimal_config_for_device(device: str) -> dict:\n    \"\"\"–ü–æ–ª—É—á–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞\"\"\"\n    if device.startswith('cuda'):\n        return {\n            'batch_size': 16,\n            'workers': 4,\n            'mixed_precision': True,\n            'cache': True,\n            'optimizer': 'AdamW',\n            'lr0': 0.001,\n            'epochs': 100\n        }\n    else:\n        return {\n            'batch_size': 4,\n            'workers': 2,\n            'mixed_precision': False,\n            'cache': False,\n            'optimizer': 'SGD',\n            'lr0': 0.01,\n            'epochs': 50\n        }\n\ndef create_yolov13_config(num_classes: int = 1) -> dict:\n    \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ YOLOv13 —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–µ–π\"\"\"\n    return {\n        'nc': num_classes,\n        'depth_multiple': 0.67,\n        'width_multiple': 0.75,\n        'anchors': 3,\n        \n        # Backbone —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö –º–æ–¥—É–ª–µ–π\n        'backbone': [\n            [-1, 1, 'Conv', [64, 6, 2, 2]],  # 0-P1/2\n            [-1, 1, 'Conv', [128, 3, 2]],    # 1-P2/4\n            [-1, 3, 'C2f', [128, True]],     # 2\n            [-1, 1, 'Conv', [256, 3, 2]],    # 3-P3/8\n            [-1, 6, 'C2f', [256, True]],     # 4\n            [-1, 1, 'Conv', [512, 3, 2]],    # 5-P4/16\n            [-1, 6, 'C2f', [512, True]],     # 6\n            [-1, 1, 'Conv', [1024, 3, 2]],   # 7-P5/32\n            [-1, 3, 'C2f', [1024, True]],    # 8\n            [-1, 1, 'SPPF', [1024, 5]],      # 9\n        ],\n        \n        # Head —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π –¥–ª—è UAV\n        'head': [\n            [-1, 1, 'nn.Upsample', [None, 2, 'nearest']],  # 10\n            [[-1, 6], 1, 'Concat', [1]],                    # 11\n            [-1, 3, 'C2f', [512]],                          # 12\n            [-1, 1, 'nn.Upsample', [None, 2, 'nearest']],   # 13\n            [[-1, 4], 1, 'Concat', [1]],                    # 14\n            [-1, 3, 'C2f', [256]],                          # 15\n            [-1, 1, 'Conv', [256, 3, 2]],                   # 16\n            [[-1, 12], 1, 'Concat', [1]],                   # 17\n            [-1, 3, 'C2f', [512]],                          # 18\n            [-1, 1, 'Conv', [512, 3, 2]],                   # 19\n            [[-1, 9], 1, 'Concat', [1]],                    # 20\n            [-1, 3, 'C2f', [1024]],                         # 21\n            [[15, 18, 21], 1, 'Detect', [num_classes]],     # 22 Detect\n        ]\n    }\n\ndef train_yolov13_dynamic_uav(\n    data_yaml: str = None,\n    epochs: int = 100,\n    batch_size: int = 16,\n    img_size: int = 640,\n    device: str = 'auto',\n    project: str = 'yolov13_uav_runs',\n    name: str = 'dynamic_routing_experiment',\n    save_dir: str = './models',\n    use_kaggle_datasets: bool = True,\n    **kwargs\n) -> Tuple[object, Dict]:\n    \"\"\"–û–±—É—á–µ–Ω–∏–µ YOLOv13 —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–µ–π –¥–ª—è UAV\n    \n    Args:\n        data_yaml: –ü—É—Ç—å –∫ YAML —Ñ–∞–π–ª—É —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –¥–∞—Ç–∞—Å–µ—Ç–∞ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è Kaggle –¥–∞—Ç–∞—Å–µ—Ç—ã)\n        epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è\n        batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞\n        img_size: –†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n        device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ('auto', 'cpu', 'cuda')\n        project: –ü–∞–ø–∫–∞ –ø—Ä–æ–µ–∫—Ç–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n        name: –ò–º—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞\n        save_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π\n        use_kaggle_datasets: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Kaggle –¥–∞—Ç–∞—Å–µ—Ç—ã –≤–º–µ—Å—Ç–æ data_yaml\n        **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n    \n    Returns:\n        Tuple[object, Dict]: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è\n    \"\"\"\n    \n    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞\n    if device == 'auto':\n        device = detect_device_kaggle()\n    \n    # –ü–æ–ª—É—á–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏\n    device_config = get_optimal_config_for_device(device)\n    \n    # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n    batch_size = min(batch_size, device_config['batch_size'])\n    epochs = min(epochs, device_config['epochs'])\n    \n    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π\n    os.makedirs(save_dir, exist_ok=True)\n    os.makedirs(project, exist_ok=True)\n    \n    logger.info(\"üöÅ YOLOv13 Dynamic UAV - –°–∏—Å—Ç–µ–º–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ª—é–¥–µ–π –¥–ª—è –ë–ü–õ–ê\")\n    logger.info(\"=\" * 65)\n    \n    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∂–∏–º–∞ –æ—Ç–ª–∞–¥–∫–∏\n    if IS_DEBUG:\n        epochs = 1\n        logger.info(\"üêõ –†–ï–ñ–ò–ú –û–¢–õ–ê–î–ö–ò –ê–ö–¢–ò–í–ï–ù\")\n        logger.info(f\"   –≠–ø–æ—Ö–∏: {epochs} (–ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏)\")\n        logger.info(f\"   –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 800\")\n        logger.info(f\"   –í–∞–ª–∏–¥–∞—Ü–∏—è: –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö\")\n    \n    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞\n    if use_kaggle_datasets or data_yaml is None:\n        logger.info(\"üìä –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Kaggle –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\")\n        \n        # –ê–Ω–∞–ª–∏–∑ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\n        datasets_analysis = analyze_all_datasets()\n        \n        # –°–æ–∑–¥–∞–Ω–∏–µ YAML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\n        yaml_path = create_combined_dataset_yaml('kaggle_combined_data.yaml')\n        if not yaml_path:\n            raise ValueError(\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å YAML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\")\n        \n        data_yaml = yaml_path\n        logger.info(f\"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞: {data_yaml}\")\n    \n    logger.info(f\"üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:\")\n    logger.info(f\"   –î–∞—Ç–∞—Å–µ—Ç: {data_yaml}\")\n    logger.info(f\"   –≠–ø–æ—Ö–∏: {epochs}\")\n    logger.info(f\"   –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {batch_size}\")\n    logger.info(f\"   –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {img_size}\")\n    logger.info(f\"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n    logger.info(f\"   –†–µ–∂–∏–º –æ—Ç–ª–∞–¥–∫–∏: {IS_DEBUG}\")\n    \n    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –∫—ç—à–∞\n    cache_dir = '/kaggle/working/cache'\n    os.makedirs(cache_dir, exist_ok=True)\n    logger.info(f\"üíΩ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∫—ç—à–∞: {cache_dir}\")\n    \n    # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è UAV\n    training_config = {\n        'data': data_yaml,\n        'epochs': epochs,\n        'batch': batch_size,\n        'imgsz': img_size,\n        'device': device,\n        'project': project,\n        'name': name,\n        'save': True,\n        'save_period': 5,\n        'patience': 20,\n        'workers': device_config['workers'],\n        'cache': 'disk',  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–∏—Å–∫–æ–≤–æ–≥–æ –∫—ç—à–∞\n        \n        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä\n        'optimizer': device_config['optimizer'],\n        'lr0': device_config['lr0'],\n        'lrf': 0.01,\n        'momentum': 0.937,\n        'weight_decay': 0.0005,\n        'warmup_epochs': 3,\n        'warmup_momentum': 0.8,\n        'warmup_bias_lr': 0.1,\n        'cos_lr': True,\n        \n        # UAV-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏\n        'hsv_h': 0.015,\n        'hsv_s': 0.7,\n        'hsv_v': 0.4,\n        'degrees': 15.0,\n        'translate': 0.1,\n        'scale': 0.5,\n        'shear': 2.0,\n        'perspective': 0.0,\n        'flipud': 0.0,\n        'fliplr': 0.5,\n        'mosaic': 1.0,\n        'mixup': 0.15,\n        'copy_paste': 0.3,\n        \n        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Å–∞ –ø–æ—Ç–µ—Ä—å –¥–ª—è –ª—é–¥–µ–π\n        'box': 7.5,\n        'cls': 0.5,\n        'dfl': 1.5,\n        \n        # NMS –ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n        'iou': 0.7,\n        'conf': 0.25,\n        \n        # –í–∞–ª–∏–¥–∞—Ü–∏—è\n        'val': True,\n        'plots': True,\n        'rect': False,\n        \n        **kwargs\n    }\n    \n    try:\n        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑–æ–≤–æ–π YOLO –º–æ–¥–µ–ª–∏\n        logger.info(\"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π YOLO –º–æ–¥–µ–ª–∏...\")\n        base_model = YOLO('yolo12n.pt')\n        \n        # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ YOLOv13\n        yolov13_config = create_yolov13_config(num_classes=1)\n        \n        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏\n        config_path = os.path.join(save_dir, 'yolov13_dynamic_uav.yaml')\n        with open(config_path, 'w') as f:\n            yaml.dump(yolov13_config, f, default_flow_style=False)\n        \n        logger.info(f\"üíæ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è YOLOv13 —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {config_path}\")\n        \n        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n        logger.info(\"üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è YOLOv13 Dynamic UAV...\")\n        start_time = time.time()\n        \n        results = base_model.train(**training_config)\n        \n        training_time = time.time() - start_time\n        logger.info(f\"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {training_time/60:.2f} –º–∏–Ω—É—Ç\")\n        \n        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏\n        best_model_path = os.path.join(save_dir, 'yolov13_dynamic_uav_best.pt')\n        \n        if hasattr(results, 'save_dir') and results.save_dir:\n            weights_dir = Path(results.save_dir) / 'weights'\n            best_path = weights_dir / 'best.pt'\n            \n            if best_path.exists():\n                shutil.copy2(best_path, best_model_path)\n                logger.info(f\"‚úÖ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {best_model_path}\")\n            else:\n                logger.warning(\"‚ö†Ô∏è –§–∞–π–ª best.pt –Ω–µ –Ω–∞–π–¥–µ–Ω\")\n        \n        # –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏\n        logger.info(\"üìä –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏...\")\n        try:\n            val_results = base_model.val()\n            if hasattr(val_results, 'box'):\n                logger.info(f\"üìà mAP50: {val_results.box.map50:.4f}\")\n                logger.info(f\"üìà mAP50-95: {val_results.box.map:.4f}\")\n        except Exception as e:\n            logger.warning(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}\")\n            val_results = None\n        \n        # –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ –ø—Ä–∏–≤–∞—Ç–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)\n        if os.path.exists(best_model_path):\n            logger.info(\"üîí –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ –ø—Ä–∏–≤–∞—Ç–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ...\")\n            private_val_results = validate_on_private_dataset(best_model_path)\n            if 'error' not in private_val_results:\n                logger.info(\"‚úÖ –ü—Ä–∏–≤–∞—Ç–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ\")\n            else:\n                logger.warning(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏–≤–∞—Ç–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {private_val_results['error']}\")\n        \n        # –≠–∫—Å–ø–æ—Ä—Ç –º–æ–¥–µ–ª–∏\n        logger.info(\"üì¶ –≠–∫—Å–ø–æ—Ä—Ç –º–æ–¥–µ–ª–∏...\")\n        export_formats = ['onnx']\n        if device.startswith('cuda'):\n            export_formats.append('torchscript')\n        \n        for fmt in export_formats:\n            try:\n                export_path = base_model.export(\n                    format=fmt,\n                    imgsz=img_size,\n                    optimize=True,\n                    half=device.startswith('cuda')\n                )\n                logger.info(f\"‚úÖ –ú–æ–¥–µ–ª—å —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ –≤ {fmt}: {export_path}\")\n            except Exception as e:\n                logger.warning(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ {fmt}: {e}\")\n        \n        # –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞\n        report = {\n            'model_path': best_model_path,\n            'config_path': config_path,\n            'training_config': training_config,\n            'training_time_minutes': training_time / 60,\n            'results': val_results.results_dict if val_results and hasattr(val_results, 'results_dict') else {},\n            'training_completed': True,\n            'device_used': device,\n            'final_epochs': epochs,\n            'final_batch_size': batch_size,\n            'yolov13_features': {\n                'dynamic_routing': True,\n                'uav_optimization': True,\n                'human_detection': True,\n                'multi_scale_features': True\n            }\n        }\n        \n        logger.info(\"\\nüéâ –û–±—É—á–µ–Ω–∏–µ YOLOv13 Dynamic UAV –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!\")\n        logger.info(f\"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {best_model_path}\")\n        logger.info(f\"üéØ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ª—é–¥–µ–π —Å UAV!\")\n        \n        return base_model, report\n        \n    except Exception as e:\n        logger.error(f\"‚ùå –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {e}\")\n        import traceback\n        traceback.print_exc()\n        \n        return None, {\n            'config': training_config,\n            'error': str(e),\n            'training_completed': False,\n            'device_used': device\n        }\n\ndef prepare_uav_dataset(dataset_path: str, output_path: str) -> str:\n    \"\"\"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è UAV –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ª—é–¥–µ–π\"\"\"\n    \n    # –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π\n    dirs = {\n        'train_img': os.path.join(output_path, \"train/images\"),\n        'train_lbl': os.path.join(output_path, \"train/labels\"),\n        'val_img': os.path.join(output_path, \"val/images\"),\n        'val_lbl': os.path.join(output_path, \"val/labels\")\n    }\n    \n    for dir_path in dirs.values():\n        os.makedirs(dir_path, exist_ok=True)\n    \n    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞\n    images_path = os.path.join(dataset_path, \"images\")\n    labels_path = os.path.join(dataset_path, \"labels\")\n    \n    if not os.path.exists(images_path):\n        logger.warning(f\"‚ö†Ô∏è –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {images_path}\")\n        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ data.yaml\n        return create_test_data_yaml(output_path)\n    \n    # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n    image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff')\n    image_files = [f for f in os.listdir(images_path) \n                  if f.lower().endswith(image_extensions)]\n    \n    if len(image_files) == 0:\n        logger.warning(\"‚ö†Ô∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã\")\n        return create_test_data_yaml(output_path)\n    \n    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val (80/20)\n    train_files, val_files = train_test_split(\n        image_files, test_size=0.2, random_state=42\n    )\n    \n    def copy_dataset_files(files, split_type):\n        copied_count = 0\n        for file in files:\n            # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n            img_src = os.path.join(images_path, file)\n            img_dst = os.path.join(dirs[f'{split_type}_img'], file)\n            \n            if os.path.exists(img_src):\n                shutil.copy2(img_src, img_dst)\n                copied_count += 1\n                \n                # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞ –º–µ—Ç–æ–∫\n                label_file = os.path.splitext(file)[0] + '.txt'\n                label_src = os.path.join(labels_path, label_file)\n                label_dst = os.path.join(dirs[f'{split_type}_lbl'], label_file)\n                \n                if os.path.exists(label_src):\n                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –º–µ—Ç–æ–∫ –¥–ª—è –∫–ª–∞—Å—Å–∞ \"—á–µ–ª–æ–≤–µ–∫\"\n                    with open(label_src, 'r') as f:\n                        lines = f.readlines()\n                    \n                    corrected_lines = []\n                    for line in lines:\n                        parts = line.strip().split()\n                        if len(parts) >= 5:\n                            # –ò–∑–º–µ–Ω—è–µ–º –∫–ª–∞—Å—Å –Ω–∞ 0 (—á–µ–ª–æ–≤–µ–∫) –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ\n                            parts[0] = '0'\n                            corrected_lines.append(' '.join(parts) + '\\n')\n                    \n                    with open(label_dst, 'w') as f:\n                        f.writelines(corrected_lines)\n                else:\n                    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—É—Å—Ç–æ–≥–æ —Ñ–∞–π–ª–∞ –º–µ—Ç–æ–∫\n                    with open(label_dst, 'w') as f:\n                        pass\n        \n        return copied_count\n    \n    train_count = copy_dataset_files(train_files, \"train\")\n    val_count = copy_dataset_files(val_files, \"val\")\n    \n    # –°–æ–∑–¥–∞–Ω–∏–µ data.yaml\n    data_yaml_content = f\"\"\"# YOLOv13 Dynamic UAV Dataset Configuration\n# Optimized for aerial human detection from UAV imagery\n\npath: {output_path}\ntrain: train/images\nval: val/images\n\nnc: 1\nnames: ['person']\n\n# Dataset Statistics\ntrain_images: {train_count}\nval_images: {val_count}\ntotal_images: {train_count + val_count}\n\n# UAV Specific Settings\naltitude_range: \"10-500m\"\noptimal_detection_height: \"50-150m\"\nmin_object_size: \"32x32 pixels\"\nrecommended_confidence: 0.25\nrecommended_iou: 0.7\n\"\"\"\n    \n    data_yaml_path = os.path.join(output_path, 'data.yaml')\n    with open(data_yaml_path, 'w') as file:\n        file.write(data_yaml_content)\n    \n    logger.info(f\"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω: {train_count} train, {val_count} val\")\n    logger.info(f\"üìÅ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞: {data_yaml_path}\")\n    \n    return data_yaml_path\n\ndef create_test_data_yaml(output_path: str) -> str:\n    \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ data.yaml —Ñ–∞–π–ª–∞\"\"\"\n    data_yaml_content = f\"\"\"# YOLOv13 Dynamic UAV Test Configuration\npath: {output_path}\ntrain: train/images\nval: val/images\n\nnc: 1\nnames: ['person']\n\n# Test configuration - no actual dataset\ntest_mode: true\n\"\"\"\n    \n    data_yaml_path = os.path.join(output_path, 'data.yaml')\n    with open(data_yaml_path, 'w') as file:\n        file.write(data_yaml_content)\n    \n    logger.info(f\"üìù –°–æ–∑–¥–∞–Ω —Ç–µ—Å—Ç–æ–≤—ã–π data.yaml: {data_yaml_path}\")\n    return data_yaml_path\n\ndef main():\n    \"\"\"–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è YOLOv13 —Å Kaggle –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏\"\"\"\n    logger.info(\"üöÅ YOLOv13 Dynamic UAV - –°–∏—Å—Ç–µ–º–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ª—é–¥–µ–π –¥–ª—è –ë–ü–õ–ê\")\n    logger.info(\"=\" * 70)\n    # –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö debug —Ñ–∞–π–ª–æ–≤\n    if IS_DEBUG:\n        cleanup_debug_files()\n    \n    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∂–∏–º–∞ –æ—Ç–ª–∞–¥–∫–∏\n    if IS_DEBUG:\n        logger.info(\"üêõ –†–ï–ñ–ò–ú –û–¢–õ–ê–î–ö–ò –ê–ö–¢–ò–í–ï–ù\")\n        logger.info(\"   ‚Ä¢ –û–±—É—á–µ–Ω–∏–µ –Ω–∞ 800 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö\")\n        logger.info(\"   ‚Ä¢ 1 —ç–ø–æ—Ö–∞ –æ–±—É—á–µ–Ω–∏—è\")\n        logger.info(\"   ‚Ä¢ –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö\")\n    else:\n        logger.info(\"üéØ –ü–û–õ–ù–´–ô –†–ï–ñ–ò–ú –û–ë–£–ß–ï–ù–ò–Ø\")\n        logger.info(\"   ‚Ä¢ –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\")\n        logger.info(\"   ‚Ä¢ –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö\")\n    \n    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π\n    data_path = Path(\"./data\")\n    models_path = Path(\"./models\")\n    \n    data_path.mkdir(exist_ok=True)\n    models_path.mkdir(exist_ok=True)\n    \n    # –ê–Ω–∞–ª–∏–∑ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö Kaggle –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\n    logger.info(\"\\nüìä –ê–Ω–∞–ª–∏–∑ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö Kaggle –¥–∞—Ç–∞—Å–µ—Ç–æ–≤...\")\n    datasets_analysis = analyze_all_datasets()\n    \n    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\n    available_datasets = sum(1 for stats in datasets_analysis.values() if stats['exists'])\n    if available_datasets == 0:\n        logger.error(\"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ Kaggle –¥–∞—Ç–∞—Å–µ—Ç–∞!\")\n        logger.error(\"   –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç–∏ –∫ –¥–∞—Ç–∞—Å–µ—Ç–∞–º:\")\n        logger.error(f\"   ‚Ä¢ {TRAIN_DATASET_1}\")\n        logger.error(f\"   ‚Ä¢ {TRAIN_DATASET_2}\")\n        logger.error(f\"   ‚Ä¢ {VAL_DATASET_PUBLIC}\")\n        logger.error(f\"   ‚Ä¢ {VAL_DATASET_PRIVATE}\")\n        return None, {'error': '–î–∞—Ç–∞—Å–µ—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã'}\n    \n    logger.info(f\"‚úÖ –ù–∞–π–¥–µ–Ω–æ {available_datasets} –∏–∑ 4 –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\")\n    \n    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è\n    training_params = {\n        'data_yaml': None,  # –ë—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑ Kaggle –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\n        'epochs': 1 if IS_DEBUG else 50,\n        'batch_size': 8 if IS_DEBUG else 16,\n        'img_size': 640,\n        'device': 'auto',\n        'save_dir': models_path,\n        'project': 'yolov13_uav_human_detection',\n        'name': f'kaggle_training_debug_{int(time.time())}' if IS_DEBUG else f'kaggle_training_{int(time.time())}',\n        'use_kaggle_datasets': True\n    }\n    \n    logger.info(f\"\\nüìã –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:\")\n    for key, value in training_params.items():\n        logger.info(f\"   {key}: {value}\")\n    \n    try:\n        # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è\n        logger.info(\"\\nüöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è...\")\n        model, results = train_yolov13_dynamic_uav(**training_params)\n        \n        if results.get('success', False):\n            logger.info(\"\\nüéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!\")\n            logger.info(f\"üìÅ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {results['model_path']}\")\n            \n            # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫\n            best_metrics = results.get('best_metrics', {})\n            if best_metrics:\n                logger.info(f\"üìä –õ—É—á—à–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è:\")\n                for metric, value in best_metrics.items():\n                    if isinstance(value, (int, float)):\n                        logger.info(f\"   {metric}: {value:.4f}\")\n                    else:\n                        logger.info(f\"   {metric}: {value}\")\n            \n            # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\n            try:\n                logger.info(\"\\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...\")\n                test_model = YOLO(results['model_path'])\n                logger.info(\"‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞\")\n                logger.info(\"üéØ –ì–æ—Ç–æ–≤–∞ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ª—é–¥–µ–π —Å UAV\")\n                \n                # –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ –ø—Ä–∏–≤–∞—Ç–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ (—Ç–æ–ª—å–∫–æ –≤ –ø–æ–ª–Ω–æ–º —Ä–µ–∂–∏–º–µ)\n                if not IS_DEBUG and os.path.exists(VAL_DATASET_PRIVATE):\n                    logger.info(\"\\nüîí –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ –ø—Ä–∏–≤–∞—Ç–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ...\")\n                    final_private_results = validate_on_private_dataset(results['model_path'])\n                    if 'error' not in final_private_results:\n                        logger.info(\"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –Ω–∞ –ø—Ä–∏–≤–∞—Ç–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ:\")\n                        for metric, value in final_private_results.items():\n                            if isinstance(value, (int, float)):\n                                logger.info(f\"   {metric}: {value:.4f}\")\n                    else:\n                        logger.warning(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏–≤–∞—Ç–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {final_private_results['error']}\")\n                elif IS_DEBUG:\n                    logger.info(\"üêõ –ü—Ä–∏–≤–∞—Ç–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–æ–ø—É—â–µ–Ω–∞ –≤ —Ä–µ–∂–∏–º–µ –æ—Ç–ª–∞–¥–∫–∏\")\n                \n            except Exception as e:\n                logger.error(f\"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}\")\n            \n            # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é\n            logger.info(\"\\nüìã –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –º–æ–¥–µ–ª–∏:\")\n            logger.info(\"   ‚Ä¢ –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –≤—ã—Å–æ—Ç–∞ —Å—ä–µ–º–∫–∏: 50-150–º\")\n            logger.info(\"   ‚Ä¢ –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ: 1920x1080 –∏–ª–∏ –≤—ã—à–µ\")\n            logger.info(\"   ‚Ä¢ –°–∫–æ—Ä–æ—Å—Ç—å –ø–æ–ª–µ—Ç–∞: –Ω–µ –±–æ–ª–µ–µ 15 –º/—Å\")\n            logger.info(\"   ‚Ä¢ –£–≥–æ–ª –∫–∞–º–µ—Ä—ã: 45-90¬∞ –≤–Ω–∏–∑\")\n            logger.info(\"   ‚Ä¢ –õ—É—á—à–∏–µ —É—Å–ª–æ–≤–∏—è: –¥–Ω–µ–≤–Ω–æ–µ –æ—Å–≤–µ—â–µ–Ω–∏–µ, —è—Å–Ω–∞—è –ø–æ–≥–æ–¥–∞\")\n            logger.info(\"   ‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: model.predict('path_to_uav_image.jpg')\")\n            \n            if IS_DEBUG:\n                logger.info(\"\\nüêõ –†–µ–∂–∏–º –æ—Ç–ª–∞–¥–∫–∏ –∑–∞–≤–µ—Ä—à–µ–Ω.\")\n                logger.info(\"   –î–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ IS_DEBUG = False\")\n                logger.info(\"   –∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å–∫—Ä–∏–ø—Ç.\")\n            \n            return model, results\n            \n        else:\n            logger.error(f\"\\n‚ùå –û–±—É—á–µ–Ω–∏–µ –Ω–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {results.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}\")\n            return None, results\n            \n    except Exception as e:\n        logger.error(f\"\\nüí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}\")\n        import traceback\n        traceback.print_exc()\n        return None, {'error': str(e)}\n        \n    finally:\n        \n        \n        logger.info(\"\\n\" + \"=\" * 70)\n        logger.info(\"üèÅ –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –ø—Ä–æ–≥—Ä–∞–º–º—ã\")\n\nif __name__ == \"__main__\":\n    # –ó–∞–ø—É—Å–∫ –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è YOLOv13 —Å Kaggle –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏\n    model, results = main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:58:16.414981Z","iopub.execute_input":"2025-08-01T13:58:16.415949Z"}},"outputs":[{"name":"stderr","text":"2025-08-01 13:58:16,546 - MAIN - INFO - <cell line: 0>:97 - Logging setup completed\n2025-08-01 13:58:16,558 - MAIN - INFO - main:1725 - üöÅ YOLOv13 Dynamic UAV - –°–∏—Å—Ç–µ–º–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ª—é–¥–µ–π –¥–ª—è –ë–ü–õ–ê\n2025-08-01 13:58:16,559 - MAIN - INFO - main:1726 - ======================================================================\n2025-08-01 13:58:16,561 - MAIN - WARNING - cleanup_debug_files:1002 - ‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ debug —Ñ–∞–π–ª–æ–≤: [Errno 16] Device or resource busy: '/kaggle/working'\n2025-08-01 13:58:16,561 - MAIN - INFO - main:1733 - üêõ –†–ï–ñ–ò–ú –û–¢–õ–ê–î–ö–ò –ê–ö–¢–ò–í–ï–ù\n2025-08-01 13:58:16,562 - MAIN - INFO - main:1734 -    ‚Ä¢ –û–±—É—á–µ–Ω–∏–µ –Ω–∞ 800 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö\n2025-08-01 13:58:16,563 - MAIN - INFO - main:1735 -    ‚Ä¢ 1 —ç–ø–æ—Ö–∞ –æ–±—É—á–µ–Ω–∏—è\n2025-08-01 13:58:16,564 - MAIN - INFO - main:1736 -    ‚Ä¢ –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö\n2025-08-01 13:58:16,565 - MAIN - INFO - main:1750 - \nüìä –ê–Ω–∞–ª–∏–∑ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö Kaggle –¥–∞—Ç–∞—Å–µ—Ç–æ–≤...\n2025-08-01 13:58:16,566 - MAIN - INFO - analyze_all_datasets:1015 - üìä –ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤...\n2025-08-01 13:58:16,596 - MAIN - INFO - get_dataset_statistics:431 - üìÅ –ê–Ω–∞–ª–∏–∑ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≤ /kaggle/input/01trains1datasethumanrescu1\n2025-08-01 13:58:16,598 - MAIN - INFO - collect_hierarchical_images:222 - üìÅ –ù–∞–π–¥–µ–Ω–æ 18 –ø–æ–¥–ø–∞–ø–æ–∫ –≤ /kaggle/input/01trains1datasethumanrescu1/images: ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '19', '35']\n","output_type":"stream"},{"name":"stdout","text":"[DEBUG] Logging setup completed successfully.\n","output_type":"stream"},{"name":"stderr","text":"2025-08-01 13:58:34,553 - MAIN - INFO - collect_hierarchical_images:253 - ‚úÖ –°–æ–±—Ä–∞–Ω–æ 8484 –ø–∞—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-–º–µ—Ç–∫–∞ –∏–∑ /kaggle/input/01trains1datasethumanrescu1\n","output_type":"stream"}],"execution_count":null}]}