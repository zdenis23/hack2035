{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-01T12:23:11.423834Z",
     "iopub.status.busy": "2025-08-01T12:23:11.423667Z",
     "iopub.status.idle": "2025-08-01T12:24:24.267621Z",
     "shell.execute_reply": "2025-08-01T12:24:24.266574Z",
     "shell.execute_reply.started": "2025-08-01T12:23:11.423818Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install ultralytics > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T13:58:16.415949Z",
     "iopub.status.busy": "2025-08-01T13:58:16.414981Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-01 13:58:16,546 - MAIN - INFO - <cell line: 0>:97 - Logging setup completed\n",
      "2025-08-01 13:58:16,558 - MAIN - INFO - main:1725 - üöÅ YOLOv13 Dynamic UAV - –°–∏—Å—Ç–µ–º–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ª—é–¥–µ–π –¥–ª—è –ë–ü–õ–ê\n",
      "2025-08-01 13:58:16,559 - MAIN - INFO - main:1726 - ======================================================================\n",
      "2025-08-01 13:58:16,561 - MAIN - WARNING - cleanup_debug_files:1002 - ‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ debug —Ñ–∞–π–ª–æ–≤: [Errno 16] Device or resource busy: '/kaggle/working'\n",
      "2025-08-01 13:58:16,561 - MAIN - INFO - main:1733 - üêõ –†–ï–ñ–ò–ú –û–¢–õ–ê–î–ö–ò –ê–ö–¢–ò–í–ï–ù\n",
      "2025-08-01 13:58:16,562 - MAIN - INFO - main:1734 -    ‚Ä¢ –û–±—É—á–µ–Ω–∏–µ –Ω–∞ 800 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö\n",
      "2025-08-01 13:58:16,563 - MAIN - INFO - main:1735 -    ‚Ä¢ 1 —ç–ø–æ—Ö–∞ –æ–±—É—á–µ–Ω–∏—è\n",
      "2025-08-01 13:58:16,564 - MAIN - INFO - main:1736 -    ‚Ä¢ –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö\n",
      "2025-08-01 13:58:16,565 - MAIN - INFO - main:1750 - \n",
      "üìä –ê–Ω–∞–ª–∏–∑ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö Kaggle –¥–∞—Ç–∞—Å–µ—Ç–æ–≤...\n",
      "2025-08-01 13:58:16,566 - MAIN - INFO - analyze_all_datasets:1015 - üìä –ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤...\n",
      "2025-08-01 13:58:16,596 - MAIN - INFO - get_dataset_statistics:431 - üìÅ –ê–Ω–∞–ª–∏–∑ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≤ /kaggle/input/01trains1datasethumanrescu1\n",
      "2025-08-01 13:58:16,598 - MAIN - INFO - collect_hierarchical_images:222 - üìÅ –ù–∞–π–¥–µ–Ω–æ 18 –ø–æ–¥–ø–∞–ø–æ–∫ –≤ /kaggle/input/01trains1datasethumanrescu1/images: ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '19', '35']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Logging setup completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-01 13:58:34,553 - MAIN - INFO - collect_hierarchical_images:253 - ‚úÖ –°–æ–±—Ä–∞–Ω–æ 8484 –ø–∞—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-–º–µ—Ç–∫–∞ –∏–∑ /kaggle/input/01trains1datasethumanrescu1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.nn.modules import Conv, C2f, SPPF, Detect\n",
    "from ultralytics.utils import LOGGER\n",
    "from ultralytics.models.yolo.detect import DetectionTrainer\n",
    "from ultralytics.utils.torch_utils import select_device, smart_inference_mode\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Optional, Any, Union\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import AdamW, SGD\n",
    "import warnings\n",
    "import logging\n",
    "import gc\n",
    "import time\n",
    "from scipy import ndimage\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å non-interactive backend\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')\n",
    "# –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã –¥–ª—è matplotlib warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning, module='matplotlib')\n",
    "warnings.filterwarnings('ignore', message='invalid value encountered in less')\n",
    "warnings.filterwarnings('ignore', message='invalid value encountered in greater')\n",
    "warnings.filterwarnings('ignore', message='divide by zero encountered')\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ matplotlib –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è warnings\n",
    "plt.ioff()  # –û—Ç–∫–ª—é—á–∏—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º\n",
    "\n",
    "# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è –º–µ—Ç—Ä–∏–∫ –∏–∑ metric.py\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "from numba import jit\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Ä–µ–∂–∏–º–∞ –æ—Ç–ª–∞–¥–∫–∏\n",
    "# –ï—Å–ª–∏ True - –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ 800 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö, –º–∏–Ω–∏–º—É–º 10 —ç–ø–æ—Ö, –≤–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "# –ï—Å–ª–∏ False - –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "IS_DEBUG = False\n",
    "\n",
    "# –ü—É—Ç–∏ –∫ –¥–∞—Ç–∞—Å–µ—Ç–∞–º Kaggle\n",
    "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç–∏ –∫ –¥–∞—Ç–∞—Å–µ—Ç–∞–º –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å—Ä–µ–¥—ã\n",
    "if os.path.exists('/kaggle/input/'):\n",
    "    # Kaggle –æ–∫—Ä—É–∂–µ–Ω–∏–µ\n",
    "    TRAIN_DATASET_1 = '/kaggle/input/01trains1datasethumanrescu1'\n",
    "    TRAIN_DATASET_2 = '/kaggle/input/02secondpartdatasethumanrescue'\n",
    "    VAL_DATASET_PUBLIC = '/kaggle/input/03validationdatasethumanrescue/public'\n",
    "    VAL_DATASET_PRIVATE = '/kaggle/input/03validationdatasethumanrescue/private'\n",
    "else:\n",
    "    # –õ–æ–∫–∞–ª—å–Ω–∞—è —Å—Ä–µ–¥–∞ - —Å–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –ø—É—Ç–∏\n",
    "    TRAIN_DATASET_1 = './datasets/train1'\n",
    "    TRAIN_DATASET_2 = './datasets/train2'\n",
    "    VAL_DATASET_PUBLIC = './datasets/val_public'\n",
    "    VAL_DATASET_PRIVATE = './datasets/val_private'\n",
    "    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç\n",
    "    for dataset_path in [TRAIN_DATASET_1, TRAIN_DATASET_2, VAL_DATASET_PUBLIC, VAL_DATASET_PRIVATE]:\n",
    "        os.makedirs(dataset_path, exist_ok=True)\n",
    "\n",
    "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –ª–æ–≥–æ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å—Ä–µ–¥—ã\n",
    "if os.path.exists(\"/kaggle/working/\"):\n",
    "    log_dir = \"/kaggle/working/\"\n",
    "else:\n",
    "    log_dir = \"./logs/\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "logger = logging.getLogger('ml')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Prevent adding handlers multiple times\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "\n",
    "# File handlers for different log levels\n",
    "from logging.handlers import RotatingFileHandler\n",
    "\n",
    "# Error log handler\n",
    "error_handler = RotatingFileHandler(\n",
    "    os.path.join(log_dir, 'main_error.log'),\n",
    "    maxBytes=1*1024*1024,  # 1MB\n",
    "    backupCount=5,\n",
    "    encoding='utf-8'\n",
    ")\n",
    "error_handler.setLevel(logging.ERROR)\n",
    "\n",
    "# Warning log handler\n",
    "warning_handler = RotatingFileHandler(\n",
    "    os.path.join(log_dir, 'main_warning.log'),\n",
    "    maxBytes=1*1024*1024,  # 1MB\n",
    "    backupCount=5,\n",
    "    encoding='utf-8'\n",
    ")\n",
    "warning_handler.setLevel(logging.WARNING)\n",
    "\n",
    "# Debug log handler\n",
    "debug_handler = RotatingFileHandler(\n",
    "    os.path.join(log_dir, 'main_debug.log'),\n",
    "    maxBytes=1*1024*1024,  # 1MB\n",
    "    backupCount=5,\n",
    "    encoding='utf-8'\n",
    ")\n",
    "debug_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "# Console handler\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "\n",
    "# Formatter\n",
    "formatter = logging.Formatter(\n",
    "    '%(asctime)s - MAIN - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'\n",
    ")\n",
    "error_handler.setFormatter(formatter)\n",
    "warning_handler.setFormatter(formatter)\n",
    "debug_handler.setFormatter(formatter)\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "# Add handlers\n",
    "logger.addHandler(error_handler)\n",
    "logger.addHandler(warning_handler)\n",
    "logger.addHandler(debug_handler)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "logger.info(\"Logging setup completed\")\n",
    "print(\"[DEBUG] Logging setup completed successfully.\")\n",
    "\n",
    "def validate_dataset_annotations(dataset_path: str, max_check: int = 50) -> Dict:\n",
    "    \"\"\"–í–∞–ª–∏–¥–∞—Ü–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ —Ñ–æ—Ä–º–∞—Ç–∞ YOLO\"\"\"\n",
    "    validation_results = {\n",
    "        'valid_annotations': 0,\n",
    "        'invalid_annotations': 0,\n",
    "        'empty_annotations': 0,\n",
    "        'class_distribution': {},\n",
    "        'coordinate_errors': 0,\n",
    "        'format_errors': 0,\n",
    "        'sample_annotations': []\n",
    "    }\n",
    "    \n",
    "    labels_dir = os.path.join(dataset_path, 'labels')\n",
    "    if not os.path.exists(labels_dir):\n",
    "        logger.warning(f\"‚ö†Ô∏è –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –º–µ—Ç–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {labels_dir}\")\n",
    "        return validation_results\n",
    "    \n",
    "    # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π\n",
    "    label_files = [f for f in os.listdir(labels_dir) if f.endswith('.txt')]\n",
    "    check_files = label_files[:max_check] if len(label_files) > max_check else label_files\n",
    "    \n",
    "    logger.info(f\"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ {len(check_files)} —Ñ–∞–π–ª–æ–≤ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –∏–∑ {len(label_files)}\")\n",
    "    \n",
    "    for label_file in check_files:\n",
    "        label_path = os.path.join(labels_dir, label_file)\n",
    "        \n",
    "        try:\n",
    "            with open(label_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            if not lines or all(line.strip() == '' for line in lines):\n",
    "                validation_results['empty_annotations'] += 1\n",
    "                continue\n",
    "            \n",
    "            file_valid = True\n",
    "            for line_num, line in enumerate(lines, 1):\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                \n",
    "                parts = line.split()\n",
    "                if len(parts) != 5:\n",
    "                    logger.warning(f\"‚ö†Ô∏è {label_file}:{line_num} - –ù–µ–≤–µ—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–Ω–∞—á–µ–Ω–∏–π: {len(parts)} (–æ–∂–∏–¥–∞–µ—Ç—Å—è 5)\")\n",
    "                    validation_results['format_errors'] += 1\n",
    "                    file_valid = False\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    class_id = int(parts[0])\n",
    "                    x_center, y_center, width, height = map(float, parts[1:5])\n",
    "                    \n",
    "                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç (–¥–æ–ª–∂–Ω—ã –±—ã—Ç—å 0-1)\n",
    "                    if not (0 <= x_center <= 1 and 0 <= y_center <= 1 and \n",
    "                           0 <= width <= 1 and 0 <= height <= 1):\n",
    "                        logger.warning(f\"‚ö†Ô∏è {label_file}:{line_num} - –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ [0,1]: {parts[1:5]}\")\n",
    "                        validation_results['coordinate_errors'] += 1\n",
    "                        file_valid = False\n",
    "                    \n",
    "                    # –ü–æ–¥—Å—á–µ—Ç –∫–ª–∞—Å—Å–æ–≤\n",
    "                    validation_results['class_distribution'][class_id] = \\\n",
    "                        validation_results['class_distribution'].get(class_id, 0) + 1\n",
    "                    \n",
    "                except ValueError as e:\n",
    "                    logger.warning(f\"‚ö†Ô∏è {label_file}:{line_num} - –û—à–∏–±–∫–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è: {e}\")\n",
    "                    validation_results['format_errors'] += 1\n",
    "                    file_valid = False\n",
    "            \n",
    "            if file_valid:\n",
    "                validation_results['valid_annotations'] += 1\n",
    "                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–∞ –≤–∞–ª–∏–¥–Ω–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏\n",
    "                if len(validation_results['sample_annotations']) < 3:\n",
    "                    validation_results['sample_annotations'].append({\n",
    "                        'file': label_file,\n",
    "                        'content': lines[:3]  # –ü–µ—Ä–≤—ã–µ 3 —Å—Ç—Ä–æ–∫–∏\n",
    "                    })\n",
    "            else:\n",
    "                validation_results['invalid_annotations'] += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {label_file}: {e}\")\n",
    "            validation_results['invalid_annotations'] += 1\n",
    "    \n",
    "    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "    logger.info(f\"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π:\")\n",
    "    logger.info(f\"   ‚úÖ –í–∞–ª–∏–¥–Ω—ã–µ: {validation_results['valid_annotations']}\")\n",
    "    logger.info(f\"   ‚ùå –ù–µ–≤–∞–ª–∏–¥–Ω—ã–µ: {validation_results['invalid_annotations']}\")\n",
    "    logger.info(f\"   üìÑ –ü—É—Å—Ç—ã–µ: {validation_results['empty_annotations']}\")\n",
    "    logger.info(f\"   üî¢ –û—à–∏–±–∫–∏ —Ñ–æ—Ä–º–∞—Ç–∞: {validation_results['format_errors']}\")\n",
    "    logger.info(f\"   üìç –û—à–∏–±–∫–∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç: {validation_results['coordinate_errors']}\")\n",
    "    logger.info(f\"   üè∑Ô∏è –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {validation_results['class_distribution']}\")\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "def create_combined_dataset_yaml(output_path: str = 'combined_data.yaml') -> str:\n",
    "    \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ YAML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\"\"\"\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "    val_images_path = os.path.join(VAL_DATASET_PUBLIC, 'images')\n",
    "    val_labels_path = os.path.join(VAL_DATASET_PUBLIC, 'labels')\n",
    "    \n",
    "    if os.path.exists(VAL_DATASET_PUBLIC):\n",
    "        if not os.path.exists(val_images_path):\n",
    "            logger.warning(f\"‚ö†Ô∏è –ü–∞–ø–∫–∞ images –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ: {val_images_path}\")\n",
    "        if not os.path.exists(val_labels_path):\n",
    "            logger.warning(f\"‚ö†Ô∏è –ü–∞–ø–∫–∞ labels –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ: {val_labels_path}\")\n",
    "        \n",
    "        if os.path.exists(val_images_path) and os.path.exists(val_labels_path):\n",
    "            val_images_count = len([f for f in os.listdir(val_images_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "            val_labels_count = len([f for f in os.listdir(val_labels_path) if f.lower().endswith('.txt')])\n",
    "            logger.info(f\"üìä –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç: {val_images_count} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, {val_labels_count} –º–µ—Ç–æ–∫\")\n",
    "        else:\n",
    "            logger.error(f\"‚ùå –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞: {VAL_DATASET_PUBLIC}\")\n",
    "    else:\n",
    "        logger.warning(f\"‚ö†Ô∏è –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {VAL_DATASET_PUBLIC}\")\n",
    "    \n",
    "    # –í —Ä–µ–∂–∏–º–µ –æ—Ç–ª–∞–¥–∫–∏ —Å–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É\n",
    "    if IS_DEBUG:\n",
    "        logger.info(\"üêõ –°–æ–∑–¥–∞–Ω–∏–µ YAML –¥–ª—è debug —Ä–µ–∂–∏–º–∞\")\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–Ω–∏–µ debug —Å—Ç—Ä—É–∫—Ç—É—Ä—ã\n",
    "        train_datasets = [TRAIN_DATASET_1, TRAIN_DATASET_2]\n",
    "        debug_train_dir = create_debug_dataset_structure(train_datasets, 800)\n",
    "        \n",
    "        if debug_train_dir:\n",
    "            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–≥–æ –ø—É—Ç–∏ –¥–ª—è debug –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "            if os.path.exists('/kaggle/working/'):\n",
    "                debug_base_path = '/kaggle/working/debug_dataset'\n",
    "            else:\n",
    "                debug_base_path = './debug_dataset'\n",
    "            \n",
    "            # –í–∞–ª–∏–¥–∞—Ü–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –≤ debug –¥–∞—Ç–∞—Å–µ—Ç–µ\n",
    "            debug_dataset_path = os.path.join(debug_base_path, 'train')\n",
    "            validation_results = validate_dataset_annotations(debug_dataset_path)\n",
    "            logger.info(f\"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π: {validation_results}\")\n",
    "            \n",
    "            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π\n",
    "            labels_dir = os.path.join(debug_dataset_path, 'labels')\n",
    "            if os.path.exists(labels_dir):\n",
    "                label_files = [f for f in os.listdir(labels_dir) if f.endswith('.txt')]\n",
    "                logger.info(f\"üìÅ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π: {len(label_files)}\")\n",
    "                \n",
    "                # –ü—Ä–æ–≤–µ—Ä–∏–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π\n",
    "                sample_files = label_files[:5] if len(label_files) >= 5 else label_files\n",
    "                for label_file in sample_files:\n",
    "                    label_path = os.path.join(labels_dir, label_file)\n",
    "                    try:\n",
    "                        with open(label_path, 'r') as f:\n",
    "                            lines = f.readlines()\n",
    "                            if lines:\n",
    "                                logger.info(f\"üìÑ {label_file}: {len(lines)} –æ–±—ä–µ–∫—Ç–æ–≤, –ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞: {lines[0].strip()}\")\n",
    "                            else:\n",
    "                                logger.info(f\"üìÑ {label_file}: –ø—É—Å—Ç–æ–π —Ñ–∞–π–ª\")\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {label_file}: {e}\")\n",
    "            else:\n",
    "                logger.warning(f\"‚ö†Ô∏è –ü–∞–ø–∫–∞ —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {labels_dir}\")\n",
    "            \n",
    "            yaml_config = {\n",
    "                'path': debug_base_path,\n",
    "                'train': os.path.join(debug_dataset_path, 'images'),\n",
    "                'val': os.path.join(VAL_DATASET_PUBLIC, 'images') if os.path.exists(VAL_DATASET_PUBLIC) else '',\n",
    "                'test': VAL_DATASET_PRIVATE if os.path.exists(VAL_DATASET_PRIVATE) else '',\n",
    "                'nc': 1,\n",
    "                'names': ['human']\n",
    "            }\n",
    "            \n",
    "            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ YAML —Ñ–∞–π–ª–∞\n",
    "            try:\n",
    "                with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                    yaml.dump(yaml_config, f, default_flow_style=False, allow_unicode=True)\n",
    "                logger.info(f\"‚úÖ Debug YAML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_path}\")\n",
    "                logger.info(f\"   –û–±—É—á–µ–Ω–∏–µ: {yaml_config['train']}\")\n",
    "                logger.info(f\"   –í–∞–ª–∏–¥–∞—Ü–∏—è: {yaml_config['val']}\")\n",
    "                return output_path\n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è debug YAML: {e}\")\n",
    "                return ''\n",
    "    \n",
    "    # –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º - —Å–æ–∑–¥–∞–Ω–∏–µ YAML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –±–µ–∑ –∏–∑–±—ã—Ç–æ—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞\n",
    "    logger.info(\"üìù –°–æ–∑–¥–∞–Ω–∏–µ YAML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞ –æ–±—É—á–µ–Ω–∏—è\")\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ YAML\n",
    "    yaml_config = {\n",
    "        'path': os.path.dirname(os.path.abspath(output_path)),\n",
    "        'train': [],\n",
    "        'val': os.path.join(VAL_DATASET_PUBLIC, 'images') if os.path.exists(VAL_DATASET_PUBLIC) else '',\n",
    "        'test': VAL_DATASET_PRIVATE if os.path.exists(VAL_DATASET_PRIVATE) else '',\n",
    "        'nc': 1,\n",
    "        'names': ['human']\n",
    "    }\n",
    "    \n",
    "    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\n",
    "    if os.path.exists(TRAIN_DATASET_1):\n",
    "        yaml_config['train'].append(os.path.join(TRAIN_DATASET_1, 'images'))\n",
    "    if os.path.exists(TRAIN_DATASET_2):\n",
    "        yaml_config['train'].append(os.path.join(TRAIN_DATASET_2, 'images'))\n",
    "    \n",
    "    # –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –¥–∞—Ç–∞—Å–µ—Ç, —É–±–∏—Ä–∞–µ–º —Å–ø–∏—Å–æ–∫\n",
    "    if len(yaml_config['train']) == 1:\n",
    "        yaml_config['train'] = yaml_config['train'][0]\n",
    "    elif len(yaml_config['train']) == 0:\n",
    "        logger.error(\"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –æ–±—É—á–∞—é—â–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞!\")\n",
    "        yaml_config['train'] = ''\n",
    "    \n",
    "    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ YAML —Ñ–∞–π–ª–∞\n",
    "    try:\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            yaml.dump(yaml_config, f, default_flow_style=False, allow_unicode=True)\n",
    "        logger.info(f\"‚úÖ YAML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_path}\")\n",
    "        return output_path\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è YAML: {e}\")\n",
    "        return ''\n",
    "\n",
    "def collect_hierarchical_images(dataset_path: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    –°–±–æ—Ä –≤—Å–µ—Ö –ø–∞—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-–º–µ—Ç–∫–∞ –∏–∑ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É —Å –ø–æ–¥–ø–∞–ø–∫–∞–º–∏\n",
    "        \n",
    "    Returns:\n",
    "        List[Tuple[str, str]]: –°–ø–∏—Å–æ–∫ –ø–∞—Ä (–ø—É—Ç—å_–∫_–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é, –ø—É—Ç—å_–∫_–º–µ—Ç–∫–µ)\n",
    "    \"\"\"\n",
    "    image_label_pairs = []\n",
    "    \n",
    "    images_base = os.path.join(dataset_path, 'images')\n",
    "    labels_base = os.path.join(dataset_path, 'labels')\n",
    "    \n",
    "    if not (os.path.exists(images_base) and os.path.exists(labels_base)):\n",
    "        logger.warning(f\"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω—ã –ø–∞–ø–∫–∏ images –∏–ª–∏ labels –≤ {dataset_path}\")\n",
    "        return image_label_pairs\n",
    "    \n",
    "    # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –ø–æ–¥–ø–∞–ø–æ–∫ –≤ images\n",
    "    try:\n",
    "        image_subdirs = [d for d in os.listdir(images_base) \n",
    "                        if os.path.isdir(os.path.join(images_base, d))]\n",
    "        image_subdirs = sorted(image_subdirs)\n",
    "        \n",
    "        logger.info(f\"üìÅ –ù–∞–π–¥–µ–Ω–æ {len(image_subdirs)} –ø–æ–¥–ø–∞–ø–æ–∫ –≤ {images_base}: {image_subdirs}\")\n",
    "        \n",
    "        for subdir in image_subdirs:\n",
    "            images_subdir = os.path.join(images_base, subdir)\n",
    "            labels_subdir = os.path.join(labels_base, subdir)\n",
    "            \n",
    "            if not os.path.exists(labels_subdir):\n",
    "                logger.warning(f\"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∞—è –ø–∞–ø–∫–∞ –º–µ—Ç–æ–∫: {labels_subdir}\")\n",
    "                continue\n",
    "            \n",
    "            # –°–±–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ –ø–æ–¥–ø–∞–ø–∫–∏\n",
    "            try:\n",
    "                image_files = [f for f in os.listdir(images_subdir) \n",
    "                             if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "                \n",
    "                for img_file in image_files:\n",
    "                    img_path = os.path.join(images_subdir, img_file)\n",
    "                    \n",
    "                    # –ü–æ–∏—Å–∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π –º–µ—Ç–∫–∏\n",
    "                    label_name = os.path.splitext(img_file)[0] + '.txt'\n",
    "                    label_path = os.path.join(labels_subdir, label_name)\n",
    "                    \n",
    "                    if os.path.exists(label_path):\n",
    "                        image_label_pairs.append((img_path, label_path))\n",
    "                    else:\n",
    "                        logger.warning(f\"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–∞ –º–µ—Ç–∫–∞ –¥–ª—è {img_file}: {label_path}\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ–¥–ø–∞–ø–∫–∏ {subdir}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        logger.info(f\"‚úÖ –°–æ–±—Ä–∞–Ω–æ {len(image_label_pairs)} –ø–∞—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-–º–µ—Ç–∫–∞ –∏–∑ {dataset_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå –û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ {dataset_path}: {e}\")\n",
    "    \n",
    "    return image_label_pairs\n",
    "\n",
    "def create_debug_dataset_structure(train_datasets: List[str], limit: int = 800) -> str:\n",
    "    \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\"\"\"\n",
    "    if not IS_DEBUG:\n",
    "        return None\n",
    "    \n",
    "    logger.info(f\"üêõ –°–æ–∑–¥–∞–Ω–∏–µ debug —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º {limit} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\")\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π - –∞–¥–∞–ø—Ç–∞—Ü–∏—è –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Å—Ä–µ–¥—ã\n",
    "    if os.path.exists('/kaggle/working/'):\n",
    "        debug_base_dir = '/kaggle/working/debug_dataset'\n",
    "    else:\n",
    "        debug_base_dir = './debug_dataset'\n",
    "        os.makedirs(debug_base_dir, exist_ok=True)\n",
    "    \n",
    "    debug_train_dir = os.path.join(debug_base_dir, 'train')\n",
    "    debug_train_images = os.path.join(debug_train_dir, 'images')\n",
    "    debug_train_labels = os.path.join(debug_train_dir, 'labels')\n",
    "    \n",
    "    # –û—á–∏—Å—Ç–∫–∞ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π\n",
    "    if os.path.exists(debug_base_dir):\n",
    "        shutil.rmtree(debug_base_dir)\n",
    "    \n",
    "    os.makedirs(debug_train_images, exist_ok=True)\n",
    "    os.makedirs(debug_train_labels, exist_ok=True)\n",
    "    \n",
    "    total_copied = 0\n",
    "    \n",
    "    # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ñ–∞–π–ª–æ–≤\n",
    "    for dataset_path in train_datasets:\n",
    "        if total_copied >= limit:\n",
    "            break\n",
    "            \n",
    "        images_dir = os.path.join(dataset_path, 'images')\n",
    "        labels_dir = os.path.join(dataset_path, 'labels')\n",
    "        \n",
    "        if not (os.path.exists(images_dir) and os.path.exists(labels_dir)):\n",
    "            logger.warning(f\"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω—ã –ø–∞–ø–∫–∏ images/labels –≤ {dataset_path}\")\n",
    "            continue\n",
    "        \n",
    "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É (–ø–æ–¥–ø–∞–ø–∫–∏)\n",
    "        try:\n",
    "            subdirs = [d for d in os.listdir(images_dir) \n",
    "                      if os.path.isdir(os.path.join(images_dir, d))]\n",
    "            \n",
    "            if subdirs:  # –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ (–∫–∞–∫ TRAIN_DATASET_1)\n",
    "                logger.info(f\"üìÅ –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤ {dataset_path}\")\n",
    "                image_label_pairs = collect_hierarchical_images(dataset_path)\n",
    "                \n",
    "                # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–∞—Ä –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏\n",
    "                selected_pairs = image_label_pairs[:limit - total_copied]\n",
    "                \n",
    "                for img_path, label_path in selected_pairs:\n",
    "                    if total_copied >= limit:\n",
    "                        break\n",
    "                    \n",
    "                    # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞\n",
    "                    img_filename = os.path.basename(img_path)\n",
    "                    label_filename = os.path.basename(label_path)\n",
    "                    \n",
    "                    # –°–æ–∑–¥–∞–Ω–∏–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏–º–µ–Ω –¥–ª—è –æ—Ç–ª–∞–¥–æ—á–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è\n",
    "                    base_name = f\"{total_copied:06d}_{os.path.splitext(img_filename)[0]}\"\n",
    "                    img_ext = os.path.splitext(img_filename)[1]\n",
    "                    dst_img = os.path.join(debug_train_images, f\"{base_name}{img_ext}\")\n",
    "                    dst_label = os.path.join(debug_train_labels, f\"{base_name}.txt\")\n",
    "                    \n",
    "                    # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤\n",
    "                    try:\n",
    "                        shutil.copy2(img_path, dst_img)\n",
    "                        shutil.copy2(label_path, dst_label)\n",
    "                        total_copied += 1\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è {img_path}: {e}\")\n",
    "                        continue\n",
    "                        \n",
    "            else:  # –ü–ª–æ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ (—Å—Ç–∞—Ä—ã–π –∫–æ–¥)\n",
    "                logger.info(f\"üìÑ –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –ø–ª–æ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤ {dataset_path}\")\n",
    "                \n",
    "                # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
    "                images = [f for f in os.listdir(images_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "                images = sorted(images)[:min(limit - total_copied, len(images))]\n",
    "                \n",
    "                for img_file in images:\n",
    "                    if total_copied >= limit:\n",
    "                        break\n",
    "                        \n",
    "                    # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∏–º–µ–Ω\n",
    "                    src_img = os.path.join(images_dir, img_file)\n",
    "                    base_name = f\"{total_copied:06d}_{os.path.splitext(img_file)[0]}\"\n",
    "                    img_ext = os.path.splitext(img_file)[1]\n",
    "                    dst_img = os.path.join(debug_train_images, f\"{base_name}{img_ext}\")\n",
    "                    \n",
    "                    try:\n",
    "                        shutil.copy2(src_img, dst_img)\n",
    "                        \n",
    "                        # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π –º–µ—Ç–∫–∏\n",
    "                        label_file = os.path.splitext(img_file)[0] + '.txt'\n",
    "                        src_label = os.path.join(labels_dir, label_file)\n",
    "                        dst_label = os.path.join(debug_train_labels, f\"{base_name}.txt\")\n",
    "                        \n",
    "                        if os.path.exists(src_label):\n",
    "                            shutil.copy2(src_label, dst_label)\n",
    "                        \n",
    "                        total_copied += 1\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è {img_file}: {e}\")\n",
    "                        continue\n",
    "                        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ {dataset_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    logger.info(f\"‚úÖ –°–æ–∑–¥–∞–Ω–∞ debug —Å—Ç—Ä—É–∫—Ç—É—Ä–∞: {total_copied} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ {debug_train_dir}\")\n",
    "    return debug_train_dir\n",
    "\n",
    "def prepare_debug_dataset(train_paths: List[str], limit: int = 800) -> List[str]:\n",
    "    \"\"\"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏\"\"\"\n",
    "    if not IS_DEBUG:\n",
    "        return train_paths\n",
    "    \n",
    "    logger.info(f\"üêõ –†–µ–∂–∏–º –æ—Ç–ª–∞–¥–∫–∏: –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–æ {limit} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\")\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã\n",
    "    debug_dir = create_debug_dataset_structure(train_paths, limit)\n",
    "    if debug_dir:\n",
    "        return [debug_dir]\n",
    "    \n",
    "    # Fallback –∫ —Å—Ç–∞—Ä–æ–π –ª–æ–≥–∏–∫–µ –µ—Å–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –Ω–µ —É–¥–∞–ª–æ—Å—å\n",
    "    debug_train_paths = []\n",
    "    total_images = 0\n",
    "    \n",
    "    for train_path in train_paths:\n",
    "        if isinstance(train_path, str):\n",
    "            images_dir = train_path\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        if os.path.exists(images_dir):\n",
    "            images = [f for f in os.listdir(images_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "            images = sorted(images)[:min(limit - total_images, len(images))]\n",
    "            \n",
    "            if images:\n",
    "                debug_train_paths.append(images_dir)\n",
    "                total_images += len(images)\n",
    "                logger.info(f\"üìä –î–æ–±–∞–≤–ª–µ–Ω–æ {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ {images_dir}\")\n",
    "            \n",
    "            if total_images >= limit:\n",
    "                break\n",
    "    \n",
    "    logger.info(f\"üéØ –ò—Ç–æ–≥–æ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏: {total_images} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\")\n",
    "    return debug_train_paths\n",
    "\n",
    "def get_dataset_statistics(dataset_path: str) -> Dict:\n",
    "    \"\"\"–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã\"\"\"\n",
    "    stats = {\n",
    "        'images_count': 0,\n",
    "        'labels_count': 0,\n",
    "        'classes_distribution': {},\n",
    "        'image_sizes': [],\n",
    "        'exists': False,\n",
    "        'structure_type': 'unknown'\n",
    "    }\n",
    "    \n",
    "    if not os.path.exists(dataset_path):\n",
    "        return stats\n",
    "    \n",
    "    stats['exists'] = True\n",
    "    images_dir = os.path.join(dataset_path, 'images')\n",
    "    labels_dir = os.path.join(dataset_path, 'labels')\n",
    "    \n",
    "    if not (os.path.exists(images_dir) and os.path.exists(labels_dir)):\n",
    "        return stats\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É\n",
    "    try:\n",
    "        subdirs = [d for d in os.listdir(images_dir) \n",
    "                  if os.path.isdir(os.path.join(images_dir, d))]\n",
    "        \n",
    "        if subdirs:  # –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞\n",
    "            stats['structure_type'] = 'hierarchical'\n",
    "            logger.info(f\"üìÅ –ê–Ω–∞–ª–∏–∑ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≤ {dataset_path}\")\n",
    "            \n",
    "            # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ collect_hierarchical_images –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞\n",
    "            image_label_pairs = collect_hierarchical_images(dataset_path)\n",
    "            stats['images_count'] = len(image_label_pairs)\n",
    "            stats['labels_count'] = len(image_label_pairs)\n",
    "            \n",
    "            # –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Å–æ–≤ –∏–∑ –º–µ—Ç–æ–∫\n",
    "            class_counts = {}\n",
    "            for _, label_path in image_label_pairs:\n",
    "                try:\n",
    "                    with open(label_path, 'r') as f:\n",
    "                        lines = f.readlines()\n",
    "                        for line in lines:\n",
    "                            if line.strip():\n",
    "                                class_id = int(line.split()[0])\n",
    "                                class_counts[class_id] = class_counts.get(class_id, 0) + 1\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "            \n",
    "            stats['classes_distribution'] = class_counts\n",
    "            \n",
    "        else:  # –ü–ª–æ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞\n",
    "            stats['structure_type'] = 'flat'\n",
    "            logger.info(f\"üìÑ –ê–Ω–∞–ª–∏–∑ –ø–ª–æ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≤ {dataset_path}\")\n",
    "            \n",
    "            # –ü–æ–¥—Å—á–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
    "            try:\n",
    "                image_files = [f for f in os.listdir(images_dir) \n",
    "                              if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "                stats['images_count'] = len(image_files)\n",
    "                \n",
    "                # –ü–æ–¥—Å—á–µ—Ç –º–µ—Ç–æ–∫\n",
    "                label_files = [f for f in os.listdir(labels_dir) \n",
    "                              if f.lower().endswith('.txt')]\n",
    "                stats['labels_count'] = len(label_files)\n",
    "                \n",
    "                # –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Å–æ–≤\n",
    "                class_counts = {}\n",
    "                for label_file in label_files:\n",
    "                    label_path = os.path.join(labels_dir, label_file)\n",
    "                    try:\n",
    "                        with open(label_path, 'r') as f:\n",
    "                            lines = f.readlines()\n",
    "                            for line in lines:\n",
    "                                if line.strip():\n",
    "                                    class_id = int(line.split()[0])\n",
    "                                    class_counts[class_id] = class_counts.get(class_id, 0) + 1\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "                \n",
    "                stats['classes_distribution'] = class_counts\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø–ª–æ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã {dataset_path}: {e}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–∞ {dataset_path}: {e}\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def validate_on_private_dataset(model_path: str, private_dataset_path: str = VAL_DATASET_PRIVATE, debug_conf: float = None, debug_iou: float = None, debug_mode: bool = False) -> Dict:\n",
    "    \"\"\"–í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –Ω–∞ –ø—Ä–∏–≤–∞—Ç–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ\"\"\"\n",
    "    logger.info(\"üîí –ù–∞—á–∞–ª–æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –Ω–∞ –ø—Ä–∏–≤–∞—Ç–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ...\")\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        logger.error(f\"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}\")\n",
    "        return {'error': 'Model not found'}\n",
    "    \n",
    "    if not os.path.exists(private_dataset_path):\n",
    "        logger.error(f\"‚ùå –ü—Ä–∏–≤–∞—Ç–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {private_dataset_path}\")\n",
    "        return {'error': 'Private dataset not found'}\n",
    "    \n",
    "    try:\n",
    "        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏\n",
    "        model = YOLO(model_path)\n",
    "        logger.info(f\"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_path}\")\n",
    "        \n",
    "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–∏–≤–∞—Ç–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "        private_images_path = os.path.join(private_dataset_path, 'images')\n",
    "        private_labels_path = os.path.join(private_dataset_path, 'labels')\n",
    "        \n",
    "        if not os.path.exists(private_images_path):\n",
    "            logger.error(f\"‚ùå –ü–∞–ø–∫–∞ images –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –ø—Ä–∏–≤–∞—Ç–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ: {private_images_path}\")\n",
    "            return {'error': 'Private dataset images folder not found'}\n",
    "        \n",
    "        if not os.path.exists(private_labels_path):\n",
    "            logger.error(f\"‚ùå –ü–∞–ø–∫–∞ labels –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –ø—Ä–∏–≤–∞—Ç–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ: {private_labels_path}\")\n",
    "            return {'error': 'Private dataset labels folder not found'}\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ YAML –¥–ª—è –ø—Ä–∏–≤–∞—Ç–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "        private_yaml_config = {\n",
    "            'path': os.path.dirname(private_dataset_path),\n",
    "            'train': os.path.join(TRAIN_DATASET_1, 'images') if os.path.exists(TRAIN_DATASET_1) else private_images_path,  # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π 'train' –∫–ª—é—á\n",
    "            'val': private_images_path,  # –£–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞ –ø–∞–ø–∫—É images\n",
    "            'nc': 1,\n",
    "            'names': ['human']\n",
    "        }\n",
    "        \n",
    "        private_yaml_path = 'private_validation.yaml'\n",
    "        with open(private_yaml_path, 'w', encoding='utf-8') as f:\n",
    "            yaml.dump(private_yaml_config, f, default_flow_style=False, allow_unicode=True)\n",
    "        \n",
    "        # –í–∞–ª–∏–¥–∞—Ü–∏—è\n",
    "        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞ –æ—Ç–ª–∞–¥–∫–∏\n",
    "        val_conf = debug_conf if debug_mode and debug_conf is not None else 0.001\n",
    "        val_iou = debug_iou if debug_mode and debug_iou is not None else 0.6\n",
    "        \n",
    "        logger.info(f\"üîç –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: conf={val_conf}, iou={val_iou}...\")\n",
    "        val_results = model.val(\n",
    "            data=private_yaml_path,\n",
    "            imgsz=640,\n",
    "            batch=16,\n",
    "            conf=val_conf,  # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏\n",
    "            iou=val_iou,    # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π IoU –ø–æ—Ä–æ–≥\n",
    "            device='auto',\n",
    "            plots=True,\n",
    "            save_json=True,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "        results_dict = {\n",
    "            'map50': float(val_results.box.map50) if hasattr(val_results, 'box') else 0.0,\n",
    "            'map50_95': float(val_results.box.map) if hasattr(val_results, 'box') else 0.0,\n",
    "            'precision': float(val_results.box.mp) if hasattr(val_results, 'box') else 0.0,\n",
    "            'recall': float(val_results.box.mr) if hasattr(val_results, 'box') else 0.0,\n",
    "            'f1_score': 0.0,\n",
    "            'custom_metric_q': 0.0\n",
    "        }\n",
    "        \n",
    "        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ F1-score\n",
    "        if results_dict['precision'] > 0 and results_dict['recall'] > 0:\n",
    "            results_dict['f1_score'] = 2 * (results_dict['precision'] * results_dict['recall']) / (results_dict['precision'] + results_dict['recall'])\n",
    "        \n",
    "        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏ Q\n",
    "        logger.info(\"üéØ –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏ Q...\")\n",
    "        try:\n",
    "            # –°–æ–∑–¥–∞–Ω–∏–µ DataFrame —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏ –∏—Å–ø–æ–ª—å–∑—É—è process_images_adapted\n",
    "            predicted_df = process_images_adapted(private_dataset_path)\n",
    "            \n",
    "            if predicted_df is not None and len(predicted_df) > 0:\n",
    "                logger.info(f\"üìä –°–æ–∑–¥–∞–Ω DataFrame —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏: {len(predicted_df)} –∑–∞–ø–∏—Å–µ–π\")\n",
    "                \n",
    "                # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è DataFrame –≤ bytes –¥–ª—è evaluate\n",
    "                predicted_bytes = df_to_bytes(predicted_df)\n",
    "                \n",
    "                # –ó–∞–≥—Ä—É–∑–∫–∞ ground truth –¥–∞–Ω–Ω—ã—Ö\n",
    "                if os.path.exists(PUBLIC_GT_CSV_PATH):\n",
    "                    gt_bytes = open_df_as_bytes(PUBLIC_GT_CSV_PATH)\n",
    "                    \n",
    "                    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑—É—è evaluate\n",
    "                    custom_q = evaluate(\n",
    "                        predicted_file=predicted_bytes,\n",
    "                        gt_file=gt_bytes,\n",
    "                        thresholds=np.round(np.arange(0.3, 1.0, 0.07), 2),\n",
    "                        beta=1.0,\n",
    "                        m=500,\n",
    "                        parallelize=True\n",
    "                    )\n",
    "                    \n",
    "                    if custom_q is not None and not np.isnan(custom_q) and not np.isinf(custom_q):\n",
    "                        results_dict['custom_metric_q'] = custom_q\n",
    "                        logger.info(f\"‚úÖ –ö–∞—Å—Ç–æ–º–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ Q —É—Å–ø–µ—à–Ω–æ –≤—ã—á–∏—Å–ª–µ–Ω–∞: {custom_q:.4f}\")\n",
    "                    else:\n",
    "                        logger.warning(\"‚ö†Ô∏è –ü–æ–ª—É—á–µ–Ω–æ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏\")\n",
    "                        results_dict['custom_metric_q'] = 0.0\n",
    "                else:\n",
    "                    logger.warning(f\"‚ö†Ô∏è –§–∞–π–ª ground truth –Ω–µ –Ω–∞–π–¥–µ–Ω: {PUBLIC_GT_CSV_PATH}\")\n",
    "                    results_dict['custom_metric_q'] = 0.0\n",
    "            else:\n",
    "                logger.warning(\"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å DataFrame —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏\")\n",
    "                results_dict['custom_metric_q'] = 0.0\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå –û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏: {e}\")\n",
    "            import traceback\n",
    "            logger.error(f\"‚ùå –ü–æ–ª–Ω–∞—è —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞ –æ—à–∏–±–∫–∏: {traceback.format_exc()}\")\n",
    "            results_dict['custom_metric_q'] = 0.0\n",
    "        \n",
    "        logger.info(\"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –Ω–∞ –ø—Ä–∏–≤–∞—Ç–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ:\")\n",
    "        logger.info(f\"   mAP@0.5: {results_dict['map50']:.4f}\")\n",
    "        logger.info(f\"   mAP@0.5:0.95: {results_dict['map50_95']:.4f}\")\n",
    "        logger.info(f\"   Precision: {results_dict['precision']:.4f}\")\n",
    "        logger.info(f\"   Recall: {results_dict['recall']:.4f}\")\n",
    "        logger.info(f\"   F1-Score: {results_dict['f1_score']:.4f}\")\n",
    "        logger.info(f\"   üéØ –ö–∞—Å—Ç–æ–º–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ Q: {results_dict['custom_metric_q']:.4f}\")\n",
    "        \n",
    "        # –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞\n",
    "        try:\n",
    "            os.remove(private_yaml_path)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return results_dict\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –Ω–∞ –ø—Ä–∏–≤–∞—Ç–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {'error': str(e)}\n",
    "\n",
    "def calculate_custom_metric(predictions: List[np.ndarray], ground_truths: List[np.ndarray], beta: float = 1.0) -> float:\n",
    "    \"\"\"\n",
    "    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏ —Å–æ–≥–ª–∞—Å–Ω–æ —Ñ–æ—Ä–º—É–ª–µ (1)\n",
    "    \n",
    "    Args:\n",
    "        predictions: –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –º–∞—Å–æ–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "        ground_truths: –°–ø–∏—Å–æ–∫ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –º–∞—Å–æ–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è  \n",
    "        beta: –ü–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è F-beta –º–µ—Ä—ã (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1.0)\n",
    "        \n",
    "    Returns:\n",
    "        float: –ó–Ω–∞—á–µ–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏ Q\n",
    "    \"\"\"\n",
    "    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ None –∏ –ø—É—Å—Ç—ã–µ —Å–ø–∏—Å–∫–∏\n",
    "    if predictions is None or ground_truths is None:\n",
    "        logger.error(\"‚ùå –ü–æ–ª—É—á–µ–Ω—ã None –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∏–ª–∏ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –º–∞—Å–æ–∫\")\n",
    "        return 0.0\n",
    "        \n",
    "    if len(predictions) != len(ground_truths):\n",
    "        logger.error(f\"‚ùå –ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π ({len(predictions)}) –∏ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –º–∞—Å–æ–∫ ({len(ground_truths)})\")\n",
    "        return 0.0\n",
    "    \n",
    "    if len(predictions) == 0:\n",
    "        logger.warning(\"‚ö†Ô∏è –ü—É—Å—Ç—ã–µ —Å–ø–∏—Å–∫–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∏ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –º–∞—Å–æ–∫\")\n",
    "        return 0.0\n",
    "    \n",
    "    # –ü–æ—Ä–æ–≥–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –æ—Ç 0.3 –¥–æ 0.93 —Å —à–∞–≥–æ–º 0.07\n",
    "    thresholds = np.arange(0.3, 0.94, 0.07)\n",
    "    n_thresholds = len(thresholds)\n",
    "    \n",
    "    logger.info(f\"üìä –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è {n_thresholds} –ø–æ—Ä–æ–≥–æ–≤: {thresholds}\")\n",
    "    \n",
    "    total_f_beta = 0.0\n",
    "    valid_calculations = 0\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        try:\n",
    "            f_beta_t = calculate_f_beta_for_threshold(predictions, ground_truths, threshold, beta)\n",
    "            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ None, NaN –∏ Inf\n",
    "            if f_beta_t is not None and not np.isnan(f_beta_t) and not np.isinf(f_beta_t):\n",
    "                total_f_beta += f_beta_t\n",
    "                valid_calculations += 1\n",
    "                logger.info(f\"   –ü–æ—Ä–æ–≥ {threshold:.2f}: F-beta = {f_beta_t:.4f}\")\n",
    "            else:\n",
    "                logger.warning(f\"‚ö†Ô∏è –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ F-beta –¥–ª—è –ø–æ—Ä–æ–≥–∞ {threshold:.2f}: {f_beta_t}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå –û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è F-beta –¥–ª—è –ø–æ—Ä–æ–≥–∞ {threshold:.2f}: {e}\")\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –≤–∞–ª–∏–¥–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π\n",
    "    if valid_calculations == 0:\n",
    "        logger.error(\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—á–∏—Å–ª–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ –≤–∞–ª–∏–¥–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è F-beta\")\n",
    "        return 0.0\n",
    "    \n",
    "    # –§–∏–Ω–∞–ª—å–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ - —Å—Ä–µ–¥–Ω–µ–µ –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–æ–µ –ø–æ –≤–∞–ª–∏–¥–Ω—ã–º –≤—ã—á–∏—Å–ª–µ–Ω–∏—è–º\n",
    "    try:\n",
    "        custom_metric = total_f_beta / valid_calculations\n",
    "        logger.info(f\"üéØ –ò—Ç–æ–≥–æ–≤–∞—è –∫–∞—Å—Ç–æ–º–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ Q = {custom_metric:.4f} (–Ω–∞ –æ—Å–Ω–æ–≤–µ {valid_calculations}/{n_thresholds} –≤–∞–ª–∏–¥–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π)\")\n",
    "        return custom_metric\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå –û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def calculate_f_beta_for_threshold(predictions: List[np.ndarray], ground_truths: List[np.ndarray], \n",
    "                                  threshold: float, beta: float = 1.0) -> float:\n",
    "    \"\"\"\n",
    "    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ F-beta –º–µ—Ä—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞ IoU\n",
    "    \n",
    "    Args:\n",
    "        predictions: –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –º–∞—Å–æ–∫\n",
    "        ground_truths: –°–ø–∏—Å–æ–∫ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –º–∞—Å–æ–∫\n",
    "        threshold: –ü–æ—Ä–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ IoU\n",
    "        beta: –ü–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è F-beta –º–µ—Ä—ã\n",
    "        \n",
    "    Returns:\n",
    "        float: –ó–Ω–∞—á–µ–Ω–∏–µ F-beta –º–µ—Ä—ã –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞\n",
    "    \"\"\"\n",
    "    total_tp = 0\n",
    "    total_fp = 0\n",
    "    total_fn = 0\n",
    "    \n",
    "    for pred_mask, gt_mask in zip(predictions, ground_truths):\n",
    "        tp, fp, fn = calculate_tp_fp_fn_for_image(pred_mask, gt_mask, threshold)\n",
    "        total_tp += tp\n",
    "        total_fp += fp\n",
    "        total_fn += fn\n",
    "    \n",
    "    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ F-beta –º–µ—Ä—ã –ø–æ —Ñ–æ—Ä–º—É–ª–µ (2)\n",
    "    if total_tp + total_fp == 0 and total_tp + total_fn == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    if total_tp + total_fp == 0:\n",
    "        precision = 0.0\n",
    "    else:\n",
    "        precision = total_tp / (total_tp + total_fp)\n",
    "    \n",
    "    if total_tp + total_fn == 0:\n",
    "        recall = 0.0\n",
    "    else:\n",
    "        recall = total_tp / (total_tp + total_fn)\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # F-beta —Ñ–æ—Ä–º—É–ª–∞ —Å beta = 1\n",
    "    f_beta = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n",
    "    \n",
    "    return f_beta\n",
    "\n",
    "\n",
    "def calculate_tp_fp_fn_for_image(pred_mask: np.ndarray, gt_mask: np.ndarray, threshold: float) -> Tuple[int, int, int]:\n",
    "    \"\"\"\n",
    "    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ TP, FP, FN –¥–ª—è –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–Ω–æ –∞–ª–≥–æ—Ä–∏—Ç–º—É\n",
    "    \n",
    "    Args:\n",
    "        pred_mask: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–∞—Å–∫–∞ (2D –º–∞—Å—Å–∏–≤ —Å 0 –∏ 1)\n",
    "        gt_mask: –ò—Å—Ç–∏–Ω–Ω–∞—è –º–∞—Å–∫–∞ (2D –º–∞—Å—Å–∏–≤ —Å 0 –∏ 1)\n",
    "        threshold: –ü–æ—Ä–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ IoU\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[int, int, int]: TP, FP, FN\n",
    "    \"\"\"\n",
    "    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–±–ª–∞—Å—Ç–µ–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ —Ä–∞–∑–º–µ—Ç–∫–∏\n",
    "    pred_regions = extract_regions(pred_mask)\n",
    "    gt_regions = extract_regions(gt_mask)\n",
    "    \n",
    "    if len(pred_regions) == 0 and len(gt_regions) == 0:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    if len(pred_regions) == 0:\n",
    "        return 0, 0, len(gt_regions)\n",
    "    \n",
    "    if len(gt_regions) == 0:\n",
    "        return 0, len(pred_regions), 0\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è IoU\n",
    "    iou_matrix = create_iou_matrix(pred_regions, gt_regions)\n",
    "    \n",
    "    # –ü–æ–¥—Å—á–µ—Ç TP, FP, FN —Å–æ–≥–ª–∞—Å–Ω–æ –∞–ª–≥–æ—Ä–∏—Ç–º—É\n",
    "    tp, fp, fn = count_tp_fp_fn_from_matrix(iou_matrix, threshold)\n",
    "    \n",
    "    return tp, fp, fn\n",
    "\n",
    "\n",
    "def extract_regions(mask: np.ndarray) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–≤—è–∑–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π –∏–∑ –±–∏–Ω–∞—Ä–Ω–æ–π –º–∞—Å–∫–∏\n",
    "    \n",
    "    Args:\n",
    "        mask: –ë–∏–Ω–∞—Ä–Ω–∞—è –º–∞—Å–∫–∞ (2D –º–∞—Å—Å–∏–≤ —Å 0 –∏ 1)\n",
    "        \n",
    "    Returns:\n",
    "        List[np.ndarray]: –°–ø–∏—Å–æ–∫ –º–∞—Å–æ–∫ –¥–ª—è –∫–∞–∂–¥–æ–π —Å–≤—è–∑–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏\n",
    "    \"\"\"\n",
    "    # –ü–æ–∏—Å–∫ —Å–≤—è–∑–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç\n",
    "    labeled_mask, num_features = ndimage.label(mask)\n",
    "    \n",
    "    regions = []\n",
    "    for i in range(1, num_features + 1):\n",
    "        region_mask = (labeled_mask == i).astype(np.uint8)\n",
    "        regions.append(region_mask)\n",
    "    \n",
    "    return regions\n",
    "\n",
    "\n",
    "def get_model_predictions_as_masks(model, dataset_path: str, img_size: int = 640, conf_threshold: float = 0.25) -> Tuple[List[np.ndarray], List[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–∏ –≤ –≤–∏–¥–µ –±–∏–Ω–∞—Ä–Ω—ã—Ö –º–∞—Å–æ–∫\n",
    "    \n",
    "    Args:\n",
    "        model: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å YOLO\n",
    "        dataset_path: –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É\n",
    "        img_size: –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞\n",
    "        conf_threshold: –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–π\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[List[np.ndarray], List[np.ndarray]]: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–∞—Å–∫–∏ –∏ –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–∞—Å–∫–∏\n",
    "    \"\"\"\n",
    "    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "    if model is None:\n",
    "        logger.error(\"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∞ (None)\")\n",
    "        return [], []\n",
    "    \n",
    "    if not dataset_path or not os.path.exists(dataset_path):\n",
    "        logger.error(f\"‚ùå –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {dataset_path}\")\n",
    "        return [], []\n",
    "    \n",
    "    logger.info(f\"üîç –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–∞: {dataset_path}\")\n",
    "    logger.info(f\"‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: img_size={img_size}, conf_threshold={conf_threshold}\")\n",
    "    \n",
    "    images_dir = os.path.join(dataset_path, 'images')\n",
    "    labels_dir = os.path.join(dataset_path, 'labels')\n",
    "    \n",
    "    logger.info(f\"üìÅ –ü–∞–ø–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {images_dir}\")\n",
    "    logger.info(f\"üè∑Ô∏è –ü–∞–ø–∫–∞ –º–µ—Ç–æ–∫: {labels_dir}\")\n",
    "    \n",
    "    if not os.path.exists(images_dir) or not os.path.exists(labels_dir):\n",
    "        logger.error(f\"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã –ø–∞–ø–∫–∏ images –∏–ª–∏ labels –≤ {dataset_path}\")\n",
    "        logger.error(f\"   images_dir exists: {os.path.exists(images_dir)}\")\n",
    "        logger.error(f\"   labels_dir exists: {os.path.exists(labels_dir)}\")\n",
    "        return [], []\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "    try:\n",
    "        subdirs = [d for d in os.listdir(images_dir) if os.path.isdir(os.path.join(images_dir, d))]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –ø–∞–ø–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {e}\")\n",
    "        return [], []\n",
    "    \n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    processed_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    if subdirs:  # –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞\n",
    "        logger.info(f\"üìÅ –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å {len(subdirs)} –ø–æ–¥–ø–∞–ø–∫–∞–º–∏: {subdirs}\")\n",
    "        image_label_pairs = collect_hierarchical_images(dataset_path)\n",
    "        \n",
    "        if not image_label_pairs:\n",
    "            logger.error(f\"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–∞—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-—Ä–∞–∑–º–µ—Ç–∫–∞ –≤ {dataset_path}\")\n",
    "            return [], []\n",
    "            \n",
    "        logger.info(f\"üîç –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(image_label_pairs)} –ø–∞—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-—Ä–∞–∑–º–µ—Ç–∫–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –º–∞—Å–æ–∫...\")\n",
    "        \n",
    "        for i, (img_path, label_path) in enumerate(image_label_pairs):\n",
    "            if i % 50 == 0:\n",
    "                logger.info(f\"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i}/{len(image_label_pairs)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–æ—à–∏–±–æ–∫: {error_count})\")\n",
    "            \n",
    "            try:\n",
    "                # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "                try:\n",
    "                    image = Image.open(img_path)\n",
    "                    img_width, img_height = image.size\n",
    "                    \n",
    "                    if img_width <= 0 or img_height <= 0:\n",
    "                        logger.warning(f\"‚ö†Ô∏è –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {img_path}: {img_width}x{img_height}\")\n",
    "                        error_count += 1\n",
    "                        continue\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {img_path}: {e}\")\n",
    "                    error_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏\n",
    "                try:\n",
    "                    results = model.predict(img_path, imgsz=img_size, conf=conf_threshold, verbose=False)\n",
    "                    if results and len(results) > 0:\n",
    "                        pred_mask = create_mask_from_yolo_results(results[0], img_width, img_height)\n",
    "                    else:\n",
    "                        logger.debug(f\"üîç –ü—É—Å—Ç—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è {img_path}\")\n",
    "                        pred_mask = np.zeros((img_height, img_width), dtype=np.uint8)\n",
    "                        \n",
    "                    if pred_mask is None:\n",
    "                        logger.warning(f\"‚ö†Ô∏è create_mask_from_yolo_results –≤–µ—Ä–Ω—É–ª–∞ None –¥–ª—è {img_path}\")\n",
    "                        pred_mask = np.zeros((img_height, img_width), dtype=np.uint8)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è {img_path}: {e}\")\n",
    "                    pred_mask = np.zeros((img_height, img_width), dtype=np.uint8)\n",
    "                    error_count += 1\n",
    "                \n",
    "                # –ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ç–∏–Ω–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏\n",
    "                try:\n",
    "                    if os.path.exists(label_path):\n",
    "                        gt_mask = create_mask_from_yolo_labels(label_path, img_width, img_height)\n",
    "                        \n",
    "                        if gt_mask is None:\n",
    "                            logger.warning(f\"‚ö†Ô∏è create_mask_from_yolo_labels –≤–µ—Ä–Ω—É–ª–∞ None –¥–ª—è {label_path}\")\n",
    "                            gt_mask = np.zeros((img_height, img_width), dtype=np.uint8)\n",
    "                    else:\n",
    "                        logger.debug(f\"üîç –§–∞–π–ª —Ä–∞–∑–º–µ—Ç–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {label_path}\")\n",
    "                        gt_mask = np.zeros((img_height, img_width), dtype=np.uint8)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–∞–∑–º–µ—Ç–∫–∏ –¥–ª—è {label_path}: {e}\")\n",
    "                    gt_mask = np.zeros((img_height, img_width), dtype=np.uint8)\n",
    "                    error_count += 1\n",
    "                \n",
    "                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤ –º–∞—Å–æ–∫\n",
    "                if pred_mask.shape != gt_mask.shape:\n",
    "                    logger.debug(f\"üîß –ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ –º–∞—Å–æ–∫: pred={pred_mask.shape}, gt={gt_mask.shape}\")\n",
    "                \n",
    "                # –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –º–∞—Å–æ–∫ –¥–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞\n",
    "                try:\n",
    "                    if pred_mask.shape != (img_size, img_size):\n",
    "                        pred_mask = cv2.resize(pred_mask, (img_size, img_size), interpolation=cv2.INTER_NEAREST)\n",
    "                    if gt_mask.shape != (img_size, img_size):\n",
    "                        gt_mask = cv2.resize(gt_mask, (img_size, img_size), interpolation=cv2.INTER_NEAREST)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –º–∞—Å–æ–∫ –¥–ª—è {img_path}: {e}\")\n",
    "                    pred_mask = np.zeros((img_size, img_size), dtype=np.uint8)\n",
    "                    gt_mask = np.zeros((img_size, img_size), dtype=np.uint8)\n",
    "                    error_count += 1\n",
    "                \n",
    "                predictions.append(pred_mask)\n",
    "                ground_truths.append(gt_mask)\n",
    "                processed_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–∞—Ä—ã {i+1}/{len(image_label_pairs)} ({img_path}): {e}\")\n",
    "                error_count += 1\n",
    "                continue\n",
    "            \n",
    "    else:  # –ü–ª–æ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞\n",
    "        logger.info(f\"üìÑ –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –ø–ª–æ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞\")\n",
    "        \n",
    "        try:\n",
    "            image_files = [f for f in os.listdir(images_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {e}\")\n",
    "            return [], []\n",
    "        \n",
    "        logger.info(f\"üîç –ù–∞–π–¥–µ–Ω–æ {len(image_files)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –ø–ª–æ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ\")\n",
    "        logger.info(f\"üîç –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(image_files)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –º–∞—Å–æ–∫...\")\n",
    "        \n",
    "        for i, img_file in enumerate(image_files):\n",
    "            if i % 50 == 0:\n",
    "                logger.info(f\"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i}/{len(image_files)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–æ—à–∏–±–æ–∫: {error_count})\")\n",
    "                \n",
    "            img_path = os.path.join(images_dir, img_file)\n",
    "            label_path = os.path.join(labels_dir, img_file.rsplit('.', 1)[0] + '.txt')\n",
    "            \n",
    "            try:\n",
    "                # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "                try:\n",
    "                    image = Image.open(img_path)\n",
    "                    img_width, img_height = image.size\n",
    "                    \n",
    "                    if img_width <= 0 or img_height <= 0:\n",
    "                        logger.warning(f\"‚ö†Ô∏è –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {img_file}: {img_width}x{img_height}\")\n",
    "                        error_count += 1\n",
    "                        continue\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {img_file}: {e}\")\n",
    "                    error_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏\n",
    "                try:\n",
    "                    results = model.predict(img_path, imgsz=img_size, conf=conf_threshold, verbose=False)\n",
    "                    if results and len(results) > 0:\n",
    "                        pred_mask = create_mask_from_yolo_results(results[0], img_width, img_height)\n",
    "                    else:\n",
    "                        logger.debug(f\"üîç –ü—É—Å—Ç—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è {img_file}\")\n",
    "                        pred_mask = np.zeros((img_height, img_width), dtype=np.uint8)\n",
    "                        \n",
    "                    if pred_mask is None:\n",
    "                        logger.warning(f\"‚ö†Ô∏è create_mask_from_yolo_results –≤–µ—Ä–Ω—É–ª–∞ None –¥–ª—è {img_file}\")\n",
    "                        pred_mask = np.zeros((img_height, img_width), dtype=np.uint8)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è {img_file}: {e}\")\n",
    "                    pred_mask = np.zeros((img_height, img_width), dtype=np.uint8)\n",
    "                    error_count += 1\n",
    "                \n",
    "                # –ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ç–∏–Ω–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏\n",
    "                try:\n",
    "                    if os.path.exists(label_path):\n",
    "                        gt_mask = create_mask_from_yolo_labels(label_path, img_width, img_height)\n",
    "                        \n",
    "                        if gt_mask is None:\n",
    "                            logger.warning(f\"‚ö†Ô∏è create_mask_from_yolo_labels –≤–µ—Ä–Ω—É–ª–∞ None –¥–ª—è {label_path}\")\n",
    "                            gt_mask = np.zeros((img_height, img_width), dtype=np.uint8)\n",
    "                    else:\n",
    "                        logger.debug(f\"üîç –§–∞–π–ª —Ä–∞–∑–º–µ—Ç–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {label_path}\")\n",
    "                        gt_mask = np.zeros((img_height, img_width), dtype=np.uint8)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–∞–∑–º–µ—Ç–∫–∏ –¥–ª—è {img_file}: {e}\")\n",
    "                    gt_mask = np.zeros((img_height, img_width), dtype=np.uint8)\n",
    "                    error_count += 1\n",
    "                \n",
    "                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤ –º–∞—Å–æ–∫\n",
    "                if pred_mask.shape != gt_mask.shape:\n",
    "                    logger.debug(f\"üîß –ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ –º–∞—Å–æ–∫: pred={pred_mask.shape}, gt={gt_mask.shape}\")\n",
    "                \n",
    "                # –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –º–∞—Å–æ–∫ –¥–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞\n",
    "                try:\n",
    "                    if pred_mask.shape != (img_size, img_size):\n",
    "                        pred_mask = cv2.resize(pred_mask, (img_size, img_size), interpolation=cv2.INTER_NEAREST)\n",
    "                    if gt_mask.shape != (img_size, img_size):\n",
    "                        gt_mask = cv2.resize(gt_mask, (img_size, img_size), interpolation=cv2.INTER_NEAREST)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –º–∞—Å–æ–∫ –¥–ª—è {img_file}: {e}\")\n",
    "                    pred_mask = np.zeros((img_size, img_size), dtype=np.uint8)\n",
    "                    gt_mask = np.zeros((img_size, img_size), dtype=np.uint8)\n",
    "                    error_count += 1\n",
    "                \n",
    "                predictions.append(pred_mask)\n",
    "                ground_truths.append(gt_mask)\n",
    "                processed_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {i+1}/{len(image_files)} ({img_file}): {e}\")\n",
    "                error_count += 1\n",
    "                continue\n",
    "    \n",
    "    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "    logger.info(f\"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ {dataset_path}\")\n",
    "    logger.info(f\"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed_count} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –æ—à–∏–±–æ–∫ {error_count}\")\n",
    "    logger.info(f\"üìä –ü–æ–ª—É—á–µ–Ω–æ {len(predictions)} –º–∞—Å–æ–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∏ {len(ground_truths)} GT –º–∞—Å–æ–∫\")\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "    if not predictions or not ground_truths:\n",
    "        logger.error(f\"‚ùå –ü–æ–ª—É—á–µ–Ω—ã –ø—É—Å—Ç—ã–µ —Å–ø–∏—Å–∫–∏ –º–∞—Å–æ–∫! predictions: {len(predictions)}, ground_truths: {len(ground_truths)}\")\n",
    "        return [], []\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞\n",
    "    if len(predictions) != len(ground_truths):\n",
    "        logger.error(f\"‚ùå –ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –º–∞—Å–æ–∫! predictions: {len(predictions)}, ground_truths: {len(ground_truths)}\")\n",
    "        return [], []\n",
    "    \n",
    "    return predictions, ground_truths\n",
    "\n",
    "\n",
    "def create_mask_from_yolo_results(results, img_width: int, img_height: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    –°–æ–∑–¥–∞–Ω–∏–µ –±–∏–Ω–∞—Ä–Ω–æ–π –º–∞—Å–∫–∏ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ YOLO\n",
    "    \n",
    "    Args:\n",
    "        results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ—Ç–µ–∫—Ü–∏–∏ YOLO\n",
    "        img_width: –®–∏—Ä–∏–Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "        img_height: –í—ã—Å–æ—Ç–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: –ë–∏–Ω–∞—Ä–Ω–∞—è –º–∞—Å–∫–∞\n",
    "    \"\"\"\n",
    "    mask = np.zeros((img_height, img_width), dtype=np.uint8)\n",
    "    \n",
    "    if results.boxes is not None and len(results.boxes) > 0:\n",
    "        boxes = results.boxes.xyxy.cpu().numpy()  # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã bbox –≤ —Ñ–æ—Ä–º–∞—Ç–µ x1,y1,x2,y2\n",
    "        \n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = map(int, box[:4])\n",
    "            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —Ä–∞–∑–º–µ—Ä–∞–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "            x1 = max(0, min(x1, img_width-1))\n",
    "            y1 = max(0, min(y1, img_height-1))\n",
    "            x2 = max(0, min(x2, img_width-1))\n",
    "            y2 = max(0, min(y2, img_height-1))\n",
    "            \n",
    "            # –ó–∞–ø–æ–ª–Ω—è–µ–º –æ–±–ª–∞—Å—Ç—å bbox –µ–¥–∏–Ω–∏—Ü–∞–º–∏\n",
    "            mask[y1:y2+1, x1:x2+1] = 1\n",
    "    \n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask_from_yolo_labels(label_path: str, img_width: int, img_height: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    –°–æ–∑–¥–∞–Ω–∏–µ –±–∏–Ω–∞—Ä–Ω–æ–π –º–∞—Å–∫–∏ –∏–∑ YOLO —Ä–∞–∑–º–µ—Ç–∫–∏\n",
    "    \n",
    "    Args:\n",
    "        label_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Ä–∞–∑–º–µ—Ç–∫–∏\n",
    "        img_width: –®–∏—Ä–∏–Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "        img_height: –í—ã—Å–æ—Ç–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: –ë–∏–Ω–∞—Ä–Ω–∞—è –º–∞—Å–∫–∞\n",
    "    \"\"\"\n",
    "    mask = np.zeros((img_height, img_width), dtype=np.uint8)\n",
    "    \n",
    "    if not os.path.exists(label_path):\n",
    "        return mask\n",
    "    \n",
    "    try:\n",
    "        with open(label_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 5:\n",
    "                # YOLO —Ñ–æ—Ä–º–∞—Ç: class_id center_x center_y width height (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã)\n",
    "                center_x = float(parts[1]) * img_width\n",
    "                center_y = float(parts[2]) * img_height\n",
    "                width = float(parts[3]) * img_width\n",
    "                height = float(parts[4]) * img_height\n",
    "                \n",
    "                # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã bbox\n",
    "                x1 = int(center_x - width/2)\n",
    "                y1 = int(center_y - height/2)\n",
    "                x2 = int(center_x + width/2)\n",
    "                y2 = int(center_y + height/2)\n",
    "                \n",
    "                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —Ä–∞–∑–º–µ—Ä–∞–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "                x1 = max(0, min(x1, img_width-1))\n",
    "                y1 = max(0, min(y1, img_height-1))\n",
    "                x2 = max(0, min(x2, img_width-1))\n",
    "                y2 = max(0, min(y2, img_height-1))\n",
    "                \n",
    "                # –ó–∞–ø–æ–ª–Ω—è–µ–º –æ–±–ª–∞—Å—Ç—å bbox –µ–¥–∏–Ω–∏—Ü–∞–º–∏\n",
    "                mask[y1:y2+1, x1:x2+1] = 1\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ç–∫–∏ {label_path}: {e}\")\n",
    "    \n",
    "    return mask\n",
    "\n",
    "\n",
    "def calculate_iou(region_a: np.ndarray, region_b: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ IoU –º–µ–∂–¥—É –¥–≤—É–º—è –æ–±–ª–∞—Å—Ç—è–º–∏ –ø–æ —Ñ–æ—Ä–º—É–ª–µ (3)\n",
    "    \n",
    "    Args:\n",
    "        region_a: –ü–µ—Ä–≤–∞—è –æ–±–ª–∞—Å—Ç—å (–±–∏–Ω–∞—Ä–Ω–∞—è –º–∞—Å–∫–∞)\n",
    "        region_b: –í—Ç–æ—Ä–∞—è –æ–±–ª–∞—Å—Ç—å (–±–∏–Ω–∞—Ä–Ω–∞—è –º–∞—Å–∫–∞)\n",
    "        \n",
    "    Returns:\n",
    "        float: –ó–Ω–∞—á–µ–Ω–∏–µ IoU\n",
    "    \"\"\"\n",
    "    intersection = np.logical_and(region_a, region_b).sum()\n",
    "    union = np.logical_or(region_a, region_b).sum()\n",
    "    \n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return intersection / union\n",
    "\n",
    "\n",
    "def create_iou_matrix(pred_regions: List[np.ndarray], gt_regions: List[np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    –°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã IoU –º–µ–∂–¥—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º–∏ –∏ –∏—Å—Ç–∏–Ω–Ω—ã–º–∏ –æ–±–ª–∞—Å—Ç—è–º–∏\n",
    "    \n",
    "    Args:\n",
    "        pred_regions: –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π\n",
    "        gt_regions: –°–ø–∏—Å–æ–∫ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: –ú–∞—Ç—Ä–∏—Ü–∞ IoU —Ä–∞–∑–º–µ—Ä–æ–º [len(pred_regions), len(gt_regions)]\n",
    "    \"\"\"\n",
    "    iou_matrix = np.zeros((len(pred_regions), len(gt_regions)))\n",
    "    \n",
    "    for i, pred_region in enumerate(pred_regions):\n",
    "        for j, gt_region in enumerate(gt_regions):\n",
    "            iou_matrix[i, j] = calculate_iou(pred_region, gt_region)\n",
    "    \n",
    "    return iou_matrix\n",
    "\n",
    "\n",
    "def count_tp_fp_fn_from_matrix(iou_matrix: np.ndarray, threshold: float) -> Tuple[int, int, int]:\n",
    "    \"\"\"\n",
    "    –ü–æ–¥—Å—á–µ—Ç TP, FP, FN –∏–∑ –º–∞—Ç—Ä–∏—Ü—ã IoU —Å–æ–≥–ª–∞—Å–Ω–æ –æ–ø–∏—Å–∞–Ω–Ω–æ–º—É –∞–ª–≥–æ—Ä–∏—Ç–º—É\n",
    "    \n",
    "    Args:\n",
    "        iou_matrix: –ú–∞—Ç—Ä–∏—Ü–∞ IoU\n",
    "        threshold: –ü–æ—Ä–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[int, int, int]: TP, FP, FN\n",
    "    \"\"\"\n",
    "    tp = 0\n",
    "    matrix_copy = iou_matrix.copy()\n",
    "    \n",
    "    while matrix_copy.size > 0:\n",
    "        # –ü–æ–∏—Å–∫ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞\n",
    "        max_val = np.max(matrix_copy)\n",
    "        \n",
    "        if max_val >= threshold:\n",
    "            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º TP\n",
    "            tp += 1\n",
    "            \n",
    "            # –ù–∞—Ö–æ–¥–∏–º –ø–æ–∑–∏—Ü–∏—é –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞\n",
    "            max_pos = np.unravel_index(np.argmax(matrix_copy), matrix_copy.shape)\n",
    "            row_idx, col_idx = max_pos\n",
    "            \n",
    "            # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫—É –∏ —Å—Ç–æ–ª–±–µ—Ü\n",
    "            matrix_copy = np.delete(matrix_copy, row_idx, axis=0)\n",
    "            matrix_copy = np.delete(matrix_copy, col_idx, axis=1)\n",
    "        else:\n",
    "            # –ü—Ä–µ—Ä—ã–≤–∞–µ–º –ø—Ä–æ—Ü–µ–¥—É—Ä—É\n",
    "            break\n",
    "    \n",
    "    # FN = –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è —Å—Ç–æ–ª–±—Ü–æ–≤ (–∏—Å—Ç–∏–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤)\n",
    "    fn = matrix_copy.shape[1] if matrix_copy.size > 0 else 0\n",
    "    \n",
    "    # FP = –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è —Å—Ç—Ä–æ–∫ (–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤)\n",
    "    fp = matrix_copy.shape[0] if matrix_copy.size > 0 else 0\n",
    "    \n",
    "    return tp, fp, fn\n",
    "\n",
    "\n",
    "def cleanup_debug_files():\n",
    "    \"\"\"–û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö debug —Ñ–∞–π–ª–æ–≤\"\"\"\n",
    "    debug_base_dir = '/kaggle/working'\n",
    "    \n",
    "    if os.path.exists(debug_base_dir):\n",
    "        try:\n",
    "            shutil.rmtree(debug_base_dir)\n",
    "            logger.info(\"üßπ –í—Ä–µ–º–µ–Ω–Ω—ã–µ debug —Ñ–∞–π–ª—ã –æ—á–∏—â–µ–Ω—ã\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ debug —Ñ–∞–π–ª–æ–≤: {e}\")\n",
    "    \n",
    "    # –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö YAML —Ñ–∞–π–ª–æ–≤\n",
    "    temp_yaml_files = ['kaggle_combined_data.yaml', 'combined_data.yaml', 'private_validation.yaml']\n",
    "    for yaml_file in temp_yaml_files:\n",
    "        if os.path.exists(yaml_file):\n",
    "            try:\n",
    "                os.remove(yaml_file)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "def analyze_all_datasets() -> Dict:\n",
    "    \"\"\"–ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\"\"\"\n",
    "    logger.info(\"üìä –ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤...\")\n",
    "    \n",
    "    datasets = {\n",
    "        'train_dataset_1': get_dataset_statistics(TRAIN_DATASET_1),\n",
    "        'train_dataset_2': get_dataset_statistics(TRAIN_DATASET_2),\n",
    "        'val_dataset_public': get_dataset_statistics(VAL_DATASET_PUBLIC),\n",
    "        'val_dataset_private': get_dataset_statistics(VAL_DATASET_PRIVATE)\n",
    "    }\n",
    "    \n",
    "    total_train_images = 0\n",
    "    total_train_labels = 0\n",
    "    \n",
    "    for name, stats in datasets.items():\n",
    "        if stats['exists']:\n",
    "            structure_info = f\" ({stats['structure_type']})\" if 'structure_type' in stats else \"\"\n",
    "            logger.info(f\"‚úÖ {name}{structure_info}: {stats['images_count']} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, {stats['labels_count']} –º–µ—Ç–æ–∫\")\n",
    "            if 'train' in name:\n",
    "                total_train_images += stats['images_count']\n",
    "                total_train_labels += stats['labels_count']\n",
    "            if stats['classes_distribution']:\n",
    "                logger.info(f\"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {stats['classes_distribution']}\")\n",
    "        else:\n",
    "            logger.warning(f\"‚ùå {name}: –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω\")\n",
    "    \n",
    "    logger.info(f\"üéØ –ò—Ç–æ–≥–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {total_train_images} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, {total_train_labels} –º–µ—Ç–æ–∫\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# ===== –§–£–ù–ö–¶–ò–ò –ò–ó METRIC.PY =====\n",
    "\n",
    "# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è –º–µ—Ç—Ä–∏–∫\n",
    "PUBLIC_GT_CSV_PATH: str = 'public_gt_solution_24-10-24.csv'\n",
    "COLUMNS = ['image_id', 'label', 'xc', 'yc', 'w', 'h', 'w_img', 'h_img', 'score', 'time_spent']\n",
    "EXTENSION = '.jpg'  # –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∞ –∏–∑ metric_counter.py\n",
    "\n",
    "def df_to_bytes(df: pd.DataFrame) -> bytes:\n",
    "    df_byte: bytes = df.to_json().encode(encoding=\"utf-8\")\n",
    "    \n",
    "    return df_byte\n",
    "\n",
    "def bytes_to_dict(df_byte: bytes) -> dict:\n",
    "    if isinstance(df_byte, bytes):\n",
    "        df_byte = df_byte.decode(\"utf-8\")\n",
    "    df_byte = df_byte.replace(\"'\", '\"')\n",
    "    df_dict: dict = json.loads(df_byte)\n",
    "    \n",
    "    return df_dict\n",
    "\n",
    "def bytes_to_df(df_byte: bytes) -> pd.DataFrame:\n",
    "    predicted_dict = bytes_to_dict(df_byte)\n",
    "    predicted_df = pd.DataFrame(predicted_dict)\n",
    "    \n",
    "    return predicted_df\n",
    "\n",
    "def open_df_as_bytes(csv_path: str) -> bytes:\n",
    "    df = pd.read_csv(csv_path, sep=\",\", decimal=\".\", \n",
    "                     converters={'image_id': str,\n",
    "                                 'time_spent': float})\n",
    "    df_bytes = df_to_bytes(df)\n",
    "\n",
    "    return df_bytes\n",
    "\n",
    "def set_types(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.astype({'image_id': str,\n",
    "        'label': int,\n",
    "        'xc': float,\n",
    "        'yc': float,\n",
    "        'w': float,\n",
    "        'h': float,\n",
    "        'w_img': int,\n",
    "        'h_img': int,\n",
    "        },\n",
    "        errors = 'ignore'\n",
    "    )\n",
    "\n",
    "def get_time_spent(df: pd.DataFrame, m: int) -> np.ndarray:\n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ time_spent –¥–ª—è –∫–∞–∂–¥–æ–≥–æ image_id –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ\n",
    "    for image_id, group in df.groupby('image_id'):\n",
    "        assert group['time_spent'].nunique() == 1, f\"–†–∞–∑–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è time_spent –¥–ª—è image_id: {image_id}\"\n",
    "    \n",
    "    # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ time_spent –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ image_id\n",
    "    time_spent = df.groupby('image_id')['time_spent'].first()\n",
    "    time_spent = time_spent.reset_index()['time_spent'].values\n",
    "    assert len(time_spent) == m, f'–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—ä–µ–∫—Ç–æ–≤ time_spent –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å {m} (—É –≤–∞—Å {len(time_spent)})'\n",
    "    \n",
    "    return time_spent\n",
    "\n",
    "def preprocess_predicted_df(predicted_file: bytes, gt_file: bytes, m: int) -> Tuple[pd.DataFrame, pd.DataFrame, np.ndarray]:\n",
    "    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –±–∞–π—Ç–∫–æ–¥ –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º\n",
    "    if gt_file is None:\n",
    "        gt_file: bytes = open_df_as_bytes(PUBLIC_GT_CSV_PATH)\n",
    "    predicted_df = bytes_to_df(predicted_file)\n",
    "    gt_df = bytes_to_df(gt_file)\n",
    "    # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "    assert set(predicted_df.columns) == set(COLUMNS), Exception(f'–û—à–∏–±–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏—è —Å—Ç–æ–ª–±—Ü–æ–≤ –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ: —É –≤–∞—Å {list(predicted_df.columns)}, –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å {COLUMNS}')\n",
    "\n",
    "    assert 'score' in predicted_df, \"–î–∞—Ç–∞—Ñ—Ä–µ–π–º –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±–µ—Ü score –≤—Ä–µ–º–µ–Ω–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏\"\n",
    "    assert not np.any(predicted_df['w'].values < 0.0) and not np.any(predicted_df['w'].values > 1), \"–®–∏—Ä–∏–Ω–∞ (w) –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö [0, 1].\"\n",
    "    assert not np.any(predicted_df['h'].values < 0.0) and not np.any(predicted_df['h'].values > 1), \"–í—ã—Å–æ—Ç–∞ (h) –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö [0, 1].\"\n",
    "    assert not np.any(predicted_df['w_img'].values < 1) and not np.any(predicted_df['w_img'].values > 15000), \"–®–∏—Ä–∏–Ω–∞ (w_img) –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö [0, 15000].\"\n",
    "    assert not np.any(predicted_df['h_img'].values < 1) and not np.any(predicted_df['h_img'].values > 15000), \"–í—ã—Å–æ—Ç–∞ (h_img) –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö [0, 15000].\"\n",
    "    assert not np.any(predicted_df['xc'].values < 0.0) and not np.any(predicted_df['xc'].values > 1.0), \"–¶–µ–Ω—Ç—Ä –æ–±—ä–µ–∫—Ç–∞ (xc) –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö [0, 1].\"\n",
    "    assert not np.any(predicted_df['yc'].values < 0.0) and not np.any(predicted_df['yc'].values > 1.0), \"–¶–µ–Ω—Ç—Ä –æ–±—ä–µ–∫—Ç–∞ (yc) –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö [0, 1].\"\n",
    "    assert not np.any(predicted_df['score'].values < 0.0) and not np.any(predicted_df['score'].values > 1.0), \"–°—Ç–æ–ª–±–µ—Ü score –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö [0, 1]\"\n",
    "    assert 'time_spent' in predicted_df, \"–î–∞—Ç–∞—Ñ—Ä–µ–π–º –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±–µ—Ü time_spent –≤—Ä–µ–º–µ–Ω–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏\"\n",
    "    \n",
    "    # –ó–∞–±–∏—Ä–∞–µ–º –≤—Ä–µ–º—è, –≤ —Ç–æ–º —á–∏—Å–ª–µ –¥–ª—è –ø—É—Å—Ç—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π, –∏ —É–¥–∞–ª—è–µ–º –∏–∑ df\n",
    "    time_spent = get_time_spent(df=predicted_df, m=m)\n",
    "    del predicted_df['time_spent']\n",
    "    predicted_df = predicted_df.dropna()\n",
    "    \n",
    "    # –ü—Ä–∏–≤–æ–¥–∏–º —Ñ–æ—Ä–º–∞—Ç—ã —Å—Ç–æ–ª–±—Ü–æ–≤ –∫ —Ç–∏–ø–∞–º\n",
    "    predicted_df = set_types(predicted_df)\n",
    "    gt_df = set_types(gt_df)\n",
    "    \n",
    "    # –î–µ–ª–∞–µ–º image_id –∏–Ω–¥–µ–∫—Å–æ–º –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–æ—Ä—è–¥–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤\n",
    "    gt_df = gt_df.set_index('image_id').sort_index()\n",
    "    predicted_df = predicted_df.set_index('image_id').sort_index()\n",
    "    \n",
    "    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
    "    unique_image_ids = tuple(set(predicted_df.index.to_list() + gt_df.index.to_list()))\n",
    "    assert len(unique_image_ids) <= m, Exception(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö ID –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–µ –¥–æ–ª–∂–Ω–æ –ø—Ä–µ–≤—ã—à–∞—Ç—å {m}!\")\n",
    "    assert not predicted_df.empty and not gt_df.empty, \"–î–∞—Ç–∞—Ñ—Ä–µ–π–º—ã –Ω–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø—É—Å—Ç—ã–º–∏\"\n",
    "\n",
    "    return gt_df, predicted_df, time_spent\n",
    "\n",
    "\n",
    "def get_box_coordinates(row):\n",
    "    \"\"\"–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ü–µ–Ω—Ç—Ä –∏ —Ä–∞–∑–º–µ—Ä—ã –≤ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —É–≥–ª–æ–≤ –±–æ–∫—Å–∞\"\"\"\n",
    "    w_img = int(row['w_img'])\n",
    "    h_img = int(row['h_img'])\n",
    "    \n",
    "    x1 = int((row['xc'] - row['w']/2) * w_img)\n",
    "    y1 = int((row['yc'] - row['h']/2) * h_img)\n",
    "    x2 = int((row['xc'] + row['w']/2) * w_img)\n",
    "    y2 = int((row['yc'] + row['h']/2) * h_img)\n",
    "    \n",
    "    return (x1, y1, x2, y2)\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def compute_iou_from_coords(pred_box, gt_box):\n",
    "    \"\"\"–í—ã—á–∏—Å–ª—è–µ—Ç IoU –º–µ–∂–¥—É –¥–≤—É–º—è –±–æ–∫—Å–∞–º–∏ –ø–æ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º\"\"\"\n",
    "    # pred_box –∏ gt_box –≤ —Ñ–æ—Ä–º–∞—Ç–µ (x1, y1, x2, y2)\n",
    "    x1_p, y1_p, x2_p, y2_p = pred_box\n",
    "    x1_g, y1_g, x2_g, y2_g = gt_box\n",
    "    # –ù–∞—Ö–æ–¥–∏–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è\n",
    "    x_left = max(x1_p, x1_g)\n",
    "    y_top = max(y1_p, y1_g)\n",
    "    x_right = min(x2_p, x2_g)\n",
    "    y_bottom = min(y2_p, y2_g)\n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "    \n",
    "    # –ü–ª–æ—â–∞–¥–∏ –±–æ–∫—Å–æ–≤\n",
    "    box1_area = (x2_p - x1_p) * (y2_p - y1_p)\n",
    "    box2_area = (x2_g - x1_g) * (y2_g - y1_g)\n",
    "    union_area = box1_area + box2_area - intersection_area\n",
    "    \n",
    "    return intersection_area / union_area if union_area > 0 else 0.0\n",
    "\n",
    "\n",
    "def process_image(pred_df, gt_df, thresholds):\n",
    "    \"\"\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\"\"\"\n",
    "    pred_boxes = [get_box_coordinates(row) for _, row in pred_df.iterrows()]\n",
    "    gt_boxes = [get_box_coordinates(row) for _, row in gt_df.iterrows()]\n",
    "    \n",
    "    num_pred = len(pred_boxes)\n",
    "    num_gt = len(gt_boxes)\n",
    "    iou_matrix = np.zeros((num_pred, num_gt))\n",
    "    \n",
    "    for i, pred_box in enumerate(pred_boxes):\n",
    "        for j, gt_box in enumerate(gt_boxes):\n",
    "            iou_matrix[i, j] = compute_iou_from_coords(pred_box, gt_box)\n",
    "    \n",
    "    results = {}\n",
    "    for t in thresholds:\n",
    "        matches = []\n",
    "        iou_mat = iou_matrix.copy()\n",
    "        iou_mat[iou_mat < t] = 0\n",
    "        \n",
    "        pred_indices = set()\n",
    "        gt_indices = set()\n",
    "        \n",
    "        while True:\n",
    "            max_iou = iou_mat.max()\n",
    "            if max_iou == 0:\n",
    "                break\n",
    "            i, j = np.unravel_index(np.argmax(iou_mat), iou_mat.shape)\n",
    "            if i not in pred_indices and j not in gt_indices:\n",
    "                pred_indices.add(i)\n",
    "                gt_indices.add(j)\n",
    "                matches.append((i, j))\n",
    "            iou_mat[i, :] = 0\n",
    "            iou_mat[:, j] = 0\n",
    "        \n",
    "        tp = len(matches)\n",
    "        fp = num_pred - tp\n",
    "        fn = num_gt - tp\n",
    "        \n",
    "        results[t] = {'tp': tp, 'fp': fp, 'fn': fn}\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def compute_overall_metric(\n",
    "        predicted_df: pd.DataFrame,\n",
    "        gt_df: pd.DataFrame,\n",
    "        time_spent: np.ndarray,\n",
    "        thresholds: np.ndarray,\n",
    "        beta: float,\n",
    "        m: int,\n",
    "        parallelize: bool = True\n",
    "    ) -> np.float64:\n",
    "    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
    "    unique_image_ids = tuple(set(predicted_df.index.to_list() + gt_df.index.to_list()))\n",
    "\n",
    "    total_tp = {t: 0 for t in thresholds}\n",
    "    total_fp = {t: 0 for t in thresholds}\n",
    "    total_fn = {t: 0 for t in thresholds}\n",
    "    \n",
    "    def process_image_id(image_id):\n",
    "        pred_df_image_id = predicted_df[predicted_df.index == image_id]\n",
    "        gt_df_image_id = gt_df[gt_df.index == image_id]\n",
    "        \n",
    "        # –°–ª—É—á–∞–π, –∫–æ–≥–¥–∞ –∏—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –µ—Å—Ç—å, –∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–µ—Ç\n",
    "        if not gt_df_image_id.empty and pred_df_image_id.empty:\n",
    "            num_gt = np.float64(len(gt_df_image_id))\n",
    "            return {t: {'tp': 0, 'fp': 0, 'fn': num_gt} for t in thresholds}\n",
    "        \n",
    "        # –°–ª—É—á–∞–π, –∫–æ–≥–¥–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –µ—Å—Ç—å, –∞ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –Ω–µ—Ç\n",
    "        elif not pred_df_image_id.empty and gt_df_image_id.empty:\n",
    "            num_pred = np.float64(len(pred_df_image_id))\n",
    "            return {t: {'tp': 0, 'fp': num_pred, 'fn': 0} for t in thresholds}\n",
    "        \n",
    "        # –û–±–∞ —Å–ª—É—á–∞—è –Ω–µ –ø—É—Å—Ç—ã–µ\n",
    "        elif not pred_df_image_id.empty and not gt_df_image_id.empty:\n",
    "            # –í–º–µ—Å—Ç–æ —Ä–∞–±–æ—Ç—ã —Å –º–∞—Å–∫–∞–º–∏ –ø–µ—Ä–µ–¥–∞–µ–º –Ω–∞–ø—Ä—è–º—É—é –¥–∞—Ç–∞—Ñ—Ä–µ–π–º—ã\n",
    "            result = process_image(pred_df_image_id, gt_df_image_id, thresholds)\n",
    "            return result\n",
    "        \n",
    "        return None\n",
    "\n",
    "    results = []\n",
    "    if parallelize:\n",
    "        # –†–∞—Å–ø–∞—Ä–∞–ª–ª–µ–ª–µ–Ω–Ω–æ–µ –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            results = list(executor.map(process_image_id, unique_image_ids))\n",
    "    else:\n",
    "        # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π —Ü–∏–∫–ª –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è\n",
    "        for image_id in unique_image_ids:\n",
    "            results.append(process_image_id(image_id))\n",
    "\n",
    "    for result in results:\n",
    "        if result:\n",
    "            for t in thresholds:\n",
    "                total_tp[t] += result[t]['tp']\n",
    "                total_fp[t] += result[t]['fp']\n",
    "                total_fn[t] += result[t]['fn']\n",
    "\n",
    "    metric, accuracy, fp_rate, avg_time = metric_counter(\n",
    "        time_spent=time_spent,\n",
    "        total_tp=total_tp,\n",
    "        total_fp=total_fp,\n",
    "        total_fn=total_fn,\n",
    "        thresholds=thresholds,\n",
    "        beta=beta,\n",
    "        m=m)\n",
    "    \n",
    "    return metric, accuracy, fp_rate, avg_time\n",
    "\n",
    "\n",
    "def metric_counter(\n",
    "        time_spent: np.ndarray,\n",
    "        total_tp: dict,\n",
    "        total_fp: dict,\n",
    "        total_fn: dict,\n",
    "        thresholds: np.ndarray,\n",
    "        beta: float,\n",
    "        m: int) -> np.float64:\n",
    "    # –†–∞—á–µ—Ç Q (F-beta score)\n",
    "    f_beta_scores = [] \n",
    "    beta_squared = beta ** 2\n",
    "    for t in thresholds:\n",
    "        tp = total_tp[t]\n",
    "        fp = total_fp[t]\n",
    "        fn = total_fn[t]\n",
    "        numerator = (1 + beta_squared) * tp\n",
    "        denominator = (1 + beta_squared) * tp + beta_squared * fn + fp\n",
    "        if denominator == 0:\n",
    "            f_beta_t = 0.0\n",
    "        else:\n",
    "            f_beta_t = numerator / denominator\n",
    "        f_beta_scores.append(f_beta_t)\n",
    "    \n",
    "    # –ú–µ—Ç—Ä–∏–∫–∞ —Ç–µ–ø–µ—Ä—å —Ä–∞–≤–Ω–∞ —Ç–æ–ª—å–∫–æ F-beta score (–±–µ–∑ —É—á–µ—Ç–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏)\n",
    "    metric = (1 / len(thresholds)) * np.sum(f_beta_scores)\n",
    "    metric = np.round(metric, decimals=10)\n",
    "    \n",
    "    # –¢–æ—á–Ω–æ—Å—Ç—å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è\n",
    "    accuracy = {float(t): round(float((total_tp[t] / (total_tp[t] + total_fp[t])) * 100), 3) \\\n",
    "            if (total_tp[t] + total_fp[t]) > 0 else 0 for t in thresholds}\n",
    "    # –ß–∏—Å–ª–æ –ª–æ–∂–Ω–æ–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π \n",
    "    fp_rate = {float(t): round(float((total_fp[t] / (total_fp[t] + total_tp[t])) * 100), 3) \\\n",
    "            if (total_fp[t] + total_tp[t]) > 0 else 0 for t in thresholds}\n",
    "    # –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É —Å–Ω–∏–º–∫–∞ \n",
    "    avg_time = round(float(np.mean(time_spent)), 3)\n",
    "\n",
    "    return metric, accuracy, fp_rate, avg_time\n",
    "\n",
    "\n",
    "def evaluate(predicted_file: bytes,\n",
    "        gt_file: bytes = None,\n",
    "        thresholds: np.ndarray = np.round(np.arange(0.3, 1.0, 0.07), 2),\n",
    "        beta: float = 1.0,\n",
    "        m: int = 500,\n",
    "        parallelize: bool = True) -> float:\n",
    "    metric, accuracy, fp_rate, avg_time = 0.0, {}, {}, 0.0\n",
    "    try:\n",
    "        # –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö, –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–æ–≤, –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–∏–ø–æ–≤\n",
    "        gt_df, predicted_df, time_spent = preprocess_predicted_df(gt_file=gt_file,\n",
    "                                                predicted_file=predicted_file,\n",
    "                                                m=m\n",
    "        )\n",
    "        # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫–∏, –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ (parallelize=True)\n",
    "        metric, accuracy, fp_rate, avg_time = compute_overall_metric(predicted_df=predicted_df,\n",
    "                    gt_df=gt_df,\n",
    "                    thresholds=thresholds,\n",
    "                    beta=beta,\n",
    "                    m=m,\n",
    "                    parallelize=parallelize,\n",
    "                    time_spent=time_spent\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å–∫—Ä–∏–ø—Ç–∞: {str(e)}\")\n",
    "    \n",
    "    return metric, accuracy, fp_rate, avg_time\n",
    "\n",
    "\n",
    "def predict(images: List[np.ndarray]) -> List[List[Dict]]:\n",
    "    \"\"\"–§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–Ω–æ–π YOLO –º–æ–¥–µ–ª–∏\n",
    "    \n",
    "    Args:\n",
    "        images: –°–ø–∏—Å–æ–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ numpy array (RGB)\n",
    "        \n",
    "    Returns:\n",
    "        List[List[Dict]]: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è,\n",
    "                         –≥–¥–µ –∫–∞–∂–¥—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–ª–æ–≤–∞—Ä–∏ —Å –∫–ª—é—á–∞–º–∏:\n",
    "                         'xc', 'yc', 'w', 'h', 'label', 'score'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à—É—é –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å\n",
    "        model_path = './models/yolov13_uav_runs/dynamic_routing_experiment/weights/best.pt'\n",
    "        if not os.path.exists(model_path):\n",
    "            # –ü–æ–ø—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –ø—É—Ç–∏\n",
    "            alternative_paths = [\n",
    "                './yolov13_uav_runs/dynamic_routing_experiment/weights/best.pt',\n",
    "                './runs/detect/train/weights/best.pt',\n",
    "                './best.pt'\n",
    "            ]\n",
    "            for alt_path in alternative_paths:\n",
    "                if os.path.exists(alt_path):\n",
    "                    model_path = alt_path\n",
    "                    break\n",
    "            else:\n",
    "                logger.error(f\"–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–æ –ø—É—Ç–∏ {model_path}\")\n",
    "                return [[] for _ in images]\n",
    "        \n",
    "        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å\n",
    "        model = YOLO(model_path)\n",
    "        logger.info(f\"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {model_path}\")\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        for image in images:\n",
    "            image_results = []\n",
    "            \n",
    "            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º numpy array –≤ PIL Image –¥–ª—è YOLO\n",
    "            if isinstance(image, np.ndarray):\n",
    "                # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ\n",
    "                if image.dtype != np.uint8:\n",
    "                    image = (image * 255).astype(np.uint8)\n",
    "                \n",
    "                # YOLO –æ–∂–∏–¥–∞–µ—Ç BGR, –Ω–æ —É –Ω–∞—Å RGB\n",
    "                image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            else:\n",
    "                image_bgr = image\n",
    "            \n",
    "            # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "            h_img, w_img = image_bgr.shape[:2]\n",
    "            \n",
    "            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
    "            results = model(image_bgr, conf=0.25, iou=0.45, verbose=False)\n",
    "            \n",
    "            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "            if results and len(results) > 0:\n",
    "                result = results[0]\n",
    "                if result.boxes is not None and len(result.boxes) > 0:\n",
    "                    boxes = result.boxes\n",
    "                    \n",
    "                    for i in range(len(boxes)):\n",
    "                        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ xyxy\n",
    "                        xyxy = boxes.xyxy[i].cpu().numpy()\n",
    "                        conf = float(boxes.conf[i].cpu().numpy())\n",
    "                        cls = int(boxes.cls[i].cpu().numpy())\n",
    "                        \n",
    "                        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º xyxy –≤ xywh (—Ü–µ–Ω—Ç—Ä + —Ä–∞–∑–º–µ—Ä—ã)\n",
    "                        x1, y1, x2, y2 = xyxy\n",
    "                        xc = (x1 + x2) / 2\n",
    "                        yc = (y1 + y2) / 2\n",
    "                        w = x2 - x1\n",
    "                        h = y2 - y1\n",
    "                        \n",
    "                        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ä–∞–∑–º–µ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "                        xc_norm = xc / w_img\n",
    "                        yc_norm = yc / h_img\n",
    "                        w_norm = w / w_img\n",
    "                        h_norm = h / h_img\n",
    "                        \n",
    "                        detection = {\n",
    "                            'xc': float(xc_norm),\n",
    "                            'yc': float(yc_norm),\n",
    "                            'w': float(w_norm),\n",
    "                            'h': float(h_norm),\n",
    "                            'label': cls,\n",
    "                            'score': conf\n",
    "                        }\n",
    "                        image_results.append(detection)\n",
    "            \n",
    "            all_results.append(image_results)\n",
    "        \n",
    "        return all_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"–û—à–∏–±–∫–∞ –≤ —Ñ—É–Ω–∫—Ü–∏–∏ predict: {str(e)}\")\n",
    "        return [[] for _ in images]\n",
    "\n",
    "\n",
    "def process_images_adapted(images_path: str, result_csv_path: str = None) -> pd.DataFrame:\n",
    "    \"\"\"–ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–∞–ø–∫–∏ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –∏–∑ metric_counter.py\n",
    "    \n",
    "    Args:\n",
    "        images_path (str): –ø—É—Ç—å –¥–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏\n",
    "        result_csv_path (str, optional): –ü—É—Ç—å, –∫—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: –¥–∞—Ç–∞—Ñ—Ä–µ–π–º —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
    "    \"\"\"\n",
    "    # –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –∏–∑ metric_counter.py\n",
    "    EXTENSION = '.jpg'\n",
    "    COLUMNS = ['image_id', 'label', 'xc', 'yc', 'w', 'h', 'w_img', 'h_img', 'score', 'time_spent']\n",
    "    \n",
    "    # –§–∏–∫—Å–∏—Ä—É–µ–º —Å–∏–¥—ã\n",
    "    SEED = 42\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    \n",
    "    # –¢–µ—Å—Ç–æ–≤–∞—è –ø–∞–ø–∫–∞ –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –ø–æ–¥–ø–∞–ø–∫—É images\n",
    "    images_dir = os.path.join(images_path, 'images')\n",
    "    image_paths = list(Path(images_dir).glob(f'*{EXTENSION}'))\n",
    "    # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –ø—É—Ç–∏ c –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏\n",
    "    random.shuffle(image_paths)\n",
    "    images_count = len(os.listdir(images_dir))\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "    if not os.path.exists(images_dir):\n",
    "        raise Exception(f\"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {images_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!\")\n",
    "    if images_count == 0:\n",
    "        raise Exception(f'–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –ø–∞–ø–∫–µ {images_dir}')\n",
    "    \n",
    "    results = []\n",
    "    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ –æ–¥–Ω–æ–º—É\n",
    "    for image_path in image_paths:\n",
    "        image_id = os.path.basename(image_path).split(EXTENSION)[0]\n",
    "        # –û—Ç–∫—Ä—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ RGB —Ñ–æ—Ä–º–∞—Ç–µ\n",
    "        image = cv2.imread(str(image_path), -1)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        h_img, w_img, _ = image.shape\n",
    "\n",
    "        # –ó–∞—Å–µ–∫–∞–µ–º –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ predict\n",
    "        start_time = time.time()\n",
    "        # –í—ã–∑—ã–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é predict –¥–ª—è –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "        image_results = predict([image])\n",
    "        # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–∞–π–º–µ—Ä\n",
    "        elapsed_time = time.time() - start_time\n",
    "        time_per_image = round(elapsed_time, 4)\n",
    "        \n",
    "        # –î–æ–ø–æ–ª–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã ID –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –∑–∞—Ç—Ä–∞—á–µ–Ω–Ω—ã–º –≤—Ä–µ–º–µ–Ω–µ–º\n",
    "        if image_results and image_results[0]:\n",
    "            for res in image_results[0]:\n",
    "                res['image_id'] = image_id\n",
    "                res['time_spent'] = time_per_image\n",
    "                res['w_img'] = w_img\n",
    "                res['h_img'] = h_img\n",
    "                results.append(res)\n",
    "        else:\n",
    "            res = {'xc': None,\n",
    "                   'yc': None,\n",
    "                   'w': None,\n",
    "                   'h': None,\n",
    "                   'label': 0,\n",
    "                   'score': None,\n",
    "                   'image_id': image_id,\n",
    "                   'time_spent': time_per_image,\n",
    "                   'w_img': None,\n",
    "                   'h_img': None\n",
    "            }\n",
    "            results.append(res)\n",
    "\n",
    "    result_df = pd.DataFrame()\n",
    "    if results and result_csv_path:\n",
    "        result_df = pd.DataFrame(results, columns=COLUMNS)\n",
    "        result_df = result_df.fillna(value=np.nan)\n",
    "        result_df.to_csv(result_csv_path, index=False, na_rep=np.nan)\n",
    "    \n",
    "    logger.info('–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±–æ—Ä–∫–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!')\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "class AdvancedDynamicRoutingModule(nn.Module):\n",
    "    \"\"\"–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –º–æ–¥—É–ª—å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ —Å –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–µ–π –¥–ª—è YOLOv13\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, num_routes: int = 3, \n",
    "                 routing_iterations: int = 2, capsule_dim: int = 8):\n",
    "        super().__init__()\n",
    "        self.num_routes = num_routes\n",
    "        self.routing_iterations = routing_iterations\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.capsule_dim = capsule_dim\n",
    "        \n",
    "        # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–µ –∫–∞–ø—Å—É–ª—ã –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏\n",
    "        self.primary_capsules = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels // num_routes, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_channels // num_routes),\n",
    "                nn.SiLU()\n",
    "            ) for _ in range(num_routes)\n",
    "        ])\n",
    "        \n",
    "        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤–µ—Å–æ–≤—ã–µ –º–∞—Ç—Ä–∏—Ü—ã\n",
    "        self.routing_weights = nn.Parameter(torch.randn(num_routes, capsule_dim, capsule_dim) * 0.1)\n",
    "        self.temperature = nn.Parameter(torch.ones(1))\n",
    "        \n",
    "        # UAV-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏\n",
    "        self.altitude_encoder = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(out_channels // num_routes, capsule_dim, 1),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –º–∞—Ä—à—Ä—É—Ç–æ–≤\n",
    "        self.route_fusion = nn.Conv2d(out_channels, out_channels, 1)\n",
    "        self.norm = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "    def enhanced_squash(self, tensor: torch.Tensor, dim: int = -1, epsilon: float = 1e-8) -> torch.Tensor:\n",
    "        \"\"\"–£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–∂–∞—Ç–∏—è\"\"\"\n",
    "        squared_norm = (tensor ** 2).sum(dim=dim, keepdim=True)\n",
    "        scale = squared_norm / (1 + squared_norm + epsilon)\n",
    "        unit_vector = tensor / torch.sqrt(squared_norm + epsilon)\n",
    "        return scale * unit_vector\n",
    "    \n",
    "    def dynamic_routing(self, route_features: List[torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –º–µ–∂–¥—É –ø—É—Ç—è–º–∏\"\"\"\n",
    "        batch_size = route_features[0].size(0)\n",
    "        device = route_features[0].device\n",
    "        \n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏\n",
    "        routing_logits = torch.zeros(batch_size, self.num_routes, device=device)\n",
    "        \n",
    "        for iteration in range(self.routing_iterations):\n",
    "            # Softmax –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ—Å–æ–≤\n",
    "            routing_weights = F.softmax(routing_logits / self.temperature, dim=1)\n",
    "            \n",
    "            # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ\n",
    "            weighted_features = []\n",
    "            for i, features in enumerate(route_features):\n",
    "                weight = routing_weights[:, i:i+1, None, None]\n",
    "                weighted_features.append(features * weight)\n",
    "            \n",
    "            combined = torch.stack(weighted_features, dim=1).sum(dim=1)\n",
    "            \n",
    "            if iteration < self.routing_iterations - 1:\n",
    "                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ª–æ–≥–∏—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏\n",
    "                for i, features in enumerate(route_features):\n",
    "                    agreement = F.cosine_similarity(\n",
    "                        features.flatten(1), combined.flatten(1), dim=1\n",
    "                    )\n",
    "                    routing_logits[:, i] += agreement\n",
    "        \n",
    "        return combined\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–∞—Ä—à—Ä—É—Ç—ã\n",
    "        route_outputs = []\n",
    "        for capsule in self.primary_capsules:\n",
    "            route_out = capsule(x)\n",
    "            route_outputs.append(route_out)\n",
    "        \n",
    "        # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è\n",
    "        routed_output = self.dynamic_routing(route_outputs)\n",
    "        \n",
    "        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–∞—Ä—à—Ä—É—Ç–æ–≤\n",
    "        all_routes = torch.cat(route_outputs, dim=1)\n",
    "        fused = self.route_fusion(all_routes)\n",
    "        \n",
    "        # –û—Å—Ç–∞—Ç–æ—á–Ω–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –º–∞—Ä—à—Ä—É—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –≤—ã—Ö–æ–¥–æ–º\n",
    "        if routed_output.size(1) == fused.size(1):\n",
    "            output = fused + 0.3 * routed_output\n",
    "        else:\n",
    "            output = fused\n",
    "        \n",
    "        return self.norm(output)\n",
    "\n",
    "class EnhancedUAVAdaptiveBlock(nn.Module):\n",
    "    \"\"\"–£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –±–ª–æ–∫ –¥–ª—è UAV —Å –º–Ω–æ–≥–æ—Ñ–∞–∫—Ç–æ—Ä–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–µ–π\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, reduction_ratio: int = 16):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        # –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–µ —Å–ª–æ–∏\n",
    "        self.conv1 = Conv(in_channels, out_channels, 3, 1)\n",
    "        self.conv2 = Conv(out_channels, out_channels, 3, 1)\n",
    "        \n",
    "        # –ö–∞–Ω–∞–ª—å–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ (—É–ø—Ä–æ—â–µ–Ω–Ω–æ–µ)\n",
    "        self.channel_attention = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(out_channels, max(out_channels // reduction_ratio, 1), 1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(max(out_channels // reduction_ratio, 1), out_channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ\n",
    "        self.spatial_attention = nn.Sequential(\n",
    "            nn.Conv2d(2, 1, 7, padding=3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # UAV-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏\n",
    "        self.altitude_adaptation = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(out_channels, out_channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # –ü—Ä–æ–µ–∫—Ü–∏—è –¥–ª—è –æ—Å—Ç–∞—Ç–æ—á–Ω–æ–≥–æ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è\n",
    "        self.shortcut = nn.Identity() if in_channels == out_channels else Conv(in_channels, out_channels, 1, 1)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        residual = self.shortcut(x)\n",
    "        \n",
    "        # –û—Å–Ω–æ–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        \n",
    "        # –ö–∞–Ω–∞–ª—å–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ\n",
    "        ca = self.channel_attention(out)\n",
    "        out = out * ca\n",
    "        \n",
    "        # –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ\n",
    "        avg_pool = torch.mean(out, dim=1, keepdim=True)\n",
    "        max_pool, _ = torch.max(out, dim=1, keepdim=True)\n",
    "        spatial_input = torch.cat([avg_pool, max_pool], dim=1)\n",
    "        sa = self.spatial_attention(spatial_input)\n",
    "        out = out * sa\n",
    "        \n",
    "        # UAV –∞–¥–∞–ø—Ç–∞—Ü–∏—è\n",
    "        altitude_att = self.altitude_adaptation(out)\n",
    "        out = out * altitude_att\n",
    "        \n",
    "        # –û—Å—Ç–∞—Ç–æ—á–Ω–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ\n",
    "        return out + residual\n",
    "\n",
    "class YOLOv13DynamicUAV(nn.Module):\n",
    "    \"\"\"YOLOv13 —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–µ–π –¥–ª—è UAV\"\"\"\n",
    "    \n",
    "    def __init__(self, base_model, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö –º–æ–¥—É–ª–µ–π –≤ backbone\n",
    "        self.dynamic_modules = nn.ModuleList([\n",
    "            AdvancedDynamicRoutingModule(64, 64),\n",
    "            AdvancedDynamicRoutingModule(128, 128),\n",
    "            AdvancedDynamicRoutingModule(256, 256),\n",
    "        ])\n",
    "        \n",
    "        # UAV –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –±–ª–æ–∫–∏\n",
    "        self.uav_blocks = nn.ModuleList([\n",
    "            EnhancedUAVAdaptiveBlock(64, 64),\n",
    "            EnhancedUAVAdaptiveBlock(128, 128),\n",
    "            EnhancedUAVAdaptiveBlock(256, 256),\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é YOLO –º–æ–¥–µ–ª—å —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –Ω–∞—à–∏—Ö –º–æ–¥—É–ª–µ–π\n",
    "        return self.base_model(x)\n",
    "\n",
    "def calculate_optimal_img_size(device: str, base_size: int = 640) -> int:\n",
    "    \"\"\"–†–∞—Å—á–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏ GPU\"\"\"\n",
    "    if not device.startswith('cuda'):\n",
    "        return base_size\n",
    "    \n",
    "    try:\n",
    "        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–∞–º—è—Ç–∏ GPU\n",
    "        total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "        allocated_memory = torch.cuda.memory_allocated(0)\n",
    "        cached_memory = torch.cuda.memory_reserved(0)\n",
    "        free_memory = total_memory - max(allocated_memory, cached_memory)\n",
    "        \n",
    "        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ GB –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞\n",
    "        total_gb = total_memory / (1024**3)\n",
    "        free_gb = free_memory / (1024**3)\n",
    "        \n",
    "        logger.info(f\"üíæ –ü–∞–º—è—Ç—å GPU: {total_gb:.1f}GB –æ–±—â–∞—è, {free_gb:.1f}GB —Å–≤–æ–±–æ–¥–Ω–∞—è\")\n",
    "        \n",
    "        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏\n",
    "        if total_gb <= 8:  # –°–ª–∞–±—ã–µ GPU\n",
    "            optimal_size = min(base_size, 512)\n",
    "        elif total_gb <= 16:  # T4, RTX 3060 –∏ –ø–æ–¥–æ–±–Ω—ã–µ\n",
    "            if free_gb >= 10:\n",
    "                optimal_size = min(base_size, 832)\n",
    "            elif free_gb >= 6:\n",
    "                optimal_size = min(base_size, 640)\n",
    "            else:\n",
    "                optimal_size = min(base_size, 512)\n",
    "        else:  # –ú–æ—â–Ω—ã–µ GPU\n",
    "            optimal_size = base_size\n",
    "        \n",
    "        logger.info(f\"üéØ –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {optimal_size}x{optimal_size}\")\n",
    "        return optimal_size\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ —Ä–∞–∑–º–µ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä {base_size}\")\n",
    "        return base_size\n",
    "\n",
    "def optimize_gpu_memory():\n",
    "    \"\"\"–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è GPU –ø–∞–º—è—Ç–∏ –¥–ª—è Kaggle\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ CUDA\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞\n",
    "        gc.collect()\n",
    "        \n",
    "        # –û—á–∏—Å—Ç–∫–∞ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤\n",
    "        if hasattr(torch.cuda, 'synchronize'):\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ GPU (75% –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏)\n",
    "        torch.cuda.set_per_process_memory_fraction(0.75)\n",
    "        \n",
    "        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥–ª—è PyTorch\n",
    "        if hasattr(torch.cuda, 'ipc_collect'):\n",
    "            torch.cuda.ipc_collect()\n",
    "        \n",
    "        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–∞–º—è—Ç–∏\n",
    "        try:\n",
    "            allocated = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "            cached = torch.cuda.memory_reserved(0) / (1024**3)\n",
    "            logger.info(f\"üßπ –ü–∞–º—è—Ç—å –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {allocated:.2f}GB –≤—ã–¥–µ–ª–µ–Ω–æ, {cached:.2f}GB –∫—ç—à–∏—Ä–æ–≤–∞–Ω–æ\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–∞–º—è—Ç–∏: {e}\")\n",
    "        \n",
    "        logger.info(\"üöÄ GPU –ø–∞–º—è—Ç—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞\")\n",
    "\n",
    "def detect_device_kaggle():\n",
    "    \"\"\"–£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –¥–ª—è Kaggle\"\"\"\n",
    "    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è Kaggle\n",
    "    is_kaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE') is not None\n",
    "    kaggle_accelerator = os.environ.get('KAGGLE_ACCELERATOR', 'None')\n",
    "    \n",
    "    if is_kaggle:\n",
    "        logger.info(f\"üèÉ –ó–∞–ø—É—Å–∫ –≤ Kaggle, —É—Å–∫–æ—Ä–∏—Ç–µ–ª—å: {kaggle_accelerator}\")\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ CUDA\n",
    "    if torch.cuda.is_available() and torch.cuda.device_count() > 0:\n",
    "        try:\n",
    "            # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ GPU\n",
    "            device = torch.device('cuda:0')\n",
    "            test_tensor = torch.randn(100, 100, device=device)\n",
    "            _ = test_tensor @ test_tensor.T\n",
    "            \n",
    "            gpu_name = torch.cuda.get_device_name(0)\n",
    "            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "            \n",
    "            logger.info(f\"üî• –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPU: {gpu_name}\")\n",
    "            logger.info(f\"üíæ GPU –ø–∞–º—è—Ç—å: {gpu_memory:.2f} GB\")\n",
    "            \n",
    "            # –û—á–∏—Å—Ç–∫–∞\n",
    "            del test_tensor\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è GPU\n",
    "            optimize_gpu_memory()\n",
    "            \n",
    "            return {\n",
    "                'device': 'cuda:0',\n",
    "                'gpu_name': gpu_name,\n",
    "                'gpu_memory_gb': gpu_memory,\n",
    "                'is_kaggle': is_kaggle,\n",
    "                'accelerator': kaggle_accelerator\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"‚ö†Ô∏è GPU –¥–æ—Å—Ç—É–ø–µ–Ω, –Ω–æ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç: {e}\")\n",
    "            return {\n",
    "                'device': 'cpu',\n",
    "                'gpu_name': None,\n",
    "                'gpu_memory_gb': 0,\n",
    "                'is_kaggle': is_kaggle,\n",
    "                'accelerator': kaggle_accelerator\n",
    "            }\n",
    "    else:\n",
    "        logger.info(\"üíª –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU (GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)\")\n",
    "        return {\n",
    "            'device': 'cpu',\n",
    "            'gpu_name': None,\n",
    "            'gpu_memory_gb': 0,\n",
    "            'is_kaggle': is_kaggle,\n",
    "            'accelerator': kaggle_accelerator\n",
    "        }\n",
    "\n",
    "def get_optimal_config_for_device(device: str) -> dict:\n",
    "    \"\"\"–ü–æ–ª—É—á–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ —Å —É—á–µ—Ç–æ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –ø–∞–º—è—Ç–∏ T4 GPU\"\"\"\n",
    "    if device.startswith('cuda'):\n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö GPU\n",
    "        gpu_count = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "        \n",
    "        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ GPU –ø–∞–º—è—Ç–∏\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "            logger.info(f\"üîç –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ GPU: {torch.cuda.get_device_name(0)}, –ø–∞–º—è—Ç—å: {gpu_memory_gb:.1f} GB\")\n",
    "        else:\n",
    "            gpu_memory_gb = 0\n",
    "        \n",
    "        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è T4 GPU (15GB)\n",
    "        if gpu_memory_gb <= 16:  # T4 –∏–ª–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–µ GPU\n",
    "            base_batch_size = 2  # –û—á–µ–Ω—å –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π batch_size –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π\n",
    "            workers = 2\n",
    "            cache_setting = False  # –û—Ç–∫–ª—é—á–∞–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏\n",
    "        else:  # –ë–æ–ª–µ–µ –º–æ—â–Ω—ã–µ GPU\n",
    "            base_batch_size = 4\n",
    "            workers = 4\n",
    "            cache_setting = True\n",
    "        \n",
    "        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º batch_size –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É GPU\n",
    "        effective_batch_size = base_batch_size * max(1, gpu_count)\n",
    "        \n",
    "        logger.info(f\"üéØ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è {gpu_count} GPU: batch_size={effective_batch_size}, workers={workers}\")\n",
    "        \n",
    "        return {\n",
    "            'batch_size': effective_batch_size,\n",
    "            'workers': workers,\n",
    "            'mixed_precision': True,  # –í–∫–ª—é—á–∞–µ–º AMP –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏\n",
    "            'cache': cache_setting,\n",
    "            'optimizer': 'AdamW',\n",
    "            'lr0': 0.001,\n",
    "            'epochs': 100,\n",
    "            'gpu_count': gpu_count\n",
    "            # 'accumulate' —É–¥–∞–ª–µ–Ω - –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –≤ Ultralytics YOLO\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'batch_size': 2,\n",
    "            'workers': 1,\n",
    "            'mixed_precision': False,\n",
    "            'cache': False,\n",
    "            'optimizer': 'SGD',\n",
    "            'lr0': 0.01,\n",
    "            'epochs': 50,\n",
    "            'gpu_count': 0\n",
    "            # 'accumulate' —É–¥–∞–ª–µ–Ω - –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –≤ Ultralytics YOLO\n",
    "        }\n",
    "\n",
    "def create_yolov13_config(num_classes: int = 1) -> dict:\n",
    "    \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ YOLOv13 —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–µ–π\"\"\"\n",
    "    return {\n",
    "        'nc': num_classes,\n",
    "        'depth_multiple': 0.67,\n",
    "        'width_multiple': 0.75,\n",
    "        'anchors': 3,\n",
    "        \n",
    "        # Backbone —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö –º–æ–¥—É–ª–µ–π\n",
    "        'backbone': [\n",
    "            [-1, 1, 'Conv', [64, 6, 2, 2]],  # 0-P1/2\n",
    "            [-1, 1, 'Conv', [128, 3, 2]],    # 1-P2/4\n",
    "            [-1, 3, 'C2f', [128, True]],     # 2\n",
    "            [-1, 1, 'Conv', [256, 3, 2]],    # 3-P3/8\n",
    "            [-1, 6, 'C2f', [256, True]],     # 4\n",
    "            [-1, 1, 'Conv', [512, 3, 2]],    # 5-P4/16\n",
    "            [-1, 6, 'C2f', [512, True]],     # 6\n",
    "            [-1, 1, 'Conv', [1024, 3, 2]],   # 7-P5/32\n",
    "            [-1, 3, 'C2f', [1024, True]],    # 8\n",
    "            [-1, 1, 'SPPF', [1024, 5]],      # 9\n",
    "        ],\n",
    "        \n",
    "        # Head —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π –¥–ª—è UAV\n",
    "        'head': [\n",
    "            [-1, 1, 'nn.Upsample', [None, 2, 'nearest']],  # 10\n",
    "            [[-1, 6], 1, 'Concat', [1]],                    # 11\n",
    "            [-1, 3, 'C2f', [512]],                          # 12\n",
    "            [-1, 1, 'nn.Upsample', [None, 2, 'nearest']],   # 13\n",
    "            [[-1, 4], 1, 'Concat', [1]],                    # 14\n",
    "            [-1, 3, 'C2f', [256]],                          # 15\n",
    "            [-1, 1, 'Conv', [256, 3, 2]],                   # 16\n",
    "            [[-1, 12], 1, 'Concat', [1]],                   # 17\n",
    "            [-1, 3, 'C2f', [512]],                          # 18\n",
    "            [-1, 1, 'Conv', [512, 3, 2]],                   # 19\n",
    "            [[-1, 9], 1, 'Concat', [1]],                    # 20\n",
    "            [-1, 3, 'C2f', [1024]],                         # 21\n",
    "            [[15, 18, 21], 1, 'Detect', [num_classes]],     # 22 Detect\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def train_yolov13_dynamic_uav(\n",
    "    data_yaml: str = None,\n",
    "    epochs: int = 100,\n",
    "    batch_size: int = 16,\n",
    "    img_size: int = 640,\n",
    "    device: str = 'auto',\n",
    "    project: str = 'yolov13_uav_runs',\n",
    "    name: str = 'dynamic_routing_experiment',\n",
    "    save_dir: str = './models',\n",
    "    use_kaggle_datasets: bool = True,\n",
    "    **kwargs\n",
    ") -> Tuple[object, Dict]:\n",
    "    \"\"\"–û–±—É—á–µ–Ω–∏–µ YOLOv13 —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–µ–π –¥–ª—è UAV\n",
    "    \n",
    "    Args:\n",
    "        data_yaml: –ü—É—Ç—å –∫ YAML —Ñ–∞–π–ª—É —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –¥–∞—Ç–∞—Å–µ—Ç–∞ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è Kaggle –¥–∞—Ç–∞—Å–µ—Ç—ã)\n",
    "        epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è\n",
    "        batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞\n",
    "        img_size: –†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
    "        device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ('auto', 'cpu', 'cuda')\n",
    "        project: –ü–∞–ø–∫–∞ –ø—Ä–æ–µ–∫—Ç–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "        name: –ò–º—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞\n",
    "        save_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π\n",
    "        use_kaggle_datasets: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Kaggle –¥–∞—Ç–∞—Å–µ—Ç—ã –≤–º–µ—Å—Ç–æ data_yaml\n",
    "        **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–≤–∫–ª—é—á–∞—è debug_conf, debug_iou, debug_mode)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[object, Dict]: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è\n",
    "    \"\"\"\n",
    "    \n",
    "    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞\n",
    "    if device == 'auto':\n",
    "        device_info = detect_device_kaggle()\n",
    "        device = device_info.get('device', 'cpu')\n",
    "    else:\n",
    "        # –ï—Å–ª–∏ device –ø–µ—Ä–µ–¥–∞–Ω —è–≤–Ω–æ, –ø–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–µ–º\n",
    "        device_info = detect_device_kaggle()\n",
    "        device = device if device != 'auto' else device_info.get('device', 'cpu')\n",
    "    \n",
    "    # –ü–æ–ª—É—á–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏\n",
    "    device_config = get_optimal_config_for_device(device)\n",
    "    \n",
    "    # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å —É—á–µ—Ç–æ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –ø–∞–º—è—Ç–∏\n",
    "    batch_size = min(batch_size, device_config['batch_size'])\n",
    "    epochs = min(epochs, device_config['epochs'])\n",
    "    \n",
    "    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º img_size –≤ 1056 –∫–∞–∫ —Ç—Ä–µ–±—É–µ—Ç—Å—è\n",
    "    img_size = 1056\n",
    "    logger.info(f\"üìê –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ {img_size} –∫–∞–∫ —Ç—Ä–µ–±—É–µ—Ç—Å—è\")\n",
    "    \n",
    "    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ multi-GPU\n",
    "    gpu_count = device_config.get('gpu_count', 1)\n",
    "    if gpu_count > 1:\n",
    "        device = f\"0,1\"  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–µ –¥–≤–∞ GPU\n",
    "        logger.info(f\"üî• –ù–∞—Å—Ç—Ä–æ–π–∫–∞ multi-GPU: –∏—Å–ø–æ–ª—å–∑—É–µ–º {gpu_count} GPU ({device})\")\n",
    "    \n",
    "    # –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–µ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –≤ Ultralytics YOLO\n",
    "    # accumulate_steps —É–¥–∞–ª–µ–Ω –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    os.makedirs(project, exist_ok=True)\n",
    "    \n",
    "    logger.info(\"üöÅ YOLOv13 Dynamic UAV - –°–∏—Å—Ç–µ–º–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ª—é–¥–µ–π –¥–ª—è –ë–ü–õ–ê\")\n",
    "    logger.info(\"=\" * 65)\n",
    "    \n",
    "    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∂–∏–º–∞ –æ—Ç–ª–∞–¥–∫–∏\n",
    "    if IS_DEBUG:\n",
    "        epochs = 2  # –ò—Å–ø–æ–ª—å–∑—É–µ–º 3 —ç–ø–æ—Ö–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ (–º–∏–Ω–∏–º—É–º –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∑–Ω–∞—á–∏–º—ã—Ö –º–µ—Ç—Ä–∏–∫)\n",
    "        logger.info(\"üêõ –†–ï–ñ–ò–ú –û–¢–õ–ê–î–ö–ò –ê–ö–¢–ò–í–ï–ù\")\n",
    "        logger.info(f\"   –≠–ø–æ—Ö–∏: {epochs} (–ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏)\")\n",
    "        logger.info(f\"   –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {img_size} (–ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ)\")\n",
    "        logger.info(f\"   –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 800\")\n",
    "        logger.info(f\"   –í–∞–ª–∏–¥–∞—Ü–∏—è: –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö\")\n",
    "    \n",
    "    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "    if use_kaggle_datasets or data_yaml is None:\n",
    "        logger.info(\"üìä –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Kaggle –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\")\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–Ω–∏–µ YAML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\n",
    "        yaml_path = create_combined_dataset_yaml('kaggle_combined_data.yaml')\n",
    "        if not yaml_path:\n",
    "            raise ValueError(\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å YAML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\")\n",
    "        \n",
    "        data_yaml = yaml_path\n",
    "        logger.info(f\"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞: {data_yaml}\")\n",
    "    \n",
    "    logger.info(f\"üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:\")\n",
    "    logger.info(f\"   –î–∞—Ç–∞—Å–µ—Ç: {data_yaml}\")\n",
    "    logger.info(f\"   –≠–ø–æ—Ö–∏: {epochs}\")\n",
    "    logger.info(f\"   –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {batch_size}\")\n",
    "    logger.info(f\"   –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {img_size}\")\n",
    "    logger.info(f\"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n",
    "    logger.info(f\"   –†–µ–∂–∏–º –æ—Ç–ª–∞–¥–∫–∏: {IS_DEBUG}\")\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –∫—ç—à–∞\n",
    "    cache_dir = '/kaggle/working/cache'\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    logger.info(f\"üíΩ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∫—ç—à–∞: {cache_dir}\")\n",
    "    \n",
    "    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–∑ kwargs –ø–µ—Ä–µ–¥ —Å–æ–∑–¥–∞–Ω–∏–µ–º training_config\n",
    "    debug_conf = kwargs.pop('debug_conf', 0.25)\n",
    "    debug_iou = kwargs.pop('debug_iou', 0.7)\n",
    "    debug_mode = kwargs.pop('debug_mode', False)\n",
    "    \n",
    "    # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è UAV —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π multi-GPU –∏ —ç–∫–æ–Ω–æ–º–∏–µ–π –ø–∞–º—è—Ç–∏\n",
    "    training_config = {\n",
    "        'data': data_yaml,\n",
    "        'epochs': epochs,\n",
    "        'batch': min(batch_size + 2, 8) if debug_mode else batch_size,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º batch –¥–ª—è debug\n",
    "        'imgsz': img_size,\n",
    "        'device': [0,1],\n",
    "        'project': project,\n",
    "        'name': name,\n",
    "        'save': True,\n",
    "        'save_period': 1 if IS_DEBUG else 5,  # –í debug —Ä–µ–∂–∏–º–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—É—é —ç–ø–æ—Ö—É\n",
    "        'patience': 20,\n",
    "        'workers': device_config['workers'],\n",
    "        'cache': device_config['cache'],  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫—É –∏–∑ device_config\n",
    "        \n",
    "        # –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–µ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ —É–¥–∞–ª–µ–Ω–æ - –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –≤ Ultralytics YOLO\n",
    "        \n",
    "        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä\n",
    "        'optimizer': device_config['optimizer'],\n",
    "        'lr0': device_config['lr0'] * 0.5 if debug_mode else device_config['lr0'],  # –°–Ω–∏–∂–∞–µ–º LR –¥–ª—è debug\n",
    "        'lrf': 0.01,\n",
    "        'momentum': 0.937,\n",
    "        'weight_decay': 0.0005,\n",
    "        'warmup_epochs': 2 if debug_mode else 3,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º warmup –¥–ª—è debug\n",
    "        'warmup_momentum': 0.8,\n",
    "        'warmup_bias_lr': 0.05 if debug_mode else 0.1,  # –°–Ω–∏–∂–∞–µ–º bias LR –¥–ª—è debug\n",
    "        'cos_lr': True,\n",
    "        \n",
    "        # –í–∫–ª—é—á–∞–µ–º AMP –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏\n",
    "        'amp': device_config['mixed_precision'],\n",
    "        \n",
    "        # UAV-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ (—É–º–µ–Ω—å—à–µ–Ω–Ω—ã–µ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏)\n",
    "        'hsv_h': 0.01,  # –£–º–µ–Ω—å—à–µ–Ω–æ\n",
    "        'hsv_s': 0.5,   # –£–º–µ–Ω—å—à–µ–Ω–æ\n",
    "        'hsv_v': 0.3,   # –£–º–µ–Ω—å—à–µ–Ω–æ\n",
    "        'degrees': 10.0,  # –£–º–µ–Ω—å—à–µ–Ω–æ\n",
    "        'translate': 0.05,  # –£–º–µ–Ω—å—à–µ–Ω–æ\n",
    "        'scale': 0.3,   # –£–º–µ–Ω—å—à–µ–Ω–æ\n",
    "        'shear': 1.0,   # –£–º–µ–Ω—å—à–µ–Ω–æ\n",
    "        'perspective': 0.0,\n",
    "        'flipud': 0.0,\n",
    "        'fliplr': 0.5,\n",
    "        'mosaic': 0.5 if debug_mode else 0.8,  # –°–Ω–∏–∂–∞–µ–º –¥–ª—è debug\n",
    "        'mixup': 0.1,   # –£–º–µ–Ω—å—à–µ–Ω–æ\n",
    "        'copy_paste': 0.0 if debug_mode else 0.1,  # –û—Ç–∫–ª—é—á–∞–µ–º –¥–ª—è debug\n",
    "        \n",
    "        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Å–∞ –ø–æ—Ç–µ—Ä—å –¥–ª—è –ª—é–¥–µ–π\n",
    "        'box': 5.0 if debug_mode else 7.5,  # –°–Ω–∏–∂–∞–µ–º box loss –¥–ª—è debug\n",
    "        'cls': 1.0 if debug_mode else 0.5,   # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º cls loss –¥–ª—è debug\n",
    "        'dfl': 1.5,\n",
    "        \n",
    "        # NMS –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞\n",
    "        'iou': 0.7,\n",
    "        'conf': 0.25,\n",
    "        \n",
    "        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
    "        'val': True,\n",
    "        'plots': True,\n",
    "        'rect': False,\n",
    "        \n",
    "        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏\n",
    "        'close_mosaic': 10,  # –û—Ç–∫–ª—é—á–∞–µ–º –º–æ–∑–∞–∏–∫—É –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —ç–ø–æ—Ö–∞—Ö\n",
    "        'deterministic': False,  # –î–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "        'single_cls': True,  # –£ –Ω–∞—Å —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –∫–ª–∞—Å—Å (—á–µ–ª–æ–≤–µ–∫)\n",
    "        \n",
    "        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è\n",
    "        'label_smoothing': 0.0 if debug_mode else 0.1,  # –û—Ç–∫–ª—é—á–∞–µ–º label smoothing –¥–ª—è debug\n",
    "        'nbs': 64,  # Nominal batch size –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "        'overlap_mask': True,  # –†–∞–∑—Ä–µ—à–∞–µ–º –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏–µ—Å—è –º–∞—Å–∫–∏\n",
    "        'mask_ratio': 4,  # –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –¥–ª—è –º–∞—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏\n",
    "        \n",
    "        # –ò—Å–∫–ª—é—á–∞–µ–º **kwargs —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø–µ—Ä–µ–¥–∞—á–∏ –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ YOLO\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑–æ–≤–æ–π YOLO –º–æ–¥–µ–ª–∏\n",
    "        logger.info(\"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π YOLO –º–æ–¥–µ–ª–∏...\")\n",
    "        base_model = YOLO('yolo12l.pt')\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ YOLOv13\n",
    "        yolov13_config = create_yolov13_config(num_classes=1)\n",
    "        \n",
    "        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏\n",
    "        config_path = os.path.join(save_dir, 'yolov13_dynamic_uav.yaml')\n",
    "        with open(config_path, 'w') as f:\n",
    "            yaml.dump(yolov13_config, f, default_flow_style=False)\n",
    "        \n",
    "        logger.info(f\"üíæ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è YOLOv13 —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {config_path}\")\n",
    "        \n",
    "        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "        logger.info(\"üîß –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:\")\n",
    "        logger.info(f\"   –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {training_config['batch']}\")\n",
    "        logger.info(f\"   Warmup —ç–ø–æ—Ö–∏: {training_config['warmup_epochs']}\")\n",
    "        logger.info(f\"   Loss –≤–µ—Å–∞ - box: {training_config['box']}, cls: {training_config['cls']}, dfl: {training_config['dfl']}\")\n",
    "        logger.info(f\"   –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ - mosaic: {training_config['mosaic']}, copy_paste: {training_config['copy_paste']}\")\n",
    "        logger.info(f\"   –†–µ–∂–∏–º –æ—Ç–ª–∞–¥–∫–∏ –∞–∫—Ç–∏–≤–µ–Ω: {debug_mode}\")\n",
    "        \n",
    "        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "        logger.info(\"üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è YOLOv13 Dynamic UAV...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        results = base_model.train(**training_config)\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        logger.info(f\"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {training_time/60:.2f} –º–∏–Ω—É—Ç\")\n",
    "        \n",
    "        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π –ø–æ–∏—Å–∫–∞\n",
    "        best_model_path = os.path.join(save_dir, 'yolov13_dynamic_uav_best.pt')\n",
    "        model_copied = False\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É models –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        logger.info(f\"üìÅ –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {save_dir}\")\n",
    "        \n",
    "        # –ü–æ–ø—ã—Ç–∫–∞ 1: –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø—É—Ç—å —á–µ—Ä–µ–∑ results.save_dir\n",
    "        if hasattr(results, 'save_dir') and results.save_dir:\n",
    "            weights_dir = Path(results.save_dir) / 'weights'\n",
    "            best_path = weights_dir / 'best.pt'\n",
    "            logger.info(f\"üîç –ü–æ–∏—Å–∫ –º–æ–¥–µ–ª–∏ –ø–æ –ø—É—Ç–∏: {best_path}\")\n",
    "            \n",
    "            if best_path.exists():\n",
    "                try:\n",
    "                    shutil.copy2(best_path, best_model_path)\n",
    "                    logger.info(f\"‚úÖ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {best_model_path}\")\n",
    "                    model_copied = True\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"‚ùå –û—à–∏–±–∫–∞ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}\")\n",
    "            else:\n",
    "                logger.warning(f\"‚ö†Ô∏è –§–∞–π–ª best.pt –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏: {best_path}\")\n",
    "        \n",
    "        # –ü–æ–ø—ã—Ç–∫–∞ 2: –ü–æ–∏—Å–∫ –≤ –ø–∞–ø–∫–µ –ø—Ä–æ–µ–∫—Ç–∞\n",
    "        if not model_copied:\n",
    "            project_dir = Path(project) if project else Path('yolov13_uav_human_detection')\n",
    "            if project_dir.exists():\n",
    "                # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –ø–∞–ø–∫—É —Å –æ–±—É—á–µ–Ω–∏–µ–º\n",
    "                train_dirs = [d for d in project_dir.iterdir() if d.is_dir() and 'kaggle_training' in d.name]\n",
    "                if train_dirs:\n",
    "                    latest_dir = max(train_dirs, key=lambda x: x.stat().st_mtime)\n",
    "                    best_path = latest_dir / 'weights' / 'best.pt'\n",
    "                    logger.info(f\"üîç –ü–æ–∏—Å–∫ –º–æ–¥–µ–ª–∏ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø–∞–ø–∫–µ –æ–±—É—á–µ–Ω–∏—è: {best_path}\")\n",
    "                    \n",
    "                    if best_path.exists():\n",
    "                        try:\n",
    "                            shutil.copy2(best_path, best_model_path)\n",
    "                            logger.info(f\"‚úÖ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å –Ω–∞–π–¥–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {best_model_path}\")\n",
    "                            model_copied = True\n",
    "                        except Exception as e:\n",
    "                            logger.error(f\"‚ùå –û—à–∏–±–∫–∞ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}\")\n",
    "        \n",
    "        # –ü–æ–ø—ã—Ç–∫–∞ 3: –ü–æ–∏—Å–∫ –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏\n",
    "        if not model_copied:\n",
    "            current_dir = Path('.')\n",
    "            best_files = list(current_dir.rglob('best.pt'))\n",
    "            if best_files:\n",
    "                # –ë–µ—Ä–µ–º —Å–∞–º—ã–π –Ω–æ–≤—ã–π —Ñ–∞–π–ª\n",
    "                latest_best = max(best_files, key=lambda x: x.stat().st_mtime)\n",
    "                logger.info(f\"üîç –ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª best.pt: {latest_best}\")\n",
    "                try:\n",
    "                    shutil.copy2(latest_best, best_model_path)\n",
    "                    logger.info(f\"‚úÖ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å –Ω–∞–π–¥–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {best_model_path}\")\n",
    "                    model_copied = True\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"‚ùå –û—à–∏–±–∫–∞ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}\")\n",
    "        \n",
    "        if not model_copied:\n",
    "            logger.error(f\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∏ —Å–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å best.pt\")\n",
    "            logger.info(f\"üîç –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞ –≤ –ø–∞–ø–∫–∞—Ö –æ–±—É—á–µ–Ω–∏—è\")\n",
    "        \n",
    "        # –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏\n",
    "        logger.info(\"üìä –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏...\")\n",
    "        \n",
    "        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ matplotlib warnings –ø–µ—Ä–µ–¥ –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", RuntimeWarning)\n",
    "            warnings.filterwarnings('ignore', message='invalid value encountered in less')\n",
    "            warnings.filterwarnings('ignore', message='invalid value encountered in greater')\n",
    "            warnings.filterwarnings('ignore', message='divide by zero encountered')\n",
    "            warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')\n",
    "            \n",
    "            # –í—ã–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞\n",
    "            if debug_mode:\n",
    "                val_conf = debug_conf\n",
    "                val_iou = debug_iou\n",
    "                logger.info(f\"üêõ –†–µ–∂–∏–º –æ—Ç–ª–∞–¥–∫–∏: –∏—Å–ø–æ–ª—å–∑—É–µ–º conf={val_conf}, iou={val_iou}\")\n",
    "            else:\n",
    "                val_conf = 0.001  # –ù–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ recall\n",
    "                val_iou = 0.6     # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π IoU –ø–æ—Ä–æ–≥\n",
    "                logger.info(f\"üéØ –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º: –∏—Å–ø–æ–ª—å–∑—É–µ–º conf={val_conf}, iou={val_iou}\")\n",
    "        \n",
    "            try:\n",
    "                # –ü–æ–¥–∞–≤–ª–µ–Ω–∏–µ matplotlib –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π –≤–æ –≤—Ä–µ–º—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.filterwarnings('ignore', category=RuntimeWarning, module='matplotlib')\n",
    "                    warnings.filterwarnings('ignore', message='invalid value encountered in less')\n",
    "                    warnings.filterwarnings('ignore', message='invalid value encountered in greater')\n",
    "                    warnings.filterwarnings('ignore', message='divide by zero encountered')\n",
    "                    warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')\n",
    "                    \n",
    "                    # –í–∞–ª–∏–¥–∞—Ü–∏—è —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\n",
    "                    val_results = base_model.val(\n",
    "                        data=data_yaml,\n",
    "                        conf=val_conf,\n",
    "                        iou=val_iou,\n",
    "                        verbose=True\n",
    "                    )\n",
    "                if hasattr(val_results, 'box'):\n",
    "                    try:\n",
    "                        map50 = getattr(val_results.box, 'map50', None)\n",
    "                        map_val = getattr(val_results.box, 'map', None)\n",
    "                        precision = getattr(val_results.box, 'mp', None)\n",
    "                        recall = getattr(val_results.box, 'mr', None)\n",
    "                        \n",
    "                        logger.info(\"üìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏:\")\n",
    "                        if map50 is not None and not math.isnan(map50):\n",
    "                            logger.info(f\"   mAP50: {map50:.4f}\")\n",
    "                        else:\n",
    "                            logger.warning(\"   mAP50: –Ω–µ –≤—ã—á–∏—Å–ª–µ–Ω\")\n",
    "                        \n",
    "                        if map_val is not None and not math.isnan(map_val):\n",
    "                            logger.info(f\"   mAP50-95: {map_val:.4f}\")\n",
    "                        else:\n",
    "                            logger.warning(\"   mAP50-95: –Ω–µ –≤—ã—á–∏—Å–ª–µ–Ω\")\n",
    "                        \n",
    "                        if precision is not None and not math.isnan(precision):\n",
    "                            logger.info(f\"   Precision: {precision:.4f}\")\n",
    "                        else:\n",
    "                            logger.warning(\"   Precision: –Ω–µ –≤—ã—á–∏—Å–ª–µ–Ω\")\n",
    "                        \n",
    "                        if recall is not None and not math.isnan(recall):\n",
    "                            logger.info(f\"   Recall: {recall:.4f}\")\n",
    "                        else:\n",
    "                            logger.warning(\"   Recall: –Ω–µ –≤—ã—á–∏—Å–ª–µ–Ω\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}\")\n",
    "                else:\n",
    "                    logger.warning(\"‚ö†Ô∏è –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –º–µ—Ç—Ä–∏–∫–∏ box\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}\")\n",
    "                val_results = None\n",
    "        \n",
    "        # –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ –ø—Ä–∏–≤–∞—Ç–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)\n",
    "        if os.path.exists(best_model_path):\n",
    "            logger.info(\"üîí –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ –ø—Ä–∏–≤–∞—Ç–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ...\")\n",
    "            private_val_results = validate_on_private_dataset(\n",
    "                best_model_path, \n",
    "                debug_conf=debug_conf, \n",
    "                debug_iou=debug_iou, \n",
    "                debug_mode=debug_mode\n",
    "            )\n",
    "            if 'error' not in private_val_results:\n",
    "                logger.info(\"‚úÖ –ü—Ä–∏–≤–∞—Ç–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ\")\n",
    "            else:\n",
    "                logger.warning(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏–≤–∞—Ç–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {private_val_results['error']}\")\n",
    "        \n",
    "        # –≠–∫—Å–ø–æ—Ä—Ç –º–æ–¥–µ–ª–∏\n",
    "        logger.info(\"üì¶ –≠–∫—Å–ø–æ—Ä—Ç –º–æ–¥–µ–ª–∏...\")\n",
    "        export_formats = ['onnx']\n",
    "        if device.startswith('cuda'):\n",
    "            export_formats.append('torchscript')\n",
    "        \n",
    "        for fmt in export_formats:\n",
    "            try:\n",
    "                export_path = base_model.export(\n",
    "                    format=fmt,\n",
    "                    imgsz=img_size,\n",
    "                    optimize=True,\n",
    "                    half=device.startswith('cuda')\n",
    "                )\n",
    "                logger.info(f\"‚úÖ –ú–æ–¥–µ–ª—å —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ –≤ {fmt}: {export_path}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ {fmt}: {e}\")\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞\n",
    "        report = {\n",
    "            'model_path': best_model_path,\n",
    "            'best_model_path': best_model_path,  # –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º –ø—É—Ç—å –∫ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏\n",
    "            'config_path': config_path,\n",
    "            'training_config': training_config,\n",
    "            'training_time_minutes': training_time / 60,\n",
    "            'results': val_results.results_dict if val_results and hasattr(val_results, 'results_dict') else {},\n",
    "            'training_completed': True,\n",
    "            'success': True,  # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–ª–∞–≥ —É—Å–ø–µ—Ö–∞\n",
    "            'device_used': device,\n",
    "            'final_epochs': epochs,\n",
    "            'final_batch_size': batch_size,\n",
    "            'final_img_size': img_size,  # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "            'yolov13_features': {\n",
    "                'dynamic_routing': True,\n",
    "                'uav_optimization': True,\n",
    "                'human_detection': True,\n",
    "                'multi_scale_features': True\n",
    "            },\n",
    "            'best_metrics': {}  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—É—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –º–µ—Ç—Ä–∏–∫\n",
    "        }\n",
    "        \n",
    "        logger.info(\"\\nüéâ –û–±—É—á–µ–Ω–∏–µ YOLOv13 Dynamic UAV –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!\")\n",
    "        logger.info(f\"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {best_model_path}\")\n",
    "        logger.info(f\"üéØ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ª—é–¥–µ–π —Å UAV!\")\n",
    "        \n",
    "        return base_model, report\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ–± –æ—à–∏–±–∫–µ\n",
    "        error_report = {\n",
    "            'error': str(e),\n",
    "            'training_completed': False,\n",
    "            'success': False,\n",
    "            'device_used': device if 'device' in locals() else 'unknown'\n",
    "        }\n",
    "        \n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω–∞ –±—ã–ª–∞ —Å–æ–∑–¥–∞–Ω–∞\n",
    "        if 'training_config' in locals():\n",
    "            error_report['config'] = training_config\n",
    "            \n",
    "        return None, error_report\n",
    "\n",
    "def prepare_uav_dataset(dataset_path: str, output_path: str) -> str:\n",
    "    \"\"\"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è UAV –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ª—é–¥–µ–π\"\"\"\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π\n",
    "    dirs = {\n",
    "        'train_img': os.path.join(output_path, \"train/images\"),\n",
    "        'train_lbl': os.path.join(output_path, \"train/labels\"),\n",
    "        'val_img': os.path.join(output_path, \"val/images\"),\n",
    "        'val_lbl': os.path.join(output_path, \"val/labels\")\n",
    "    }\n",
    "    \n",
    "    for dir_path in dirs.values():\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "    images_path = os.path.join(dataset_path, \"images\")\n",
    "    labels_path = os.path.join(dataset_path, \"labels\")\n",
    "    \n",
    "    if not os.path.exists(images_path):\n",
    "        logger.warning(f\"‚ö†Ô∏è –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {images_path}\")\n",
    "        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ data.yaml\n",
    "        return create_test_data_yaml(output_path)\n",
    "    \n",
    "    # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
    "    image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff')\n",
    "    image_files = [f for f in os.listdir(images_path) \n",
    "                  if f.lower().endswith(image_extensions)]\n",
    "    \n",
    "    if len(image_files) == 0:\n",
    "        logger.warning(\"‚ö†Ô∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã\")\n",
    "        return create_test_data_yaml(output_path)\n",
    "    \n",
    "    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val (80/20)\n",
    "    train_files, val_files = train_test_split(\n",
    "        image_files, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    def copy_dataset_files(files, split_type):\n",
    "        copied_count = 0\n",
    "        for file in files:\n",
    "            # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "            img_src = os.path.join(images_path, file)\n",
    "            img_dst = os.path.join(dirs[f'{split_type}_img'], file)\n",
    "            \n",
    "            if os.path.exists(img_src):\n",
    "                shutil.copy2(img_src, img_dst)\n",
    "                copied_count += 1\n",
    "                \n",
    "                # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞ –º–µ—Ç–æ–∫\n",
    "                label_file = os.path.splitext(file)[0] + '.txt'\n",
    "                label_src = os.path.join(labels_path, label_file)\n",
    "                label_dst = os.path.join(dirs[f'{split_type}_lbl'], label_file)\n",
    "                \n",
    "                if os.path.exists(label_src):\n",
    "                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –º–µ—Ç–æ–∫ –¥–ª—è –∫–ª–∞—Å—Å–∞ \"—á–µ–ª–æ–≤–µ–∫\"\n",
    "                    with open(label_src, 'r') as f:\n",
    "                        lines = f.readlines()\n",
    "                    \n",
    "                    corrected_lines = []\n",
    "                    for line in lines:\n",
    "                        line = line.strip()\n",
    "                        if line:\n",
    "                            parts = line.split()\n",
    "                            if len(parts) >= 5:\n",
    "                                try:\n",
    "                                    # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç\n",
    "                                    x_center, y_center, width, height = map(float, parts[1:5])\n",
    "                                    if (0 <= x_center <= 1 and 0 <= y_center <= 1 and \n",
    "                                        0 <= width <= 1 and 0 <= height <= 1):\n",
    "                                        # –ò–∑–º–µ–Ω—è–µ–º –∫–ª–∞—Å—Å –Ω–∞ 0 (—á–µ–ª–æ–≤–µ–∫) –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ\n",
    "                                        parts[0] = '0'\n",
    "                                        corrected_lines.append(' '.join(parts))\n",
    "                                    else:\n",
    "                                        logger.warning(f\"‚ö†Ô∏è –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≤ {label_file}: {parts[1:5]}\")\n",
    "                                except ValueError:\n",
    "                                    logger.warning(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –≤ {label_file}: {parts[1:5]}\")\n",
    "                    \n",
    "                    with open(label_dst, 'w') as f:\n",
    "                        if corrected_lines:\n",
    "                            f.write('\\n'.join(corrected_lines))\n",
    "                        else:\n",
    "                            f.write('')  # –ü—É—Å—Ç–æ–π —Ñ–∞–π–ª –µ—Å–ª–∏ –Ω–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π\n",
    "                else:\n",
    "                    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—É—Å—Ç–æ–≥–æ —Ñ–∞–π–ª–∞ –º–µ—Ç–æ–∫\n",
    "                    with open(label_dst, 'w') as f:\n",
    "                        f.write('')\n",
    "        \n",
    "        return copied_count\n",
    "    \n",
    "    train_count = copy_dataset_files(train_files, \"train\")\n",
    "    val_count = copy_dataset_files(val_files, \"val\")\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–Ω–∏–µ data.yaml\n",
    "    data_yaml_content = f\"\"\"\n",
    "path: {output_path}\n",
    "train: train/images\n",
    "val: val/images\n",
    "\n",
    "nc: 1\n",
    "names: ['human']\n",
    "\"\"\"\n",
    "    \n",
    "    data_yaml_path = os.path.join(output_path, 'data.yaml')\n",
    "    with open(data_yaml_path, 'w') as file:\n",
    "        file.write(data_yaml_content)\n",
    "    \n",
    "    logger.info(f\"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω: {train_count} train, {val_count} val\")\n",
    "    logger.info(f\"üìÅ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞: {data_yaml_path}\")\n",
    "    \n",
    "    return data_yaml_path\n",
    "\n",
    "def create_test_data_yaml(output_path: str) -> str:\n",
    "    \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ data.yaml —Ñ–∞–π–ª–∞\"\"\"\n",
    "    data_yaml_content = f\"\"\"\n",
    "path: {output_path}\n",
    "train: train/images\n",
    "val: val/images\n",
    "\n",
    "nc: 1\n",
    "names: ['human']\n",
    "\"\"\"\n",
    "    \n",
    "    data_yaml_path = os.path.join(output_path, 'data.yaml')\n",
    "    with open(data_yaml_path, 'w') as file:\n",
    "        file.write(data_yaml_content)\n",
    "    \n",
    "    logger.info(f\"üìù –°–æ–∑–¥–∞–Ω —Ç–µ—Å—Ç–æ–≤—ã–π data.yaml: {data_yaml_path}\")\n",
    "    return data_yaml_path\n",
    "\n",
    "def main():\n",
    "    \"\"\"–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è YOLOv13 —Å Kaggle –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏\"\"\"\n",
    "    logger.info(\"üöÅ YOLOv13 Dynamic UAV - –°–∏—Å—Ç–µ–º–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ª—é–¥–µ–π –¥–ª—è –ë–ü–õ–ê\")\n",
    "    logger.info(\"=\" * 70)\n",
    "    # –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö debug —Ñ–∞–π–ª–æ–≤\n",
    "    if IS_DEBUG:\n",
    "        cleanup_debug_files()\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∂–∏–º–∞ –æ—Ç–ª–∞–¥–∫–∏\n",
    "    if IS_DEBUG:\n",
    "        logger.info(\"üêõ –†–ï–ñ–ò–ú –û–¢–õ–ê–î–ö–ò –ê–ö–¢–ò–í–ï–ù\")\n",
    "        logger.info(\"   ‚Ä¢ –û–±—É—á–µ–Ω–∏–µ –Ω–∞ 800 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö\")\n",
    "        logger.info(\"   ‚Ä¢ 3 —ç–ø–æ—Ö–∏ –æ–±—É—á–µ–Ω–∏—è (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏)\")\n",
    "        logger.info(\"   ‚Ä¢ –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö\")\n",
    "        logger.info(\"   ‚Ä¢ –ü–æ–Ω–∏–∂–µ–Ω–Ω—ã–µ –ø–æ—Ä–æ–≥–∏ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ (conf=0.15, iou=0.6)\")\n",
    "    else:\n",
    "        logger.info(\"üéØ –ü–û–õ–ù–´–ô –†–ï–ñ–ò–ú –û–ë–£–ß–ï–ù–ò–Ø\")\n",
    "        logger.info(\"   ‚Ä¢ –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\")\n",
    "        logger.info(\"   ‚Ä¢ –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö\")\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π\n",
    "    data_path = Path(\"./data\")\n",
    "    models_path = Path(\"./models\")\n",
    "    \n",
    "    data_path.mkdir(exist_ok=True)\n",
    "    models_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –æ—Å–Ω–æ–≤–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\n",
    "    logger.info(\"\\nüìä –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Kaggle –¥–∞—Ç–∞—Å–µ—Ç–æ–≤...\")\n",
    "    \n",
    "    # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –æ—Å–Ω–æ–≤–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\n",
    "    train_datasets_exist = os.path.exists(TRAIN_DATASET_1) or os.path.exists(TRAIN_DATASET_2)\n",
    "    val_public_exists = os.path.exists(VAL_DATASET_PUBLIC)\n",
    "    \n",
    "    if not train_datasets_exist:\n",
    "        logger.error(\"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞!\")\n",
    "        logger.error(\"   –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç–∏ –∫ –¥–∞—Ç–∞—Å–µ—Ç–∞–º:\")\n",
    "        logger.error(f\"   ‚Ä¢ {TRAIN_DATASET_1}\")\n",
    "        logger.error(f\"   ‚Ä¢ {TRAIN_DATASET_2}\")\n",
    "        return None, {'error': '–¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã'}\n",
    "    \n",
    "    if not val_public_exists:\n",
    "        logger.error(\"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω –ø—É–±–ª–∏—á–Ω—ã–π –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç!\")\n",
    "        logger.error(f\"   –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç—å: {VAL_DATASET_PUBLIC}\")\n",
    "        return None, {'error': '–ü—É–±–ª–∏—á–Ω—ã–π –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω'}\n",
    "    \n",
    "    logger.info(\"‚úÖ –û—Å–Ω–æ–≤–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã –Ω–∞–π–¥–µ–Ω—ã\")\n",
    "    logger.info(f\"   ‚Ä¢ –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ: {TRAIN_DATASET_1 if os.path.exists(TRAIN_DATASET_1) else ''} {TRAIN_DATASET_2 if os.path.exists(TRAIN_DATASET_2) else ''}\")\n",
    "    logger.info(f\"   ‚Ä¢ –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –ø—É–±–ª–∏—á–Ω—ã–π: {VAL_DATASET_PUBLIC}\")\n",
    "    logger.info(f\"   ‚Ä¢ –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –ø—Ä–∏–≤–∞—Ç–Ω—ã–π: {'‚úÖ' if os.path.exists(VAL_DATASET_PRIVATE) else '‚ùå'}\")\n",
    "    \n",
    "    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è GPU –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º –æ–±—É—á–µ–Ω–∏—è\n",
    "    optimize_gpu_memory()\n",
    "    \n",
    "    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "    device_info = detect_device_kaggle()\n",
    "    device_str = device_info.get('device', 'cpu')\n",
    "    \n",
    "    # –†–∞—Å—á–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏ GPU\n",
    "    base_img_size = 1056 if IS_DEBUG else 832  # –†–∞–∑–º–µ—Ä 1056 –¥–ª—è debug —Ä–µ–∂–∏–º–∞ –∫–∞–∫ —Ç—Ä–µ–±—É–µ—Ç—Å—è\n",
    "    optimal_img_size = calculate_optimal_img_size(device_str, base_img_size)\n",
    "    \n",
    "    logger.info(f\"üéØ –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {optimal_img_size}x{optimal_img_size}\")\n",
    "    \n",
    "    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è\n",
    "    training_params = {\n",
    "        'data_yaml': None,  # –ë—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑ Kaggle –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\n",
    "        'epochs': 2 if IS_DEBUG else 1000,  # 3 —ç–ø–æ—Ö–∏ –¥–ª—è —Ä–µ–∂–∏–º–∞ –æ—Ç–ª–∞–¥–∫–∏\n",
    "        'batch_size': 8 if IS_DEBUG else 16,\n",
    "        'img_size': 1056,  # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è 1056 –∫–∞–∫ —Ç—Ä–µ–±—É–µ—Ç—Å—è\n",
    "        'device': 'auto',\n",
    "        'save_dir': models_path,\n",
    "        'project': 'yolov13_uav_human_detection',\n",
    "        'name': f'kaggle_training_debug_{int(time.time())}' if IS_DEBUG else f'kaggle_training_{int(time.time())}',\n",
    "        'use_kaggle_datasets': True,\n",
    "        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ä–µ–∂–∏–º–∞ –æ—Ç–ª–∞–¥–∫–∏\n",
    "        'debug_conf': 0.15 if IS_DEBUG else 0.25,  # –£–º–µ—Ä–µ–Ω–Ω—ã–π –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏\n",
    "        'debug_iou': 0.6 if IS_DEBUG else 0.7,     # –£–º–µ—Ä–µ–Ω–Ω—ã–π NMS –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏\n",
    "        'debug_mode': IS_DEBUG\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"\\nüìã –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:\")\n",
    "    for key, value in training_params.items():\n",
    "        logger.info(f\"   {key}: {value}\")\n",
    "    \n",
    "    try:\n",
    "        # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è\n",
    "        logger.info(\"\\nüöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è...\")\n",
    "        model, results = train_yolov13_dynamic_uav(**training_params)\n",
    "        \n",
    "        if results.get('success', False):\n",
    "            logger.info(\"\\nüéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!\")\n",
    "            logger.info(f\"üìÅ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {results['model_path']}\")\n",
    "            \n",
    "            # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ —Å –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏\n",
    "            best_metrics = results.get('best_metrics', {})\n",
    "            if best_metrics:\n",
    "                logger.info(f\"üìä –õ—É—á—à–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è:\")\n",
    "                for metric, value in best_metrics.items():\n",
    "                    try:\n",
    "                        if value is not None and isinstance(value, (int, float)) and not math.isnan(value):\n",
    "                            logger.info(f\"   {metric}: {value:.4f}\")\n",
    "                        elif value is not None:\n",
    "                            logger.info(f\"   {metric}: {value}\")\n",
    "                        else:\n",
    "                            logger.warning(f\"   {metric}: None (–º–µ—Ç—Ä–∏–∫–∞ –Ω–µ –≤—ã—á–∏—Å–ª–µ–Ω–∞)\")\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"   ‚ùå –û—à–∏–±–∫–∞ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫–∏ {metric}: {e}\")\n",
    "            else:\n",
    "                logger.warning(\"‚ö†Ô∏è –ú–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö\")\n",
    "            \n",
    "            # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "            try:\n",
    "                logger.info(\"\\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...\")\n",
    "                \n",
    "                # –ü–æ–∏—Å–∫ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π\n",
    "                best_model_path = None\n",
    "                search_paths = []\n",
    "                \n",
    "                # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º best_model_path –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "                if 'best_model_path' in results and results['best_model_path']:\n",
    "                    search_paths.append(results['best_model_path'])\n",
    "                \n",
    "                # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º model_path –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "                if 'model_path' in results and results['model_path']:\n",
    "                    search_paths.append(results['model_path'])\n",
    "                \n",
    "                # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø—É—Ç—å –≤ models/\n",
    "                standard_model_path = os.path.join(models_path, 'yolov13_dynamic_uav_best.pt')\n",
    "                search_paths.append(standard_model_path)\n",
    "                \n",
    "                # 4. –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞\n",
    "                if 'save_dir' in results and results['save_dir']:\n",
    "                    project_best = os.path.join(results['save_dir'], 'weights', 'best.pt')\n",
    "                    search_paths.append(project_best)\n",
    "                    project_last = os.path.join(results['save_dir'], 'weights', 'last.pt')\n",
    "                    search_paths.append(project_last)\n",
    "                \n",
    "                # 5. –ü–æ–∏—Å–∫ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø–∞–ø–∫–µ kaggle_training\n",
    "                try:\n",
    "                    project_dir = 'yolov13_uav_human_detection'\n",
    "                    if os.path.exists(project_dir):\n",
    "                        kaggle_dirs = [d for d in os.listdir(project_dir) if d.startswith('kaggle_training')]\n",
    "                        if kaggle_dirs:\n",
    "                            latest_dir = max(kaggle_dirs, key=lambda x: os.path.getctime(os.path.join(project_dir, x)))\n",
    "                            latest_best = os.path.join(project_dir, latest_dir, 'weights', 'best.pt')\n",
    "                            latest_last = os.path.join(project_dir, latest_dir, 'weights', 'last.pt')\n",
    "                            search_paths.extend([latest_best, latest_last])\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ kaggle_training –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ö: {e}\")\n",
    "                \n",
    "                # –ü–æ–∏—Å–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –º–æ–¥–µ–ª–∏\n",
    "                logger.info(f\"üîç –ü–æ–∏—Å–∫ –º–æ–¥–µ–ª–∏ –≤ {len(search_paths)} –≤–æ–∑–º–æ–∂–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö...\")\n",
    "                for i, path in enumerate(search_paths, 1):\n",
    "                    logger.debug(f\"   {i}. –ü—Ä–æ–≤–µ—Ä—è–µ–º: {path}\")\n",
    "                    if path and os.path.exists(path):\n",
    "                        best_model_path = path\n",
    "                        logger.info(f\"‚úÖ –ù–∞–π–¥–µ–Ω–∞ –º–æ–¥–µ–ª—å: {best_model_path}\")\n",
    "                        break\n",
    "                \n",
    "                if not best_model_path:\n",
    "                    logger.error(\"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –Ω–∏ –≤ –æ–¥–Ω–æ–º –∏–∑ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –º–µ—Å—Ç:\")\n",
    "                    for i, path in enumerate(search_paths, 1):\n",
    "                        logger.error(f\"   {i}. {path} - {'—Å—É—â–µ—Å—Ç–≤—É–µ—Ç' if path and os.path.exists(path) else '–Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç'}\")\n",
    "                    raise FileNotFoundError(f\"Model not found in any of the search paths\")\n",
    "                \n",
    "                # test_model = YOLO(best_model_path)\n",
    "                logger.info(\"‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞\")\n",
    "                logger.info(\"üéØ –ì–æ—Ç–æ–≤–∞ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ª—é–¥–µ–π —Å UAV\")\n",
    "                \n",
    "                # –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ –ø—Ä–∏–≤–∞—Ç–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ —Å –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏\n",
    "                if os.path.exists(VAL_DATASET_PRIVATE):\n",
    "                    logger.info(\"\\nüîí –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ –ø—Ä–∏–≤–∞—Ç–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ...\")\n",
    "                    \n",
    "                    try:\n",
    "                        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏\n",
    "                        logger.info(\"üìä –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫...\")\n",
    "                        predicted_df = process_images_adapted(\n",
    "                            VAL_DATASET_PRIVATE,\n",
    "                            result_csv_path=os.path.join(log_dir, 'private_predictions.csv')\n",
    "                        )\n",
    "                        \n",
    "                        if not predicted_df.empty:\n",
    "                            logger.info(f\"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(predicted_df)} –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\")\n",
    "                            \n",
    "                            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º DataFrame –≤ bytes –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ evaluate\n",
    "                            predicted_bytes = df_to_bytes(predicted_df)\n",
    "                            \n",
    "                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ ground truth —Ñ–∞–π–ª–∞\n",
    "                            gt_csv_path = os.path.join(VAL_DATASET_PRIVATE, 'gt.csv')\n",
    "                            if not os.path.exists(gt_csv_path):\n",
    "                                # –ü–æ–ø—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –ø—É—Ç–∏\n",
    "                                alternative_gt_paths = [\n",
    "                                    os.path.join(VAL_DATASET_PRIVATE, 'ground_truth.csv'),\n",
    "                                    os.path.join(VAL_DATASET_PRIVATE, 'labels.csv'),\n",
    "                                    PUBLIC_GT_CSV_PATH\n",
    "                                ]\n",
    "                                for alt_path in alternative_gt_paths:\n",
    "                                    if os.path.exists(alt_path):\n",
    "                                        gt_csv_path = alt_path\n",
    "                                        break\n",
    "                                else:\n",
    "                                    logger.warning(\"‚ö†Ô∏è Ground truth —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\")\n",
    "                                    # –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π GT —Ñ–∞–π–ª –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\n",
    "                                    empty_gt = pd.DataFrame(columns=COLUMNS)\n",
    "                                    gt_csv_path = os.path.join(log_dir, 'empty_gt.csv')\n",
    "                                    empty_gt.to_csv(gt_csv_path, index=False)\n",
    "                            \n",
    "                            # –ó–∞–≥—Ä—É–∂–∞–µ–º ground truth\n",
    "                            gt_bytes = open_df_as_bytes(gt_csv_path)\n",
    "                            \n",
    "                            # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Å –ø–æ–º–æ—â—å—é —Ñ—É–Ω–∫—Ü–∏–∏ evaluate\n",
    "                            logger.info(\"üßÆ –†–∞—Å—á–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫...\")\n",
    "                            metric, accuracy, fp_rate, avg_time = evaluate(\n",
    "                                predicted_file=predicted_bytes,\n",
    "                                gt_file=gt_bytes,\n",
    "                                thresholds=np.round(np.arange(0.3, 1.0, 0.07), 2),\n",
    "                                beta=1.0,\n",
    "                                m=500,\n",
    "                                parallelize=True\n",
    "                            )\n",
    "                            \n",
    "                            # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "                            logger.info(\"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏:\")\n",
    "                            logger.info(f\"   üéØ –û–±—â–∞—è –º–µ—Ç—Ä–∏–∫–∞: {metric:.4f}\")\n",
    "                            logger.info(f\"   ‚è±Ô∏è –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {avg_time:.4f}—Å\")\n",
    "                            \n",
    "                            if accuracy:\n",
    "                                logger.info(\"   üìà –¢–æ—á–Ω–æ—Å—Ç—å –ø–æ –ø–æ—Ä–æ–≥–∞–º:\")\n",
    "                                for threshold, acc in accuracy.items():\n",
    "                                    logger.info(f\"      –ü–æ—Ä–æ–≥ {threshold}: {acc:.4f}\")\n",
    "                            \n",
    "                            if fp_rate:\n",
    "                                logger.info(\"   ‚ö†Ô∏è –ß–∞—Å—Ç–æ—Ç–∞ –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π:\")\n",
    "                                for threshold, fpr in fp_rate.items():\n",
    "                                    logger.info(f\"      –ü–æ—Ä–æ–≥ {threshold}: {fpr:.4f}\")\n",
    "                            \n",
    "                            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Ñ—É–Ω–∫—Ü–∏–∏\n",
    "                            final_private_results = validate_on_private_dataset(\n",
    "                                best_model_path,\n",
    "                                debug_conf=training_params.get('debug_conf', 0.25),\n",
    "                                debug_iou=training_params.get('debug_iou', 0.7),\n",
    "                                debug_mode=training_params.get('debug_mode', False)\n",
    "                            )\n",
    "                            \n",
    "                            if 'error' not in final_private_results:\n",
    "                                logger.info(\"üìä –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏:\")\n",
    "                                for metric_name, value in final_private_results.items():\n",
    "                                    try:\n",
    "                                        if value is not None and isinstance(value, (int, float)) and not math.isnan(value):\n",
    "                                            logger.info(f\"   {metric_name}: {value:.4f}\")\n",
    "                                        elif value is not None:\n",
    "                                            logger.info(f\"   {metric_name}: {value}\")\n",
    "                                        else:\n",
    "                                            logger.warning(f\"   {metric_name}: None (–º–µ—Ç—Ä–∏–∫–∞ –Ω–µ –≤—ã—á–∏—Å–ª–µ–Ω–∞)\")\n",
    "                                    except Exception as e:\n",
    "                                        logger.error(f\"   ‚ùå –û—à–∏–±–∫–∞ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫–∏ {metric_name}: {e}\")\n",
    "                            else:\n",
    "                                logger.warning(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {final_private_results['error']}\")\n",
    "                        \n",
    "                        else:\n",
    "                            logger.warning(\"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}\")\n",
    "                        import traceback\n",
    "                        traceback.print_exc()\n",
    "                        \n",
    "                else:\n",
    "                    logger.warning(f\"‚ö†Ô∏è –ü—Ä–∏–≤–∞—Ç–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {VAL_DATASET_PRIVATE}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}\")\n",
    "            \n",
    "            # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é\n",
    "            logger.info(\"\\nüìã –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –º–æ–¥–µ–ª–∏:\")\n",
    "            logger.info(\"   ‚Ä¢ –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –≤—ã—Å–æ—Ç–∞ —Å—ä–µ–º–∫–∏: 50-150–º\")\n",
    "            logger.info(\"   ‚Ä¢ –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ: 1920x1080 –∏–ª–∏ –≤—ã—à–µ\")\n",
    "            logger.info(\"   ‚Ä¢ –°–∫–æ—Ä–æ—Å—Ç—å –ø–æ–ª–µ—Ç–∞: –Ω–µ –±–æ–ª–µ–µ 15 –º/—Å\")\n",
    "            logger.info(\"   ‚Ä¢ –£–≥–æ–ª –∫–∞–º–µ—Ä—ã: 45-90¬∞ –≤–Ω–∏–∑\")\n",
    "            logger.info(\"   ‚Ä¢ –õ—É—á—à–∏–µ —É—Å–ª–æ–≤–∏—è: –¥–Ω–µ–≤–Ω–æ–µ –æ—Å–≤–µ—â–µ–Ω–∏–µ, —è—Å–Ω–∞—è –ø–æ–≥–æ–¥–∞\")\n",
    "            logger.info(\"   ‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: model.predict('path_to_uav_image.jpg')\")\n",
    "            \n",
    "            if IS_DEBUG:\n",
    "                logger.info(\"\\nüêõ –†–µ–∂–∏–º –æ—Ç–ª–∞–¥–∫–∏ –∑–∞–≤–µ—Ä—à–µ–Ω.\")\n",
    "                logger.info(\"   –î–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ IS_DEBUG = False\")\n",
    "                logger.info(\"   –∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å–∫—Ä–∏–ø—Ç.\")\n",
    "            \n",
    "            return model, results\n",
    "            \n",
    "        else:\n",
    "            logger.error(f\"\\n‚ùå –û–±—É—á–µ–Ω–∏–µ –Ω–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {results.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}\")\n",
    "            return None, results\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"\\nüí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, {'error': str(e)}\n",
    "        \n",
    "    finally:\n",
    "        \n",
    "        \n",
    "        logger.info(\"\\n\" + \"=\" * 70)\n",
    "        logger.info(\"üèÅ –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –ø—Ä–æ–≥—Ä–∞–º–º—ã\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # –ó–∞–ø—É—Å–∫ –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è YOLOv13 —Å Kaggle –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏\n",
    "    model, results = main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7986830,
     "sourceId": 12639078,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7987619,
     "sourceId": 12640174,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7987871,
     "sourceId": 12640533,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
